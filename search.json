[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Using Julia for Introductory Statistics",
    "section": "",
    "text": "1 Julia introduction\nThis is a collection of notes for using Julia for introductory statistics.\nIn case you haven’t heard, Julia is an open-source programming language suitable for many tasks, like scientific programming. It is designed for high performance – Julia programs compile on the fly to efficient native code. Julia has a relatively easy to learn syntax for many tasks, certainly no harder to pick up than R and Python, widely used scripting languages for the tasks illustrated herein.\nWhy these notes on introductory statistics? No compelling reason save I had done something similar for R when R was a fledgling S-Plus clone. No more, R is a juggernaut, and it is almost certain Julia will never replace R as the programming langauage of choice for statistics. Besides, Julia users can already interface with R quite easily through RCall. However, there are some reasons that Julia could be a useful language when learning basic inferential statistics, especially if other real strengths of the Julia ecosystem were needed. So these notes show how Julia can be used for these tasks, and, hopefully, shows that it works pretty well.\nThere are some great books published about using Julia (Bezanson et al. 2017) with data science, within which much of this material is covered. For example, (Kamiński 2022) is a quite thorough treatmeant, (Storopoli et al. 2021) is very well done, (Nazarathy and Klok 2021) covers topics here (cf. the JuliaCon Workshop). The quarto book Embrace Uncertainty (Alday et al. 2022) covers the more advanced topic of Mixed-effects Models in Julia. Nothing here couldn’t be found in those resources, these notes are just an introduction."
  },
  {
    "objectID": "index.html#installing-and-running-julia",
    "href": "index.html#installing-and-running-julia",
    "title": "Using Julia for Introductory Statistics",
    "section": "1.1 Installing and running Julia",
    "text": "1.1 Installing and running Julia\nJulia can be downloaded from julialang.org. The language is evolving rapidly. The latest official release is recommended. These notes should work with any version since v\"1.6.0\". It is recommended to use a version v\"1.9.0\" or later, as there are significant speedups with external packages that make the user experience even better.\nOnce downloaded and installed the Julia installation will provide a command line for interactive usage and a binary to run scripts. It is envisioned most users will use an alternative interface, though Julia has an excellent REPL for command-line usage.\nSome alternatives to the REPL for interacting with Julia are:\n\nIJulia: This is a means to use the Jupyter interactive environment to interact with Julia through notebooks. It is made available by installing the package IJulia (details on package installation follow below). This relies on Julia’s seamless interaction with Python and leverages many technologies developed for that langauge.\nPluto: The Pluto environment provides a notebook interface for Julia written in Julia leveraging many JavaScript technologies for the browser. It has the feature of being reactive, making it well suited for many exploratory tasks and pedagogical demonstrations.\nVisual Studio Code: Julia is a supported language for the Visual Studio Code editor of Microsoft, a programmer’s IDE.\n\nThese notes use quarto to organize the mix of text, code, and graphics. The quarto publishing system is developed by Posit, the developers of the wildly sucessful RStudio interface for R. The code snippets are run as blocks (within IJulia) and the last command executed is shown. (If code is copy-and-pasted into the REPL, each line’s output will be displayed.) The code display occurs below the cell, as here, where we show that Julia can handle basic addition:\n\n2 + 2\n\n4"
  },
  {
    "objectID": "index.html#overview-of-some-basics",
    "href": "index.html#overview-of-some-basics",
    "title": "Using Julia for Introductory Statistics",
    "section": "1.2 Overview of some basics",
    "text": "1.2 Overview of some basics\nThis section gives a quick orientation for using Julia. See this compiled collection of tutorials for more comprehensive introductions.\nAs will be seen, Julia use multiple dispatch (as does R) where different function methods can be called using the same generic name. Different methods are dispatched depending on the type and number of the arguments. The + sign above, is actually a function call to the + function, which in base Julia has over 200 different methods, as there are many different implementations for addition. For a beginner this is great – fewer new function names to remember.\nJulia is a dynamically typed language, like R and Python, meaning variables can be reassigned to different values and with different types.1 Dynamicness makes interactive usage at the REPL or through a notebook much easier.\nJulia supports the usual mathematical operations familiar to users of a calculator, such as +, -, *, /, and ^. In addition, there a numerous built in functions such as mathematical ones like sqrt or programming oriented ones, like map.\nThese functions are called with arguments which may be positional (\\(0\\), \\(1,\\) or more positional arguments) or specified by keywords. Multiple dispatch considers the positions and types of arguments a function is called with.\n\n\n\n\n\n\nInteractive help\n\n\n\nInteracting with Julia primarily involves variables and functions. Most all functions have documentation, which can be called up by prefacing the function name with an immediate question mark, as in ?sqrt to see the documentation for sqrt. More than one method may be documented. A call like ?sqrt(9) will limit the help to the method called by sqrt(9) (the square root function for integers.)\n\n\nValues in Julia have types. A particular instance will have a concrete type but abstract types help to organize code bases and participate in dispatch. Values can be assigned to variable names, or bindings. The ability to simply create new user-defined types makes generic programming quite accessible and Julia code very composable.\nThis simple example, taking the average of several numbers, shows most of this:\n\nxs = [1, 2, 3, 7, 9]\nsum(xs) / length(xs)\n\n4.4\n\n\nThe first line assigns to a variable, xs, a value that is a vector of numbers, integers of type Int64 in this case. For this illustration, a vector is a container of different numbers. The second line calls three functions: sum to add the elements in the vector; length to count the number of elements in the vector; and / to divide these two quantities. All of these functions are generic, with different methods for different types of argument(s). The same pattern would work for different container types, such as a tuple:\n\nxs = (1, 2, 3, 7, 9) # tuple\nsum(xs) / length(xs)\n\n4.4\n\n\nThe takeaway – we can focus more on what the computations mean, and less on how to program a particular computation."
  },
  {
    "objectID": "index.html#add-on-packages",
    "href": "index.html#add-on-packages",
    "title": "Using Julia for Introductory Statistics",
    "section": "1.3 Add-on packages",
    "text": "1.3 Add-on packages\nBase Julia provides a very useful programming environment which can be extended through packages. Some packages are provided by base Julia, such as Dates, others are external add-on packages, such as IJulia, mentioned previously. Julia has one key package, Pkg, to manage the installation. By default, the installation of a single package will download all dependent packages. On installation, packages are partially compiled. This speeds up the loading of a package when it is used within a session, but can slow down package installation.\nPackages need be installed just once, but must be loaded each session. Loading a package is done by a command like using Statistics, which will load the built in Statistics package. At the REPL, calling using PKGNAME on an uninstalled package will lead to a prompt to install the package. For other interfaces, packages may need to be installed through the Pkg package, loaded through using Pkg.\nWhen a package is loaded its exported functions are made available to use directly. Non-exported functions can be accessed by qualifying the function with the name of a module (conventionally the name of the package). For example, we will see the command CSV.read which calls the read function provided in the CSV package which has a CSV module.\nMost packages are designed to extend generic functions that may be defined elsewhere. Not all. When there are conflicts, they can be resolved by either just importing the packages and qualifying all uses, or qualifying the uses that conflict.\nThese notes will utilize numerous add-on packages including:\n\nStatsBase, to extend the built-in Statistics package;\nStatsPlots, for easy-to-make statistical plots, which display on a variety of graphing backends;\nAlgebraOfGraphics and CairoMakie, for more advanced statistical graphics;\nCSV and DataFrames for working with tabular data;\nRDatasets, for some handy datasets;\nFreqTables and CategoricalArrays, for some needed functionality;\nDistributions, for probability distributions;\nHypothesisTests, for the computation of significance tests and confidence intervals; and\nGLM, Loess, and RobustModels, for statistical modeling.\n\nMost of these are maintained by the StatsBase organization, which provides the StatsKit package to load all these with a single command, though we don’t illustrate that.\n\nCopyright 2023, John Verzani. All rights reserved.\n\n\n\n\nAlday, P., Kliegl, R., and Bates, D. (2022), Embrace uncertainty: Fitting mixed-effects models with julia, https://juliamixedmodels.github.io/EmbraceUncertainty/.\n\n\nBezanson, J., Edelman, A., Karpinski, S., and Shah, V. B. (2017), “Julia: A fresh approach to numerical computing,” SIAM review, SIAM, 59, 65–98.\n\n\nKamiński, B. (2022), Julia for data analysis, Manning.\n\n\nNazarathy, Y., and Klok, H. (2021), Statistics with julia: Fundamentals for data science, machine learning and artificial intelligence, Springer.\n\n\nStoropoli, J., Huijzer, R., and Alonso, L. (2021), Julia data science."
  },
  {
    "objectID": "EDA/univariate-julia.html#data-vectors",
    "href": "EDA/univariate-julia.html#data-vectors",
    "title": "2  Univariate data",
    "section": "2.1 Data vectors",
    "text": "2.1 Data vectors\nJulia offers several containers for storing ordered data, such as in a data set \\(x_1, x_2, \\dots, x_n\\).\nWe consider this data on the number of whale beachings in a certain area by year during a decade:\n74 122 235 111 292 111 211 133 156 79\nAs the data in a data set is typically of the same type (integer here, or character, number, …) and may be quite large a vector is a natural choice for storage in Julia.\nVectors are created using square brackets with entries separated by commas. Storing the data in a vector and assigning to a variable, whale, is then done with:\n\nwhale = [74, 122, 235, 111, 292, 111, 211, 133, 156, 79]\n\n10-element Vector{Int64}:\n  74\n 122\n 235\n 111\n 292\n 111\n 211\n 133\n 156\n  79\n\n\nWithout using commas in the construction, a matrix is created:\n\nwhale_matrix = [74 122 235 111 292 111 211 133 156 79]\n\n1×10 Matrix{Int64}:\n 74  122  235  111  292  111  211  133  156  79\n\n\n\n\n\n\n\n\nSpace-saving measure\n\n\n\nFor purposes of printing only, we use show or the trailing |> show (which uses the chaining operation to call the show function) in the following to format how a vector is printed. This is only a vertical-space-saving measure and wouldn’t normally be done when interacting with Julia.1\n\nshow(whale)\n\n[74, 122, 235, 111, 292, 111, 211, 133, 156, 79]\n\n\n\n\nInternally a vector is a \\(1\\)-dimensional array, a matrix a \\(2\\)-dimensional array. Julia has a general \\(N\\)-dimensional array type that these are specializations of. For a matrix, one uses space to separate row elements and semicolons to separate rows. Vectors are useful for storing data; matrices can be used to store data, as will be seen through example, but are widely used to represent mathematical values, as they have an associated algebra that can compactly represent many operations (we will see a data frame is more idiomatic for storing rectangular data); arrays are not used in our discussion.\nAs seen in the output above, Julia prints the size and element type of a vector. In this case, a vector of integers (Int64). The constructor ([]) will promote values to a common type, which may be Any, a catch all type.\nVectors have a length, found by length, which in this case is the number of data points, often labeled \\(n\\). More generally, arrays have a size:\n\nlength(whale), size(whale)\n\n(10, (10,))\n\n\nThe size is returned as a tuple, in this case with just 1 element, as vectors are \\(1\\) dimensional.\nThe length function is a reduction, returning a summary of a vector. Similarly, sum will add the elements, returning a number. Here we see how to compute the mean:\n\nsum(whale) / length(whale)\n\n152.4\n\n\nThe mean function is not part of base Julia. It can be found in the Statistics package, which comes bundled with Julia, but we prefer to use the add-on StatsBase package:\n\nusing StatsBase\nmean(whale)\n\n152.4\n\n\n\n2.1.1 Missing values\nJulia uses missing to represent missing values. For example, data on the cost of a hip replacement at various hospitals was found from their websites and is given by:\n10500, 45000, 74100, unavailable, 83500,\n86000, 38200, unavailable, 44300, 12500,\n55700, 43900, 71900, unavailable, 62000\nWhen the cost could not be identified, it was labeled “unavailable.” We replace the colloquial “unavailable” with the special value missing and enter the values into a vector:\n\nhip_cost = [10500, 45000, 74100, missing, 83500, 86000, 38200, missing,\n            44300, 12500, 55700, 43900, 71900, missing, 62000]\nhip_cost |> permutedims\n\n1×15 reshape(::Vector{Union{Missing, Int64}}, 1, 15) with eltype Union{Missing, Int64}:\n 10500  45000  74100  missing  83500  …  55700  43900  71900  missing  62000\n\n\nWe highlight that the type of element in hip_cost is Union{Missing, Int64}, a type that allows an element to be either an integer or a missing element.\nThe missing value propagates through computations:\n\nsum(hip_cost)\n\nmissing\n\n\nIn particular, all of these combinations with missing yield missing, as they should – if data is not available, combinations based on that data are still not available:\n\n1 + missing, 1 - missing, 1*missing, 1/missing, missing^2, missing == true\n\n(missing, missing, missing, missing, missing, missing)\n\n\n\n\n\n\n\n\nAlso nothing and something\n\n\n\nIn Julia there is also nothing. The semantics are a bit different, and missing is the designed choice for data. (The nothing value is useful for general programming purposes.) For floating point values, there is also NaN, or not a number. This too is often used as a sentinel value, to indicate missingness, though missing is idiomatic. The something function is used to skip over its arguments until a non-nothing value is found. The coalesce function does a similar thing with missing values.\n\n\nTo work with missing data, it can be removed or skipped over. The skipmissing function provides a wrapper around an object which when iterated over, will skip the missing values. For example:\n\nsum(skipmissing(hip_cost))\n\n627600\n\n\nThe sum function iterates over the values in the container it is passed to reduce them to a value, mean is similar and works with skipmissing in an identical manner:\n\nmean(skipmissing(hip_cost))\n\n52300.0\n\n\n(Which is helpful, as length, used above to compute a mean, does not compose with skipmissing.)\n\n\n2.1.2 Names (named tuples)\nIt may be natural to assign the year of measurement to the values in whale, but vectors, as defined in base Julia, do not allow names as an attribute. (There are external packages that allow this, e.g. NamedArrays, which arise in our discussion of contingency tables.) If names are important, Julia provides a named tuple type, which builds on the basic tuple type.\nWhile vectors are collections of homogeneous values, tuples are collections of heterogeneous values. Tuples are constructed with commas, and typically parentheses to delimit the commas:2\n\nwhale = (74, 122, 235, 111, 292, 111, 211, 133, 156, 79)\n\n(74, 122, 235, 111, 292, 111, 211, 133, 156, 79)\n\n\nOne-element tuples are distinguished by using a trailing command and parentheses:\n\na_lone_whale = (101,)\n\n(101,)\n\n\nFor the many purposes, tuples can be exchanged for vectors, as both are iterable3:\n\nsum(whale), mean(whale), length(whale)\n\n(1524, 152.4, 10)\n\n\nUnlike vectors, but like numbers, tuples can not be modified after construction. This allows tuples to be quite useful – and performant – for programming purposes.\nTuples can also have names. The basic construction uses “key=value” pairs:\n\ntest_scores = (Alice = 87, Bob = 72, Shirley = 99)\n\n(Alice = 87, Bob = 72, Shirley = 99)\n\n\nThe names are not quoted, and are stored internally as a tuple of symbols. Extra effort is necessary to create names with spaces or number. In the following example we show that var\"...\" is useful to create more complicated symbols4:\n\nchildren = (var\"X Æ A-Xii\" = 1,\n            var\"Vivian Jenna Wilson\" = 2,\n            var\"Exa Dark Sideræl\" = 3)\n\n(var\"X Æ A-Xii\" = 1, var\"Vivian Jenna Wilson\" = 2, var\"Exa Dark Sideræl\" = 3)\n\n\nThe above also shows that Unicode values are easily used within Julia in strings or as identifiers.\nNamed tuples can also have their “names” attached via parsing, through a syntax similar to keyword arguments of functions, as identifiers imply names:\n\na = [1,2]; b = [3,4]\n(; a, b)  # same as (a=a, b=b)\n\n(a = [1, 2], b = [3, 4])\n\n\nThis gives several different ways to construct \\(1\\)-element named tuples, where something is done to disambiguate the use of enclosing parentheses:\n\n(; a), (; a=a), (; :a => a), (a=a, )\n\n((a = [1, 2],), (a = [1, 2],), (a = [1, 2],), (a = [1, 2],))\n\n\nNamed tuples can have their values accessed by name using getproperty, which has a . for convenient syntax:\n\ntest_scores.Alice\n\n87\n\n\nThe generic function keys will return the names (“keys” are used to look up a value) and values will extract the values.\nNamed tuples being both named and heterogeneous are a natural container for collecting data on several different variables for a single case, as will be seen in the discussion on tabular data.\n\nAssociative arrays\nAbstractly, an associative array is a container for storing (key,value) pairs. Julia has the notation key => value for representing a pair. A named tuple is an immutable associative array where the keys are symbols. A dictionary is a more general mutable type with the keys being arbitrary (numbers, strings, symbols, …). There are various implementations of an AbstractDict, the Dict constructor being the most commonly used, though the one provided in the Dictionaries package will be seen. As with a named tuple, the keys are returned by keys, the values by values, and pairs of (key/value)s by pairs.\n\n\n\n2.1.3 Indexing\nThe elements of a vector, like a data set, are indexed. For vectors Julia uses one-based indexing5. The whale data, as defined, has \\(10\\) elements, we can get the third, fourth, and sixth with:\n\nwhale = [74, 122, 235, 111, 292, 111, 211, 133, 156, 79]\nwhale[3], whale[4], whale[6]\n\n(235, 111, 111)\n\n\nThe above uses three function calls, and displays as a tuple does. (Since the commas produce a tuple.)\nTo retrieve these same values in a single call, we can pass a vector of indices:\n\nwhale[ [3,4,6] ]\n\n3-element Vector{Int64}:\n 235\n 111\n 111\n\n\nThe end keyword refers to the last index of a collection. Similarly, begin refers to the first index inside the square brackets.\n\nwhale[end], whale[begin]\n\n(79, 74)\n\n\nThese values allow simple arithmetic. Here we see how to trim off the first and last through indexing6:\n\nwhale[begin+1:end-1] |> show\n\n[122, 235, 111, 292, 111, 211, 133, 156]\n\n\nThe colon, :, refers to all indices in a vector; whale[:] will return a copy of the data in whale.\n\n\n\n\n\n\nIndexing with a container of values\n\n\n\nThe range 1:1 specifies the value 1, as does just 1, but for indexing the two are different:\n\nwhale[1], whale[1:1]\n\n(74, [74])\n\n\nThe design is indexing by a scalar – like 1 – can drop dimensions (the vector becomes a scalar), whereas indexing by a container – like 1:1 or, say, [1] – does not drop dimensions. The documentation for indexing of an array has: “If all the indices are scalars, then the result, X, is a single element from the array, A. Otherwise, X is an array with the same number of dimensions as the sum of the dimensionalities of all the indices.”\n\n\n\nViews\nThe extraction of values from a vector, as above, necessitates the allocation of memory to store the copy of the values. When the data set is large, or is accessed many times, these allocations may be avoided using a “view” of the data. Views create a lazy reference to the underlying data in the array. The view function takes the object as its first argument, and indices for its remaining arguments7:\n\nview(whale, [3,4,6])\n\n3-element view(::Vector{Int64}, [3, 4, 6]) with eltype Int64:\n 235\n 111\n 111\n\n\nThe end and begin keywords do not work with view8; the lastindex and firstindex functions do:\n\nview(whale, firstindex(whale)+1:lastindex(whale)-1) |> show\n\n[122, 235, 111, 292, 111, 211, 133, 156]\n\n\n\n\n\n2.1.4 Assignment\nThe assignment\n\nwhale = [74, 122, 235, 111, 292, 111, 211, 133, 156, 79]\nshow(whale)\n\n[74, 122, 235, 111, 292, 111, 211, 133, 156, 79]\n\n\nbinds the name whale to the container holding the \\(10\\) numbers.\nIf we make another assignment, as in\n\nwhale_copy = whale\nshow(whale_copy)\n\n[74, 122, 235, 111, 292, 111, 211, 133, 156, 79]\n\n\nthe container is copied, but – unlike if we had used copy(whale) – the two variables point to the same container. When a vector is passed into a function, the function works with the container, not a copy.\n\n\n2.1.5 Modification\nData may need to be modified after read in or constructed, such as is done with data cleaning. As such, the values in the data vector may need to be reassigned. This is carried out by using the indexing notation on the left-hand side of an assignment:\n\nwhale[1] = 75\n\n75\n\n\nThe value 75 is returned, as the right-hand side of an assignment is always the returned value. We can see that whale was modified:\n\nwhale[1], whale_copy[1]\n\n(75, 75)\n\n\nWe also see that as whale_copy points to the same container, it too was modified.\n\n\n\n\n\n\nMutation\n\n\n\nWhen a vector is passed to a function, if there is no copy made (as opposed to a simple naming), then changes to the vector in the function will effect the original vector as the container used in the body of the function isn’t changed. Functions which modify an argument that is passed to them (usually the first one) are conventionally named with a trailing !, though this has no semantic implication.\n\n\nMultiple values can be assigned at once. For example, if the data was mis-arranged chronologically, we might have:\n\nwhale[ [1,2,3] ] = [235, 74, 122]\nshow(whale)\n\n[235, 74, 122, 111, 292, 111, 211, 133, 156, 79]\n\n\nThe above modified the original container, so these changes would also be reflected in whale_copy.\nJulia does not recycle values to be assigned by default, where recycling involves some rules where the space referenced on the left-hand side of an assignment does not match in size the space needed to store the right-hand side of an assignment. Broadcasted assignment (cf. ?.=) can produce a similar behavior. Broadcasted assignment expands the right-hand side to match the space expected on the left side (when possible) and then does the assignment. For example, if we wanted to use a sentinel value to indicate unknown data for the first 3 values, we might have:\n\nwhale[[1,2,3]] .= 999\nshow(whale)\n\n[999, 999, 999, 111, 292, 111, 211, 133, 156, 79]\n\n\nNotice, without the dot an error will be thrown\n\nwhale[[4,5,6]] = 999  # errors\n\nLoadError: ArgumentError: indexed assignment with a single value to possibly many locations is not supported; perhaps use broadcasting `.=` instead?\n\n\nThe “recycling” above uses the left-hand side to identify the size needed. Broadcasted assignment also works with the entire collection. For example, the following command replaces the current data with the original data:\n\nwhale .= [74, 122, 235, 111, 292, 111, 211, 133, 156, 79]\nshow(whale)\n\n[74, 122, 235, 111, 292, 111, 211, 133, 156, 79]\n\n\nBut whale is not a new object – as it would be without that dot – but rather, these values are placed into the container whale already refers to – which also is the container whale_copy points at:\n\nshow(whale_copy)\n\n[74, 122, 235, 111, 292, 111, 211, 133, 156, 79]\n\n\nThis assignment avoids the needed allocation of more memory.\n\nThe use of [:] on the left-hand side also does in-place assignment9. So unlike whale = [...] which replaces the container whale points at, this command reuses the container:\n\nwhale[:] = [74, 122, 235, 111, 292, 111, 211, 133, 156, 79]\nshow(whale)\n\n[74, 122, 235, 111, 292, 111, 211, 133, 156, 79]\n\n\n\nVarious indexing patterns.\n\n\n\n\n\n\nIndex style\nExplanation\n\n\n\n\nx[1]\nThe first element of x.\n\n\nx[:]\nCopy of all elements of x.\n\n\nx[end]\nThe last element of x.\n\n\nx[first]\nThe first element of x.\n\n\nx[[2,3]]\nThe second and third elements of x.\n\n\ntypeof(x)[]\n\\(0\\)-length vector of same type as x.\n\n\neltype(x)\nElement type of container x.\n\n\nx[1] .= 5\nAssign a value of 5 to first element of x.\n\n\nx[[2,3]] .= 4\nBroadcasted assignment to second and third elements\n\n\nx[[2,3]] = [4,5]\nAssign values to second and third elements of x.\n\n\nx[:] = [1, 2, 3]\nIn-place assignment. Size and type of right-hand side must match left-hand side\n\n\n\n\nVector size\nA vector has a length which can not be changed during assignment. Attempting to assign to an index beyond the size will result in a BoundsError. To extend the size of a vector we can use the following generic functions:\n\npush!(v, x): extend the vector v pushing x to the last value\npushfirst!(v, x): extend the vector v pushing x to the first value\nappend!(v, v1): append the entries in v1 to the end of v\ninsert(v, i, x): insert x into v at index i, shifting as needed\nvcat(v, v1): vertically concatenate v and v1. (Unlike append! this returns a new vector, so the type of the output will be recomputed).\n\nThe vector can also be shrunk. These generic functions for containers are useful:\n\npop!(v): remove last element from a vector; return element\npopfirst!(v): remove first element from a vector; return element\ndeleteat!(v, i): remove ithe element from a vector; returns vector\nempty!(v): remove all elements from a vector\n\nAgain, the trailing ! in a function name is a convention to indicate to the user that the function will mutate its arguments, conventionally its first one. That is, a function like pop!(v) does two things: it returns the last element and also shortens the vector v that is passed in.\n\n\nVectors have an element type\nThe assignment whale = [74, 122, 235, ...] assigns a container of a specific type to the variable whale. The eltype(whale) command will return this element type. Subsequent assignments to this container must contain values that can be promoted to that type, otherwise an error will be thrown.\nFor example, we can’t assign a fractional number of whales:\n\nwhale[1] = 74.5\n\nLoadError: InexactError: Int64(74.5)\n\n\nAn InexactError is thrown because, whale is a vector of integers, and 74.5 can’t be automatically promoted to an integer (as 74.0 could be).\nA similar thing happens if we attempt to assign missing to a value, as in whale[1] = missing. In this case, a MethodError is thrown, which comes from the attempt to “convert” missing into the underlying integer type.\nThe way to fix this is to create a new container that allows a wider type. For Float64 values that can be achieved in the cumbersome manner of converting all the values to the wider type:\n\nwhale = convert(Vector{Float64}, whale)\nwhale[1] = 74.5\n\n74.5\n\n\nThis assigns whale to a new container which accepts floating point values, and then reassigns the first one.\nThough cumbersome, this is not typical usage, as the constructor used to create the data set will promote to a common type, so it would only matter when adjusting the initial values.\nFor the special case of assigning a missing value, the allowmissing function from the DataFrames package10 creates a vector with a type that allows – as well – missing values11. Again, re-assignment is necessary:\n\nusing DataFrames\nwhale = allowmissing(whale)\nwhale[1] = missing\n\nmissing\n\n\n\n\nBroadcasting\nAs seen, the functions length and sum are reductions – in this case, returning a single number, a scalar, from a vector of numbers. To compute a sample standard deviation, say, we follow the formula:\n\\[\ns = \\sqrt{\\frac{\\sum_i (x_i - \\bar{x})^2 }{n-1}}.\n\\]\nTo do so we would need to:\n\nSubtract a scalar value, resulting for a reduction, from from each element of the data vector: (\\(x_i - \\bar{x}\\)).\nApply the squaring function to each element of this difference.\nReduce the resulting data to a number through sum.\nDivide a number by another number and take the square root.\n\nEmbarking on this punch list with a naive attempt at the first – whale - mean(whale) – will fail.\nThe subtraction of a scalar value from a vector value is not defined, as Julia is not implicitly vectorized. Rather the user must be explicit. For this, the concept of broadcasting is useful. In this context, broadcasting will expand the scalar to match the size of the vector and then use vector subtraction to find the result. Broadcasting is done simply by adding a “.” (the dot) to the function. For infix operations like - this is before the operator:\n\nwhale = [74, 122, 235, 111, 292, 111, 211, 133, 156, 79]\nwhale .- mean(whale)  |> show\n\n[-78.4, -30.400000000000006, 82.6, -41.400000000000006, 139.6, -41.400000000000006, 58.599999999999994, -19.400000000000006, 3.5999999999999943, -73.4]\n\n\nWe also would need to square these values. This could also be done by broadcasting ^, as in:\n\n(whale .- mean(whale)).^2  |> show\n\n[6146.560000000001, 924.1600000000003, 6822.759999999999, 1713.9600000000005, 19488.16, 1713.9600000000005, 3433.959999999999, 376.36000000000024, 12.959999999999958, 5387.56]\n\n\nTo avoid having to use too many dots, it is typical to define a function for doing a scalar computation and broadcast that12. An example will wait, but to illustrate the syntax, we can broadcast the sqrt function using a “.” after the function name and before the opening parentheses:\n\nsqrt.(whale)  |> show\n\n[8.602325267042627, 11.045361017187261, 15.329709716755891, 10.535653752852738, 17.08800749063506, 10.535653752852738, 14.52583904633395, 11.532562594670797, 12.489995996796797, 8.888194417315589]\n\n\nBroadcasting works with functions of multiple arguments and multiple shapes.\nThe use of multiple shapes allows scalars and vectors to be broadcast over and is used in whale .- mean(whale). With this, the standard deviation could be computed with:\n\nsqrt( sum((whale .- mean(whale)).^2)/(length(whale) - 1) )\n\n71.5078861229849\n\n\nVector and row vectors can also be broadcast over, So [1,2] .+ [3,4] does vector addition, [1,2] .+ [3 4] pads out the vector to a matrix, the row vector to a matrix, then adds entry-by-entry:\n\n(v=[1,2], rv=[3 4], va = [1,2] .+ [3,4], ma = [1, 2] .+ [3 4], ck=[1 1; 2 2] + [3 4; 3 4])\n\n(v = [1, 2], rv = [3 4], va = [4, 6], ma = [4 5; 5 6], ck = [4 5; 5 6])\n\n\nThis behavior may not be desirable. Some objects broadcast as scalars, others as containers. But there may be times where broadcasting as a container may be incorrect. To force a value to broadcast like a scalar, the value can be wrapped in Ref. That is mean(whale) is the same as mean.(Ref(whale)) but mean.(whale) would broadcast mean over each element in whale. As mean for a single number is just that number, mean.(whale) would just be the same values."
  },
  {
    "objectID": "EDA/univariate-julia.html#sec-data-types",
    "href": "EDA/univariate-julia.html#sec-data-types",
    "title": "2  Univariate data",
    "section": "2.2 Data types",
    "text": "2.2 Data types\nAn old taxonomy of levels of measurement include nominal, ordinal, interval, and ratio; where nominal values have no rank; ordinal values have a rank, but no meaning is assigned to the difference between values; interval data has a meaning between differences but \\(0\\) is arbitrary; and ratio has a meaningful zero, such as most numeric data. As data can easily be coded (explicitly or behind the scenes) with numbers, this keeps the different types distinct. However, for use with the computer, in particular Julia here, we see it also more sense to emphasize different aspects of the data related to the underlying type.\nJulia has numerous data types. Some are “abstract” types, such as Real or Integer, others are “concrete”, often indicating how the data is stored, such as Float64 or Int64, with the “64” indicating \\(64\\) bits of memory. In Julia, these data types can be used to direct method dispatch. For statistics a few common types are used to represent data.\nThese are reviewed in the following.\n\n2.2.1 Numeric data types\nThe Julia parser readily identifies the following values of \\(1\\) and reads each using a different type:\n\n1, 1.0, 1//1, 1 + 0im, big(1), BigFloat(1)\n\n(1, 1.0, 1//1, 1 + 0im, 1, 1.0)\n\n\nRespectively, these represent integer, floating point, rational, complex, and two types of “big” numbers. Julia uses a promotion machinery when different types are mixed. For example, we have:\n\nx = 1 + 1.0 + 1//1 + 1 + 0im + big(1) + BigFloat(1)\n\n6.0 + 0.0im\n\n\nThe type of x must be both complex and be able to store the underlying numbers, which may be “big” numbers:\n\ntypeof(x)\n\nComplex{BigFloat}\n\n\nThe above illustrates that addition of an integer and a floating point yields a floating point, and adding to a complex number returns a complex number, etc.\n\n\n\n\n\n\nType stability\n\n\n\nJulia programmers try to make functions “type stable,” if possible, as this generally leads to more performant code, once compiled. A consequence is addition of numbers, like 1 + 1.0 will always be the wider type (here a floating point value), even if in the case of these particular values an integer could be the answer. That is the output type of + here is determined by the input types, not the input values.\n\n\nFor data consisting of counts, integers are typically used. If storage is an issue (e.g., lots of data, but not a lot of different values), different forms of integers which use less data may be used.\nFor data on measurements, with a continuous nature, floating point values are the natural choice. Floating point can represent most integer values exactly, and fractional and irrational values either exactly or approximately. Rational numbers can represent fractional data exactly, though it should be expected that operations with rational values are less performant than for floating point values.\nComplex values in Julia are based on an underlying data type holding the two numbers in a + bi.\n\n\n2.2.2 Categorical data types\nThere are different options available for the storage of categorical data.\n\nCharacter data\nThe String type in Julia is the default type for holding character data. Strings are created with matching single quotes or – for multiline strings – with matching triple quotes:\n\ns = \"The quick brown fox ...\"\nt = \"\"\"\nFour score and seven years ago\nour fathers brought forth...\n\"\"\"\n\n\"Four score and seven years ago\\nour fathers brought forth...\\n\"\n\n\nThe string type in Julia can hold Unicode data, such as non-ASCII letters and even emojis.\n\n\n\n\n\n\nEntering Unicode\n\n\n\nIn the REPL Unicode values may be entered using LaTeX shortcuts, e,g., \\alpha[tab] to produce \\(\\alpha\\). Many interfaces also allow this.\n\n\nDouble quotes are used to create a string. Triple, double quotes can be used to create multi-line strings.\nSingle quotes are for Char types. A character represents a Unicode code point. Strings are iterable, and iteration yields back Char values. The collect function iterates over an object and returns the values. Here we see the Chars in a string:\n\ns = \"Zoë\"\ncollect(s)\n\n3-element Vector{Char}:\n 'Z': ASCII/Unicode U+005A (category Lu: Letter, uppercase)\n 'o': ASCII/Unicode U+006F (category Ll: Letter, lowercase)\n 'ë': Unicode U+00EB (category Ll: Letter, lowercase)\n\n\nStrings are also indexable. When indexed by a single value, a Char type is returned, when indexed by a range of a values a string is returned. Be warned, indexing into non-ascii strings may error. Here is an example using a string from Julia’s manual:\n\nj = \"jμΛIα\"\nlength(j), j[2], j[2:4]  # 2:4 represent the value 2,3,4\n\n(5, 'μ', \"μΛ\")\n\n\nThe value returned by j[2] is a character, whereas that returned by j[2:4] is a string13. However, attempting to extract j[3] will be an error.\n\n\nString operations\nStrings can be combined many different ways.\nThe * operation is used to combine strings or chars. The basic usage is straight forward:\n\n\"a\" * \"bee\" * \"see\"\n\n\"abeesee\"\n\n\nThe ^ operation when used with an integer exponent will repeat the argument:14\n\n\"dot \"^3\n\n\"dot dot dot \"\n\n\nMore generally, join will combine an iterable of strings or characters. If a delimiter is specified, it will be inserted between values (with an option to indicate the last delimiter differently):\n\nj = \"jμΛIα\"\njoin(collect(j), \",\"), join(collect(j), \", \", \", and \")\n\n(\"j,μ,Λ,I,α\", \"j, μ, Λ, I, and α\")\n\n\nJulia makes string interpolation easy. Within a string, a value of a variable can be inserted using a dollar sign to reference the variable. This is more general, as computations can also be inserted. Below parentheses are used to delimit the interpolated command:\n\nx = \"Alice\"\n\"$x knows that 2 + 2 is equal to $(2+2)\"\n\n\"Alice knows that 2 + 2 is equal to 4\"\n\n\n(To include a dollar sign in the string, it can be escape with a leading slash, “\\”, or a raw string can be used, as in raw\"$(2+2) is not evaluated\".)\nFor formatted values, such as needed when printing floating point values, the built-in Printf package provides support. For more performant solutions, an IOBuffer can be useful.15\nA common means to generate strings is from reading in delimited files, such as comma-separated files. These may produce strings with leading or trailing spaces. To strip these off, Julia offers strip, lstrip, and rstrip. The lstrip function strips the left side, rstrip the right side, and strip combines the two. An option to pass in other characters to strip besides spaces is available.\n\nstrip(\"   abc    \"),  strip(\"...abc...\", '.')\n\n(\"abc\", \"abc\")\n\n\n\nDiscussing data sets for different type of data adds no more complication as vectors in Julia are typed so a data set of character data might simply be stored as a vector of string data, such as:\n\njob_title = [\"Data Scientist\", \"Machine Learning Scientist\", \"Big Data Engineer\"]\n\n3-element Vector{String}:\n \"Data Scientist\"\n \"Machine Learning Scientist\"\n \"Big Data Engineer\"\n\n\n\n\nSymbols\nJulia as a language can be used to represent the language’s code as a data structure in the language. Symbols are needed to refer to the name, or identifier, of a variable as opposed to the values in the variable. Symbols, being part of the language, are used for other purposes, such as keys for a named tuple of flags for an argument. The access pattern nt.a has been mentioned; this is a convenience for getfield(nt, :a), the symbol being used as a key. When data frames are introduced – essentially a collection of matched data vectors – symbols will be used to reference the individual variables.\nThe simple constructor for a symbol is :, as in :some_symbol. (The : constructor makes expressions, but in this use, these are interpreted as symbols.) The Symbol constructor can also be used to create symbols with spaces, e.g., Symbol(\"some symbol\"); the string macro var\"...\" is a convenience.\nThe string function (or String constructor) will create a string from a symbol.\n\n\nFactors\nWhile character data is useful for representing data that is unique to each case (like an individual’s address), there are advantages to using a different representation for data with many anticipated repeated values (like an individual’s state). Factors are a data structure that allow the user to see full labels, but internally, the computer only sees an efficient pool of possible levels or values. That is, a factor is essentially a mapping between a label and a corresponding key, with a possible ordering assigned to the labels.\nFactors are not a built-in data type, but are provided by the CategoricalArrays package, which is loaded as other packages are:\n\nusing CategoricalArrays\n\nSome main tasks when working with such data are:\n\nreordering factor levels\nchanging the labels of factor levels\ncombining several levels into one\n\nWe use an example based on coffee sizes at your neighborhood Starbucks. An order consisted of \\(4\\) drinks with these sizes, put into a categorical array:\n\nx = categorical([\"Grande\", \"Venti\", \"Tall\", \"Venti\"], ordered=true)\n\n4-element CategoricalArray{String,1,UInt32}:\n \"Grande\"\n \"Venti\"\n \"Tall\"\n \"Venti\"\n\n\nThis appears to be like a character vector, but the type is different. First, let’s peek to see how values are internally stored. The user-visible values are stored with:\n\nx.pool\n\nCategoricalPool{String, UInt32}([\"Grande\", \"Tall\", \"Venti\"]) with ordered levels\n\n\nInternally, the computer sees:\n\nx.refs\n\n4-element Vector{UInt32}:\n 0x00000001\n 0x00000003\n 0x00000002\n 0x00000003\n\n\nThe levels are the labels assigned to the internal values. We can see that the levels are internally kept as 32-bit integers, which in general is more space efficient than storing the labels. The command levelcode.(x) will show the values using the more readable 64-bit integers.\nWorking with levels is the key difference. First levels, may be unordered (nominal) or ordered (ordinal), the latter indicated by specifying ordered=true to the constructor or by calling ordered!(x, true) for an unordered variable.\nThe example data has an odd order (call levels(x) to see), it coming from lexical sorting. So Grande is before Venti, despite the latter being 16 oz. and the former 20 oz (for hot drinks, 24oz for cold)16.\n\nx[1] < x[2]\n\ntrue\n\n\nTo reorder, we call the levels! function with the desired order:\n\nlevels!(x, [\"Tall\", \"Venti\", \"Grande\"])\nx[1] > x[2]\n\ntrue\n\n\nThe levels can be extended through assignment. For example, we might prefer to change the label “Tall” for a new label “Tall, 12oz”. We can readily do this for a single value through assignment:\n\nx[3] = \"Tall, 12oz\"\n\n\"Tall, 12oz\"\n\n\nAfter this command, there are 4 levels though only 3 used in the vector. To trim out extra levels, the droplevels! function can be used.\nIn general and for multiple replacements, the replace function is useful and the levels are adjusted accordingly:\n\nreplace!(x, \"Grande\" => \"Grande, 20oz\", \"Venti\" => \"Venti, 16oz\")\n\n4-element CategoricalArray{String,1,UInt32}:\n \"Grande, 20oz\"\n \"Venti, 16oz\"\n \"Tall, 12oz\"\n \"Venti, 16oz\"\n\n\nThe same command can be used to consolidate labels. For example, suppose we wanted to use just “Really big” and “regular sized” where we lump the two smaller ones together. It could be we just use replace with the right hand side using duplicates. Below we use broadcasting to illustrate how this might be done more easily were there many levels to combine. We do this in two lines, to be less confusing, but it could be written with just one:\n\nreplace!(x, ([\"Tall, 12oz\", \"Venti, 16oz\"] .=> \"regular sized\")...)\nreplace!(x, \"Grande, 20oz\" => \"Really big\")\nx\n\n4-element CategoricalArray{String,1,UInt32}:\n \"Really big\"\n \"regular sized\"\n \"regular sized\"\n \"regular sized\"\n\n\nCategorical data, like numeric data, can also be combined with vcat, say. If ordered, as much as possible, an order will attempt to be matched.\n\n\n\n2.2.3 Logical data and operators\nThe Boolean type in Julia has two values: true and false.\nBoolean values have their own algebra. The short-circuiting && and || implement “and” and “or:”\n\nfalse && false, true && false, false && true, true && true\n\n(false, false, false, true)\n\n\n\nfalse || false, true || false, false || true, true || true\n\n(false, true, true, true)\n\n\nThese are called “short circuiting,” as the right-hand side is only evaluated if it need be (as in true || false, the statement is known to be true after the left side of || is evaluated, so the right-hand side is not evaluated). This is used frequently for error messages, as the error is not called when the expression is true. These operations may be broadcasted, so the above might have been illustrated by\n\n[true false] .&& [true, false]  # produces a matrix to accommodate sizes\n\n2×2 BitMatrix:\n 1  0\n 0  0\n\n\nThe operations have left-to-right associativity, so it is common to see them in a sequence.\nMany operations promote Boolean values to 1 and 0. For example, true + false is 1, true * false is 0, and sum([true,false, true]) is 2; internally, the complex number \\(i\\) is internally a pair (false, true) indicating no real part and an imaginary part.\nBoolean values are returned by the comparison operators <, <=, ==, ===, >=, and >. These have the expected meaning, save == is a test of equality (not =, which is used for assignment) and === is a test of whether two values are identical. (E.g. for a vector x we have x == copy(x) is true, but x === copy(x) is false, as the latter does not point to the exact same container.) The symbol ! is used for negation.\nBoolean values can be used for indexing. Suppose inds is a vector of trues and falses with the same length as a vector x, then x[inds] will return those values from x where inds is true.\nTo create such Boolean vectors, the comparison operators are typically used combined with broadcasting. For example, the following redefines the whale dataset, then filters out only those values bigger than or equal to \\(200\\):\n\nwhale = [74, 122, 235, 111, 292, 111, 211, 133, 156, 79]\nwhale[ whale .>= 200 ]\n\n3-element Vector{Int64}:\n 235\n 292\n 211\n\n\nOr, this example – which shows the mathematically natural chaining of comparison operators – filters out only the values in \\([100, 125)\\):\n\nwhale[ 100 .<= whale .< 125 ]\n\n3-element Vector{Int64}:\n 122\n 111\n 111\n\n\n\nQuerying elements\nThere are other helper functions to query the elements of a Boolean vector.\n\nany(v) will return true if any of the elements of the Boolean vector v are true.\nall(v) will return true if all of the elements of the Boolean vector v are true.\nfindfirst(v) will return the index of the first true value in the Boolean vector v or return nothing if none is found. A predicate function can be used, as in findfirst(f, w) which effectively calls findfirst on f applied to each element of a vector w.\nfindlast(v) will return the index of the last true value in the Boolean vector v or return nothing if none is found. A predicate function can be used, as in findlast(f, w) which effectively calls findlast on f applied to each element of a vector w.\nfindnext(v, i) will find the first true value after index i in the Boolean vector v or return nothing if none is found. A predicate function can be used, as in findnext(f, w, i) which effectively calls findnext on f applied to each element of a vector w.\n\nIn general, x in v will check if the element x is in the vector v. The Unicode operator ∈ (\\in[tab]) can replace in.\n\n\n\n2.2.4 Date and time types\nJulia provides the built-in Dates module for working with date and time data. This module need not be installed, but is not loaded by default, so loading or importing it is needed to access the functionality.\n\nusing Dates\n\nConstructing a date is done with the Date constructor which expects a year, followed by an optional month and day. Date and time objects have the DateTime constructor. This example uses the vernal equinox, summer solstice, autumnal equinox, and winter solstice in the year 2022 for illustration:\n\nve, ss, ae, ws = Date(2022, 3, 20), Date(2022, 6, 21), Date(2022, 9, 22), Date(2022, 12, 21)\n\n(Date(\"2022-03-20\"), Date(\"2022-06-21\"), Date(\"2022-09-22\"), Date(\"2022-12-21\"))\n\n\nDates can also be parsed from a string. The following uses the default format;17:\n\nDate.([\"2022-03-20\", \"2022-06-21\", \"2022-09-22\", \"2022-12-21\"])\n\n4-element Vector{Date}:\n 2022-03-20\n 2022-06-21\n 2022-09-22\n 2022-12-21\n\n\nDate objects have the accessors year, month, and day:\n\nyear(ve), month(ve), day(ve)\n\n(2022, 3, 20)\n\n\nThe objects allow for natural operations, such as comparisons and differences:\n\nve < ss < ae < ws, ae - ve, ve + Day(93)\n\n(true, Day(186), Date(\"2022-06-21\"))\n\n\nThe difference is returned in the number of days. The last command shows the duration of 93 days can be constructed with Day and its value added to a Date object.\nThere are ways to query how a date falls within the calendar\n\ndayofyear(ve), dayofweek(ve), dayname(ve), dayofweekofmonth(ve)\n\n(79, 7, \"Sunday\", 3)\n\n\nThe last command indicating that the vernal equinox in 2022 fell on the third Sunday of the month.\n\n\n2.2.5 Structured data\nStructured data may not represent statistical data, but is useful nonetheless, e.g, for specifying the year of the counts in the whale data set.\nFor a vector of all ones or all zeros, the ones and zeros functions are useful. The command ones(n) will return a vector of n zeros using the default Float64 type. To specify a different type, such as Int64, the two-argument form, ones(T, n), is available. Similarly, zeros is used to create a vector of zeros. The singular one(), zero() (one one(T) and zero(T)) are useful for generic programming. For example, we use the idiom one.(x) to create a vector of all 1s with the length and type of the vector x.\nArithmetic sequences, \\(a, a+h, a+2h, \\dots, b\\) can be created with the colon operator a:h:b or a:b when h is 1. This operator returns a recipe for generating the sequence, it is lazy – it does not generate the sequence. The precedence is such that simple arithmetic operations do not need parentheses. That is a+1:b-1 represents the sequence \\(a+1, a+2, \\dots, b-1\\). Arithmetic sequences prove useful for indexing into a vector.\nThe colon operator for floating point values may or may not stop at b. Programming this is harder than it seems. The simple example of 1/10:1/10:3/10 should be \\(1/10, 2/10, 3/10\\), but it turns out that on the computer 1/10 + 2*1/10 is actually just larger than 3/10. See the value of 3/10 - (1/10 + 1/10 + 1/10) to investigate. However, the algorithm of : does produce the result with \\(3\\) values here.\n\n1/10:1/10:3/10 |> collect\n\n3-element Vector{Float64}:\n 0.1\n 0.2\n 0.3\n\n\nThe range function creates sequences. The common usage is range(start, stop, length). That is a:h:b specifies a step size, whereas the positional arguments of range specify the number of values between the starting and stopping values. Keyword arguments allow other combinations of start, stop, step, and length. The range returns a similar expression as the colon operator, it does not realize the entire range of values.\nScalar multiplication, scalar division, and addition of like-sized ranges are defined, as they return an arithmetic sequence.\nFor example, if whale holds beaching numbers for the years 2010 through 2019, we can get the odd years though the following:\n\noddyrs = 2011:2:2019\nwhale[oddyrs .- 2010 .+ 1] # 1-based offset is why we add 1\n\n5-element Vector{Int64}:\n 122\n 111\n 111\n 133\n  79\n\n\nThe fill function creates an array of a specific size, filling it with a value. For example, to create a vector of all 1s of length 10, fill(1, 10) will do so. A tuple is used for the second positional argument to construct higher-dimensional filled arrays.\nFor more complicated patterns, the repeat function can prove useful. For an array, it creates a new array comprised of repeats of the given array. For a vector, the only use here, two repeating patterns are often desired: repeating the whole vector several times; repeating each entry of a vector several times then combining.\n\nv = [1, 2, 3]\nrepeat(v, 3) |> show\n\n[1, 2, 3, 1, 2, 3, 1, 2, 3]\n\n\nIn the above, the entire vector is repeated three times. The 3 is passed to the outer argument. To repeat the 1s then the 2s and then the 3s the inner argument is used:\n\nrepeat(v, inner=3) |> show\n\n[1, 1, 1, 2, 2, 2, 3, 3, 3]"
  },
  {
    "objectID": "EDA/univariate-julia.html#sec-defining-functions",
    "href": "EDA/univariate-julia.html#sec-defining-functions",
    "title": "2  Univariate data",
    "section": "2.3 Functions",
    "text": "2.3 Functions\nJulia has a simple syntax for user-defined functions.\nFor simple functions, the syntax borrowed from common mathematics is used. For example, here we define a function to find the mad defined by the median of the transformed data \\(|x_i - M_x|\\).\n\nMAD(x) = median(abs.(x .- median(x)))\n\nMAD (generic function with 1 method)\n\n\nThe function is named MAD (to distinguish it from the already defined mad function in StatsBase) and, as written, accepts a vector and returns a summary number:\n\nMAD(whale)\n\n38.5\n\n\nFor functions which are not one liners, a pair of function-end keywords will define a block. For example, the following computes the fifth standardized moment (skewness and kurtosis related to the 3rd and 4th). The first line is one way to document a function in Julia.\n\n\"Compute 5th standardized momemt: m_5 / m_2^(5/2)\"\nfunction fifth_sm(x)\n    xbar, n = mean(x), length(x)\n    m5 = sum((xi - xbar)^5 for xi in x) / n\n    m2 = sum((xi - xbar)^2 for xi in x) / n\n    m5 / m2^(5/2)\nend\nfifth_sm(whale)\n\n3.6325544657722215\n\n\n(The above uses a generator, created by the use of for and in to loop over the different values of x rather than broadcasting.)\nThe repetition above in m5 and m2 could be avoided if we made a function to compute the sample moments about the mean which accepted both the data and a value for the exponent:\n\nsample_moment(x, n=2) = sum((xi - x)^n for xi in x) / length(x)\nfifth_sm(x) = sample_moment(x, 5) / sample_moment(x)^(5/2)\n\nfifth_sm (generic function with 1 method)\n\n\nThe variable n is in the second position and has a default value of 2 which is employed in the denominator of the above, where sample_moment is called with just a single argument.\nThere can be many positional arguments, only the last ones can have default values specified.\nFunctions can have a variable number of arguments. Here is a way to find the proportions of a set of numbers that is not stored in a container, but rather is passed to the function separated by commas:\n\nproportion(xs...) = collect(xs) / sum(xs)\n\nproportion (generic function with 1 method)\n\n\nThe splat syntax ... indicates a variable number of arguments in a function definition, and can be used to expand a list of arguments when used inside a function call. The use of collect, above, is needed above to generate a vector, as xs is passed to the body of the function as a tuple and tuples do not have division defined for them.18\nThe mad function from StatsBase has signature mad(x; center=median(x), normalize=true). This shows the use of keyword arguments. These have a default value, which, as illustrated, can depend on the data passed in. To call a function without the default, the keyword is typed, as in:\n\nmad(whale; center=mean(whale))\n\n74.13011092528009\n\n\n(We use a semicolon to separate positional arguments from keyword arguments, as that is needed to define a keyword, but commas can be used to call a keyword argument. What is important is that any keywords come last when a function is called.)\nFunctions, as defined above, are methods of a generic function. That is, there can be more than one method for a given name. (There are over 200 methods for the generic function named + in base Julia – and packages can extend this even more.) To direct or dispatch a call to the appropriate method, Julia considers the number and types of its positional arguments. That is, like +, functions can be defined differently for integers and floating point values.\nWe might like our MAD function to be more graceful than to throw a MethodError if a vector of strings is passed to it. A vector of strings has type Vector{String} so we could make a method just for that type:19 `Julia makes adding methods easy, but the types that are used to extend the function shouldn’t be owned by other packages, as this is considered type-piracy. (Failing to do so may prompt a request for a letter of marque.)\n\nMAD(x::Vector{String}) = print(\"Sorry, MAD is not defined for vectors of strings\")\nMAD([\"one\", \"three\", \"four\"])\n\nSorry, MAD is not defined for vectors of strings\n\n\n(There are many different types that one might wish to exclude and there are many tricks to efficiently code for this. It is common to define a default method which errors and then a special case for the types that can be worked with. Additionally, as types can be concrete, as the above, or abstract, it is possible to parameterize the type used for dispatch so that subtypes can be identified. For example, some operations return a SubString not a String. A vector of SubString will not match Vector{String} as even though both hold string data. The subtleties of parameterization are necessary to understand to write many special cases, but won’t be necessary in these notes.)\nA higher order function is one that takes one or more functions as an argument. As example is the calling style of findfirst(predicate, x) where predicate is a function which returns a boolean value. Higher-order functions are widely used with data wrangling. For such uses it is convenient to be able to define an anonymous function. These do not create methods for a generic function, as they have no name (though anonymous functions can be assigned a name).\nAnonymous functions are easily defined through the pattern: argument(s) -> body, where body, and, when multi-line, can be contained within begin/end blocks.\nFor example, to find the index of the first year that there were 200 or more whale beachings we have:\n\nfindfirst(x -> x >= 200, whale)\n\n3\n\n\nFor the tasks of logical comparisons, there are partially-applied versions of the operators that are essentially defined like anonymous functions (cf. Base.Fix2). In particular, >=(200) can be used for the anonymous function x -> x >= 200. To illustrate, here we see the index of first year where there were 200 or more beachings and the index of the last year, using ! to negate a call (even though >=(200) would be identical):\n\nfindfirst(>=(200), whale), findlast(!<(200), whale)\n\n(3, 7)\n\n\nAnother example would be to filter those values less than 100. The filter(predicate, v) function does this:\n\nfilter(<(100), whale)\n\n2-element Vector{Int64}:\n 74\n 79\n\n\n\nBroadcasting alternatives; iteration\nAs a quick illustration of a few other concepts, we consider alternatives to broadcasting that can prove useful. Consider the simplest case of broadcasting a function f over a single collection x.\nFor example here is broadcasting:\n\nx = [1, 4, 9]\nf(x) = sqrt(x)\nf.(x)\n\n3-element Vector{Float64}:\n 1.0\n 2.0\n 3.0\n\n\nThis use of broadcasting is also called the mapping of f over x. The map(f, x) function is defined:\n\nmap(f, x)\n\n3-element Vector{Float64}:\n 1.0\n 2.0\n 3.0\n\n\nEither broadcasting or map do the following: for each element of x apply the function f. This can be represented with the chaining operator, |>, adjusted to broadcast the values of x:\n\nx .|> f\n\n3-element Vector{Float64}:\n 1.0\n 2.0\n 3.0\n\n\nThe iteration over x can be made explicit with a for loop. The essential syntax would include:\n\nout = Any[]\nfor xi in x\n   push!(out, f(xi))\nend\nout\n\n3-element Vector{Any}:\n 1.0\n 2.0\n 3.0\n\n\nFor loops require extra effort for accumulation, which, in the above, required the selection of a container. We used Any[] to create a zero-length container that can hold any object and push values onto this. If would be preferable to have a concrete type, which in this case is just Float64[], but, in general, that requires some peeking into the output of f. This is to point out some background work performed by map and broadcasting that simplify other common tasks.\nThe for xi in x part of the for loop assigns xi to each iterate of x. There are some iterations that return more than one value at once, and it is common to see these destructured in the syntax. For example, enumerate is a helper function which takes an iterable object, like a vector, and iterates over both the index and the value:\n\nfor (i, xi) in enumerate(x)\n  print(\"element $i is $xi. \")\nend\n\nelement 1 is 1. element 2 is 4. element 3 is 9. \n\n\nA comprehension is a good alternative to a for loop when accumulation is required and the computation for each iterate is simple to express. Comprehensions have the syntax [ex for x in xs] and additional syntax for multiple iterations. The expression ex can use the variable x; xs is some iterable, such as a vector. For example, we might have this alternative to f.(x)\n\n[xi - mean(x) for xi in x]\n\n3-element Vector{Float64}:\n -3.666666666666667\n -0.666666666666667\n  4.333333333333333\n\n\nComprehensions use generators, which can also be used for many other functions, such as sum, which was previously illustrated. This form has the advantage of not needing to allocate temporary space to compute. For example sum([xi for xi in x]) would have to find space for the vector created by the comprehension, but the similar – and easier to type – sum(xi for xi in x) would not.\nIn this example, we sum the squared differences, passing an optional function to sum:\n\nf(x) = x^2\nsum(f, xi - mean(whale) for xi in whale)\n\n46020.399999999994\n\n\n\nVarious convenient iterators in Julia.\n\n\n\n\n\n\nIterator\nDescription\n\n\n\n\neachindex\niterate over each index\n\n\nvalues\niterate over value in a container\n\n\nenumerate\niterate over index and value in a container\n\n\nkeys\niterate over keys for a container\n\n\npairs\niterate over (key,value) pairs in a container\n\n\nzip\niterate over multiple iterators; the value is a tuple with an element from each\n\n\neachcol\nfor tabular data, iterate over the columns\n\n\neachrow\nfor tabular data, iterate over the rows"
  },
  {
    "objectID": "EDA/univariate-julia.html#sec-numeric-data",
    "href": "EDA/univariate-julia.html#sec-numeric-data",
    "title": "2  Univariate data",
    "section": "2.4 Numeric summaries",
    "text": "2.4 Numeric summaries\nThe StatsBase package provides numerous functions to compute numeric summaries of a univariate data set.\nFor measures of center we have the mean (average), median (middle value), and mode (most common value):\n\nmean(whale), median(whale), mode(whale), mean(trim(whale, prop=0.2))\n\n(152.4, 127.5, 111, 140.66666666666666)\n\n\nThe trimmed mean is computed by composing mean with the trim function which returns a generator producing the trimmed values (with proportion to trim specified to prop).\nFor measures of spread we have the standard deviation, the median absolute deviation, and the inter-quartile range (\\(Q_3 - Q_1\\)):\n\nstd(whale), mad(whale), iqr(whale)\n\n(71.5078861229849, 57.08018541246567, 86.25)\n\n\nThe quantile(x, p) function returns measures of position. Keywords alpha and beta can be used to adjust the algorithm employed. The 0 quantile is the minimum, the 1 quantile is the maximum, and 0.5 the median, or middle value. The \\(p\\)th quqntile is an interpolated value that aspires to split the data with \\(p\\cdot 100\\)% of the values to the left and \\((1-p)\\cdot 100\\)% of the values to the right. The p specified to quantile may be an iterable, useful to computing more than one at a time. Here we see that the type of p is used to compute the output type:\n\nquantile(whale, (0, 1//2, 1.0))\n\n(74, 255//2, 292.0)\n\n\nThe quantile function is used internally by StatsBase: the iqr is defined by the difference of quantile(x, [1/4, 3/4]); the summarystats function returns a summary of a data set, with the 5-number summary of the quantiles (p=0:1/4:1), the mean, the length, and the number of missing data values.\n\nsummarystats(whale)\n\nSummary Stats:\nLength:         10\nMissing Count:  0\nMean:           152.400000\nMinimum:        74.000000\n1st Quartile:   111.000000\nMedian:         127.500000\n3rd Quartile:   197.250000\nMaximum:        292.000000\n\n\nA measure of position gives a sense of how large a value is relative to the data set. Knowing a value is the 20th percentile, say, says it is larger than 20 percent of the data and smaller than 80 percent. The percent of data less or equal a value can be computed by sum(data .<= value) or sum(x <= value for x in data).\nThe \\(z\\)-score is a different measure of position. It measures differences from the mean on a scale of standard deviations. The \\(z\\) score is computed by (value - mean(data))/std(data) or for all values at once with:\n\nzs = (whale .- mean(whale)) / std(whale)\n\n10-element Vector{Float64}:\n -1.0963825705204249\n -0.4251279355079199\n  1.155117351084019\n -0.5789571226982856\n  1.9522322301613686\n -0.5789571226982856\n  0.8194900335777664\n -0.2712987483175542\n  0.05034409762593779\n -1.0264602127066222\n\n\nA rule of thumb, based on a certain bell-shaped distribution, is that values larger than 3 in absolute value are unusual, none of which are seen in this data.\nThe \\(z\\)-score is the typical form of standardization and is also supported directly through either of the following:\n\nzs1 = zscore(whale)\nzs2 = standardize(ZScoreTransform, Float64.(whale))\n\nzs == zs1 == zs2\n\ntrue\n\n\n\n\n\n\n\n\nWhy so many measures?\n\n\n\nThere are two main measures: those based on differences and those based on position.\nThe mean is defined by\n\\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i.\n\\]\nA property of the mean is when we center by the mean, i.e. take the transformation \\(y_i = x_i - \\bar{x}\\), then the mean of the \\(y_i\\) is just \\(0\\). Put another way, the mean is the point where the differences to the left and right of the mean even out when added.\nThe standard deviation, as a sense of scale, has the property that if we center and then scale by the standard deviation, the center will be \\(0\\) and the scale will be \\(1\\). That is, the \\(z\\)-scores have mean \\(0\\) (as we’ve centered) and standard deviation \\(1\\) (as we’ve scaled) – the \\(z\\) scores speak to the “shape” or distribution of values of a data set.\nThese measures are sensitive – or not resistant – to one or more outlying values. For example, the average wealth of people in a bar changes dramatically if someone like a pre-crash Elon Musk walks in. The standard deviation is similar. So measures based on position are better when data is skewed, especially if heavily skewed. For these, the median and IQR are not impacted greatly by one large value. That is they are resistant to outliers. The extreme values (the minimum and maximum) are less so. As such, the range of the data is a poor measure of spread, the range of the middle quartiles (the IQR) a much more resistant measure of spread.\n\n\n\nVarious measures of center, spread, and position in a univariate data set.\n\n\n\n\n\n\n\nMeasure\nType\nDescription\n\n\n\n\nmean\ncenter\nAverage value\n\n\nmedian\ncenter\nMiddle value\n\n\nmean ∘ trim\ncenter\nTrimmed mean\n\n\nvar\nspread\n“Average” squared distance from mean\n\n\nstd\nspread\nThe square of the variance\n\n\nmad\nspread\nThe median absolute deviation from center\n\n\nIQR\nspread\nRange of middle half of data\n\n\nextrema\nspread\nSmallest and largest values\n\n\nquantile\nposition\nValue where data would be split for give proportion\n\n\nsummarystats\nposition\nThe quartiles and extrema\n\n\nzscore\nposition\nStandardizing transformation"
  },
  {
    "objectID": "EDA/univariate-julia.html#sec-shape",
    "href": "EDA/univariate-julia.html#sec-shape",
    "title": "2  Univariate data",
    "section": "2.5 Shape",
    "text": "2.5 Shape\nThe five-number summary gives some insight into the shape of a distribution, in that it can show skewness. The skewness function numerically quantifies this:\n\nskewness(whale)\n\n0.7785957149991207\n\n\nPerfectly symmetric data has \\(0\\) skew.\nThe kurtosis function numerically summarizes if a distribution has long or short tails compared to a benchmark distribution, the normal:\n\nkurtosis(whale)\n\n-0.5833928499406711\n\n\nThese numeric summaries are helpful, but to best see shape of a distribution graphical summaries are suggested.\nA stem or stem-and-leaf plot is useful summary of a data set that can be computed easily by hand for modest-sized data. In short, numbers are coded in terms of a stem and a leaf. The stems are not repeated, so the data can be more compactly displayed. The data is organized so that the extrema, the median, and the shape of the data can be quickly gleaned.\nA stem-and-leaf plot function is not part of the StatsBase package.20 This one, below, is modified from a RosettaCode submission. It is simplified by assuming non-negative data. Essentially, it finds the right place to split a number into two values, a stem and a leaf. Then it displays each.\n\nusing Printf\nfunction stemleaf(a::Array{T,1}, leafsize=1) where {T<:Real}\n    ls = 10^floor(Int, log10(leafsize))\n    aa = floor.(Int, sort(a)/ls)\n    out = divrem.(aa, 10)\n    stem, leaf = first.(out), last.(out)\n    io = IOBuffer()\n    for i in stem[1]:stem[end]\n        i != 0 || println(io, \"\")\n        print(io, (@sprintf \"%10d | \" i))\n        println(io, join(map(string, leaf[stem .== i]), \"\"))\n    end\n    println(io, \" Leaf Unit = \" * string(convert(T, ls)))\n    String(take!(io))\nend\n\nstemleaf (generic function with 2 methods)\n\n\nFor the whale data, which is spread out over a wide range, we use a leaf size of 10:\n\nstemleaf(whale, 10) |> print\n\n\n         0 | 77\n         1 | 11235\n         2 | 139\n Leaf Unit = 10\n\n\nWe can identify 0|7 which is 0*100 + 70 or 70 as the “smallest” value. The actual smallest value is \\(74\\), but the data is rounded for this graphic so that just two digits (the stem and leaf) can represent all the values. Similarly 290 is identified as the largest values (which is actually 292).\nFor graphical summaries of data, we will introduce the StatsPlots package, which adds to the Plots package recipes for many statistical graphics. In later chapters we will use a different plotting package, but for now, the StatsPlots package provides easy to use and understand methods to produce the desired graphics.\n\nusing StatsPlots\n\nFor a univariate data set, graphics are used to help gauge the center, spread, and shape of a distribution of values. Different graphics have their advantages. In the following, we illustrate the dot plot, the box plot, the histogram, the density plot, and the quantile-normal plot. There are basic recipes for each graphic that require little more than specifying which data to use, though we do adjust a few default values through keyword arguments, such as legend.\nA dot plot shows the data on a number line, traditionally horizontally, and with circles (dots) representing each data point. For data with ties, like the whale data set, there are many techniques to disamgibuate the data in the graphic: the tied data may be stacked, using the \\(y\\) direction; the data may be jittered in the \\(x\\) direction, adding a small random to each value; the data may be jittered in the \\(y\\) direction. In StatsPlots, with the dotplot recipe, values are jittered when plotted vertically or when plotted with just the \\(x\\) values. Here we use the dotplot(x, y) method with x representing the data to display. For this, we manually jitter the x values and we make a vector of values, y all with the same value, in this case 1 as the one function makes this convenient:\n\njitter(x, scale=1/4) = x + randn(length(x)) * scale\n\np1 = dotplot(jitter(whale), one.(whale); legend=false);\n\nIn Figure 2.1 the graph appears on the left side.\nA dot plot shows all the data from which the reader can gauge the center, spread, and shape easily enough. For a small number of data points, such as with the whale data it is a very good graphic for assessing the distribution of values. However, for a large number of values, the graphic can get too crowded, and alternatives are useful.\nThe box plot or box-and-whisker plot of Tukey is one such excellent alternative.\nThe boxplot summarizes many aspects of a data set with basically just the five-number summary presented as a box with whiskers:\n\nThe center is shown by the middle line of the box set at the median\nThe spread is seen through the IQR, the length of the box, which is set at \\(Q_1\\) and \\(Q_3\\).\nThe shape (skewed or symmetric) can be gauged by how different the different quartiles are: the first quartile is from the smallest illustrated value to the box; the second the bottom half of the box; the third the top half of the box; and the fourth the top of the box to the largest value.\nOutliers are illustrated as dots and identified by a rule: any value more the \\(1.5\\) IQRs from \\(Q_3\\) or less than \\(1.5\\) IQRs from \\(Q_1\\) are not included in the whisker, but instead are illustrated with a dot. The whisker_range argument can be set to adjust that multiplier \\(1.5\\).\nThe range is seen through the whiskers or identified outliers.\n\nBox plots are excellent graphics for comparing several distributions, and usually appear vertically oriented in such usage, the default orientation. Here, with a single data set, we show how to use the orientation argument to specify a horizontal layout. For this boxplot, we also add in a dot plot with each dot colored by its respective quartile.\n\np2 = boxplot(whale; orientation=:horizontal, legend=false)\n\nQ1, Q2,Q3 = quantile(whale, (1/4, 1/2, 3/4))\nquantile_color(x) = x < Q1 ? \"red\" : x < Q2 ? \"yellow\" : x < Q3 ? \"blue\" : \"pink\"\nwhale′ = jitter(sort(whale))\ndotplot!(p2, whale′, 1/2 .+ zero.(whale′), color=quantile_color.(whale′));\n\nThe right-hand graphic in Figure 2.1 shows the boxplot. A careful comparison shows the interpolation used to identify the quartiles. This graphic has no identified outliers. A slight skew is seen, as the right half of the figure is longer than the left half, though this may be an artifact of sampling, and not a fact about the underlying population.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.1: The whale data summarized with a dot plot on the left and a box plot and accompanying dot plot on the right.\n\n\nA histogram is a another graphic well suited for displaying large data sets. It presents an illustration of what values are seen and also how prevalent that size value is. It does this by selecting a number of “bins” over the range of the data and counting the number of values in that bin, similar to what happens in a stem-and-leaf plot. This number is then represented using a bar with the property that that area of the bar is proportional to the frequency of data being in that bin. This proportion may just represent the count (if all the bins have the same length), or may be scaled so the total area is \\(1\\) (as these graphics suggest the shape of the underlying probability density function, which has total area \\(1\\)). The histogram function in StatsPlots offers still more means to scale the bars; adjustments specified through the normalize argument.\n\np1 = histogram(whale; bins = 5, legend=false)\n\ndotplot!(p1, jitter(whale), 1/5 .+ zero.(whale), markersize=10);\n\nThis graphic is illustrated in Figure 2.2. To make it, we adjusted the default number of bins through the bins argument, and added a dotplot with somewhat larger dots, to illustrate the data being counted. The height of each bar should reflect the number of data points within the bin associated to the bar.\nHistograms are great for seeing the center of the data (the balancing point of the graphic), the spread, and the shape. However, when the are many bins, the graphic gets busy. The key information being conveyed is the general shape of the top of each bar, not the vertical lines producing the bar. The stephist function avoids these lines, though the density plot is a more widely used alternative.\nA density plot also illustrates the distribution of values in a data set along with how they cluster, however, it smooths out the binning process. Density plots are scaled to have area \\(1\\), as they will be seen to suggest the shape of a probability density function. In Figure 2.2 we create a density plot overlaid on a histogram, which is normalized to have area \\(1\\):\n\np2 = stephist(whale; bins=5, normalize=:pdf, alpha=0.75, legend=false)\n\ndensity!(p2, whale; linewidth=5);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: The whale data summarized with a histogram on the left and a density plot on the right.\n\n\nA quantile-normal plot is a graphic to assess if a data set comes from a normal distribution, a reference bell-shaped distribution. The graphic is a scatter plot of a quantile of the data set with a corresponding quantile of the reference distribution.\n\nIf the data set has a roughly bell-shaped distribution with “typical” tails, then the dots will mostly fall on a straight line;\nif the data set is skewed one edge will deviate from the line;\nif the data set is symmetric, but has long tails these will show a deviations in both edges of the line.\n\nThe qqnorm function produces Figure 2.3:\n\nqqnorm(whale; legend=false)\n\n\n\n\nFigure 2.3: A quantile-normal plot of the whale data"
  },
  {
    "objectID": "EDA/tabular-data-julia.html#data-frames",
    "href": "EDA/tabular-data-julia.html#data-frames",
    "title": "3  Tabular data",
    "section": "3.1 Data frames",
    "text": "3.1 Data frames\nThe basic framework for exploratory data analysis is a data set with 1 or more observations for \\(n\\) different cases. For each case (or observation) a variable will be a length-\\(n\\) data set, with values measuring the same type of quantity, or perhaps missing. The variables can be stored as vectors. These are traditionally arranged in a rectangular manner with each row representing the measurements for a given case and each column representing the measured values for each variable. That is the columns are homogeneous (as they measure the same thing), but the rows need not be (as different variables are measuring different quantities).\nMatrices contain rectangular data, but are homogeneous. As such, a new structure is needed to store tabular data in general. A data frame is a concept borrowed from S Plus where they were introduced in 1991. The R language has data frames as a basic built-in data container. In Julia the structure is implemented in an external package, DataFrames.\n\nusing DataFrames\n\n\n\n\n\n\n\nAn OG package\n\n\n\nThe DataFrames package dates back to 2012, at least, and must be one of Julia’s oldest packages. Julia was on version 0.1 then. It is also under continual development and has a comprehensive set of documentation and tutorials. This introduction attempts to illustrate some basic usage; consult the provided documentation for additional details and applications.\n\n\n\n3.1.1 Data frame construction\nThere are different ways to construct a data frame.\nConsider the task of the Wirecutter in trying to select the best carry on travel bag. After compiling a list of possible models by scouring travel blogs etc., they select some criteria (capacity, compartment design, aesthetics, comfort, …) and compile data, similar to what one person collected in a spreadsheet. Here we create a much simplified spreadsheet for 3 listed bags with measurements of volume, price, laptop compatability, loading style, and a last-checked date – as this market improves constantly.\nproduct         v  p    l loads       checked\nGoruck GR2      40 395  Y front panel 2022-09\nMinaal 3.0      35 349  Y front panel 2022-09\nGenius          25 228  Y clamshell   2022-10\nWe see that product is a character, volume and price numeric, laptop compatability a Boolean value, load style one of a few levels, and the last checked date, a year-month date.\nWe create vectors to hold each. We load the CategoricalArrays and Dates packages for a few of the variables:\n\nusing CategoricalArrays, Dates\nproduct = [\"Goruck GR2\", \"Minaal 3.0\", \"Genius\"]\nvolume = [40, 35, 25]\nprice = [395, 349, 228]\nlaptop_compatability = categorical([\"Y\",\"Y\",\"Y\"])\nloading_style = categorical([\"front panel\", \"front panel\", \"clamshell\"])\ndate_checked = Date.(2022, [9,9,10])\n\n3-element Vector{Date}:\n 2022-09-01\n 2022-09-01\n 2022-10-01\n\n\nWith this, we use the DataFrame constructor to combine these into one data set.\n\nd = DataFrame(product = product, volume=volume, price=price,\n              var\"laptop compatability\"=laptop_compatability,\n              var\"loading style\"=loading_style, var\"date checked\"=date_checked)\n\n\n3×6 DataFrameRowproductvolumepricelaptop compatabilityloading styledate checkedStringInt64Int64Cat…Cat…Date1Goruck GR240395Yfront panel2022-09-012Minaal 3.035349Yfront panel2022-09-013Genius25228Yclamshell2022-10-01\n\n\nWe use var\"...\" for names with spaces.\nAs called above, the constructor binds vectors of equal length into a new multi-variate container.\nThe describe function will summarize a data frame using different summaries for different types:\n\ndescribe(d)\n\n\n6×7 DataFrameRowvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyAnyAnyInt64DataType1productGeniusMinaal 3.00String2volume33.33332535.0400Int643price324.0228349.03950Int644laptop compatabilityYY0CategoricalValue{String, UInt32}5loading styleclamshellfront panel0CategoricalValue{String, UInt32}6date checked2022-09-012022-09-012022-10-010Date\n\n\nIn the above construction, we repeated the names of the variables to the constructor. A style akin to the construction of named tuple, could also have been used where variables after a semicolon are implicitly assumed to have the desired name:\n\nd = DataFrame(; product, volume, price,\n              var\"laptop compatability\"=laptop_compatability,\n              var\"loading style\"=loading_style, var\"date checked\"=date_checked)\n\n\n3×6 DataFrameRowproductvolumepricelaptop compatabilityloading styledate checkedStringInt64Int64Cat…Cat…Date1Goruck GR240395Yfront panel2022-09-012Minaal 3.035349Yfront panel2022-09-013Genius25228Yclamshell2022-10-01\n\n\n(In fact, with the rename! function, or other conveniences, it can be more direct to define the data frame, then rename the columns, avoiding the var above.)\nWe see that the DataFrame type displays values as we might expect with the column header showing the name of the variable and the storage type. The row numbers are added and may be used for reference.\nThere is a convention for attaching new columns to a data frame that allows an alternate construction:\n\nd = DataFrame()   # empty data frame\nd.product = product\nd.volume = volume\nd.price = price\nd.\"laptop compatability\" = laptop_compatability\nd.\"loading style\" = loading_style\nd.\"date checked\" = date_checked\nd\n\n\n3×6 DataFrameRowproductvolumepricelaptop compatabilityloading styledate checkedStringInt64Int64Cat…Cat…Date1Goruck GR240395Yfront panel2022-09-012Minaal 3.035349Yfront panel2022-09-013Genius25228Yclamshell2022-10-01\n\n\nThis uses one of a few different ways to refer to a column in a data frame.\nAs an alternative, the insertcols! function can insert multiple columns, for example the last one in the example, could have been written insertcols!(d, \"date checked\" => date_checked).\nBefore exploring how we might query the data frame, we note that an alternate means to describe the data set is by row, or case. For each row, we might use a named tuple with the names indicating the variable, and the values the measurement. The DataFrame constructor would accept this, here we shorten the names and only do one row (for now):\n\nd = DataFrame([\n (b = \"Goruck GR2\", v = 40, p = 395, lap = \"Y\", load = \"front panel\", d = Date(\"2022-09-01\"))\n])\n\n\n1×6 DataFrameRowbvplaploaddStringInt64Int64StringStringDate1Goruck GR240395Yfront panel2022-09-01\n\n\nHere, the variable types are different, as it takes more effort to make one categorical. The same way we defined a column allows us to redefine that column (replacing the container, not re-using it for storage). For example, we could do:\n\nd.lap = categorical(d.lap)\nd.load = categorical(d.load)\n\n1-element CategoricalArray{String,1,UInt32}:\n \"front panel\"\n\n\nand then the two variables would be categorical.\nThere are other ways to define row by row:\n\nA data frame with empty variables can be constructed and then values as tuples may be push!ed onto the data frame\n\n\npush!(d,  (\"Minaal 3.0\", 35, 349, \"Y\", \"front panel\", Date(\"2022-09-01\")))\n\n\n2×6 DataFrameRowbvplaploaddStringInt64Int64Cat…Cat…Date1Goruck GR240395Yfront panel2022-09-012Minaal 3.035349Yfront panel2022-09-01\n\n\nA named tuple can also be used.\n\nSimilarly, rows specified by dictionaries may be pushed onto a data frame\n\n\npush!(d, Dict(:b => \"Genius\", :v => 25, :p => 228, :lap => \"Y\",\n              :load => \"clamshell\", :d => Date(\"2022-10-01\")))\n\n\n3×6 DataFrameRowbvplaploaddStringInt64Int64Cat…Cat…Date1Goruck GR240395Yfront panel2022-09-012Minaal 3.035349Yfront panel2022-09-013Genius25228Yclamshell2022-10-01\n\n\n(A dictionary is a key => value container like a named tuple, but keys may be arbitrary Julia objects – not always symbols – so we explicitly use symbols in the above command.)\n\n\n\n\n\n\nThe Tables interface\n\n\n\nIn the Julia ecosystem, the Tables package provides a lightweight means to describe tabular data, like a data frame, TableOperations provides some basic means to query the data. The different ways in which a data frame can be defined, reflect the different ways Tables objects can be created.\nAs tables are viewed as rows of homogeneous types, a vector of named tuples is a natural description, whereas tables are also naturally viewed as a collection of named columns, each column a vector. As such, both of these specify a table.\n\nAs a struct of arrays. This is used by calling DataFrame(a=[1,2], b= [2,1]), say.\nAs an array of structs. This is used by calling DataFrame([(a=1,b=2), (a=2,b=1)])\n\n\n\n\nRDataset\nData frames are often read in from external sources, and not typed in. The RDataSets package has collected data sets from many different R packages and bundled them into data sets for Julia in the form of data frames. We will use some of these data sets in the examples.\n\nusing RDatasets\ncars = dataset(\"MASS\", \"Cars93\")  # package, data set name\nfirst(cars, 2)  # first 2 rows only\n\n\n2×27 DataFrameRowManufacturerModelTypeMinPricePriceMaxPriceMPGCityMPGHighwayAirBagsDriveTrainCylindersEngineSizeHorsepowerRPMRevPerMileManTransAvailFuelTankCapacityPassengersLengthWheelbaseWidthTurnCircleRearSeatRoomLuggageRoomWeightOriginMakeCat…StringCat…Float64Float64Float64Int32Int32Cat…Cat…Cat…Float64Int32Int32Int32Cat…Float64Int32Int32Int32Int32Int32Float64?Int32?Int32Cat…String1AcuraIntegraSmall12.915.918.82531NoneFront41.814063002890Yes13.25177102683726.5112705non-USAAcura Integra2AcuraLegendMidsize29.233.938.71825Driver & PassengerFront63.220055002335Yes18.05195115713830.0153560non-USAAcura Legend\n\n\n\n\nCSV.read\nFor interacting with spreadsheet data, when it is stored like a data frame, it may be convenient to export the data in some format and then read that into a Julia session. A common format is to use comma-separated values, or “csv” data. The CSV package reads such files. The CSV package reads in delimited text data (the source) using the Tables interface and can be instructed to write to a data frame (the sink). The CSV.read file is used. Here we write to some file to show that we can read it back:\n\nusing CSV\nf = tempname() * \".csv\"\nCSV.write(f, d)  # write simple data frame to file `f`\nd1 = CSV.read(f, DataFrame)\n\n\n3×6 DataFrameRowbvplaploaddString15Int64Int64String1String15Date1Goruck GR240395Yfront panel2022-09-012Minaal 3.035349Yfront panel2022-09-013Genius25228Yclamshell2022-10-01\n\n\nComma-separated-value files are simple text files, with no metadata to indicate what type a value may be.1 Some guesswork is necessary. We observe that the two categorical variables were read back in as strings. As seen above, that can be addressed through column conversion.\nThe filename, may be more general. For example, it could be download(url) for some url that points to a csv file to read data from the internet.\n\n\n\n\n\n\nRead and write\n\n\n\nThe methods read and write are qualified in the above usage with the CSV module. In the Julia ecosystem, the FileIO package provides a common framework for reading and writing files; it uses the verbs load and save. This can also be used with DataFrames, though it works through the CSVFiles package – and not CSV, as illustrated above. The read command would look like DataFrame(load(fname)) and the write command like save(fname, df). Here fname would have a “.csv” extension so that the type of file could be detected.\n\n\n\nBasic usage to read/write .csv file into a data frame.\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nCSV.read(file_name, DataFrame)\nRead csv file from file with given name\n\n\nCSV.read(IOBuffer(string), DataFrame)\nRead csv file from a string using an IOBuffer\n\n\nCSV.read(download(url), DataFrame)\nRead csv file an url\n\n\nopen(file_name,\"w\") do io; CSV.write(io, dd); end\nWrite data frame df to csv file\n\n\nCSV.write(io, dd)\n\n\n\nend\n\n\n\nDataFrame(load(file_name))\nRead csv file from file with given name using CSVFiles\n\n\nsave(file_name, df)\nWrite data frame df to a csv file using CSVFiles\n\n\n\n\n\nTableScraper\nThe TableScraper package can scrape tables from an HTML file in a manner that can be passed to DataFrame for conversion to a data frame. The command DataFrame.(scrape_tables(url)) may just work to create the tables, but in practice, only well-formed tables can be scraped successfully; tables used for formatting purposes may fail. TableScraper is built on Cascadia and Gumbo, as is the Harbest package, which provides more access to the underlying data than TableScraper.\n\n\n\n3.1.2 Column names\nThe variable names of a data set are returned by the names function, as strings:\n\nnames(d)\n\n6-element Vector{String}:\n \"b\"\n \"v\"\n \"p\"\n \"lap\"\n \"load\"\n \"d\"\n\n\nThis function has a second argument which can be used to narrow or filter the names that are returned. For example, the following is a convenience for matching columns whose element type is a subtype of Real:\n\nnames(d, Real)\n\n2-element Vector{String}:\n \"v\"\n \"p\"\n\n\nThe rename! function allows the names to be changed in-place (without returning a new data frame). There are many different calling patterns, but:\n\nto replace one name, the form rename!(df, :v => :nv) is used, where :v is the old name, and :nv the new one. The form follows the generic replace! function. The pair notation can be broadcast (.=>) to replace more than one name, as with replace!(df, [:v1, :v2] .=> [:nv1, :nv2]).\nto replace allnames, the form rename!(df, [...]) is used, where the vector is of length matching the number of columns in df.\n\n\n\n3.1.3 Indexing and assignment\nThe values in a data frame can be referenced by a row number and column number, both 1-based. For example, the 2nd row and 3rd column of d can be seen to be 349 by observation\n\nd\n\n\n3×6 DataFrameRowbvplaploaddStringInt64Int64Cat…Cat…Date1Goruck GR240395Yfront panel2022-09-012Minaal 3.035349Yfront panel2022-09-013Genius25228Yclamshell2022-10-01\n\n\nbut it can be referenced through indexing (row number first, column number second):\n\nd[2, 3]\n\n349\n\n\nA new value can also be assigned with the same indexing reference. Suppose, that bag went on sale. This new price could be adjusted, as follows:\n\nd[2, 3] = 309 # a sale price\n\n309\n\n\n\nColumn selectors\nThe 3rd column has a name, p in this case. We can use a column selector to avoid having to know which column number a variable is. For this a string or a symbol may be used, as in:\n\nd[2, :p], d[2, \"p\"]\n\n(309, 309)\n\n\nMore generally, it is possible to use more than 1 column in column selection. For example, to get both the volume and price, for the second bag we have:\n\nd[2, [:v, :p]]\n\n\nDataFrameRow (2 columns)RowvpInt64Int64235309\n\n\nAs well, either [2, 3] or [\"v\", \"p\"] could have been used.\nFor data frames with many columns, a regular expression can be used. The column selector r\"^l\" would match all columns whose name begins with l (lap and load) in this case. (Regular expressions can be very complicated, but here we only assume that r\"name\" will match “name” somewhere in the string; r\"^name\" and r\"name$\" will match “name” at the beginning and ending of a string. Using a regular expression will return a data frame row (when a row index is specified) – not a value – as it is possible to return 0, 1 or more columns in the selection.\nColumn selectors, as seen may be column number(s), column names as strings, column names as symbols, and regular expressions.\nIn addition:\n\nBoolean vectors of a length matching the number of columns (d[2, [false, true, true, false, false, false]] would return the 2nd and 3rd columns.)\nThe value All() which selects all columns\nthe value Between(a, b) where, a and b are column selectors (such as Between(:v, 3)).\nNot(...) to select those columns not specified in ...\nCols(a,b,...) which combines different selectors, such as Cols(:v, 3).\n\n\n\nSlices\nThe All() column selector has a shortcut inherited from the indexing of arrays in base Julia, this being a colon, :. When used in the column position it refers to all the values in the row:\n\nd[2, :]\n\n\nDataFrameRow (6 columns)RowbvplaploaddStringInt64Int64Cat…Cat…Date2Minaal 3.035309Yfront panel2022-09-01\n\n\nWhen used in the row position, it refers to all rows:\n\nd[:, 3]\n\n3-element Vector{Int64}:\n 395\n 309\n 228\n\n\nIn the above use, a vector is returned, not a data frame, as only one column was selected. But which vector? Is it a copy of the one that d holds, or is it the same container? In base Julia the answer depends on what side of an equals sign the use is on (meaning, if used in assignment the answer is different, as mentioned later). In this usage, a copy is returned.\nTo get the underlying column (not a copy), the selector ! is used instead:\nd[!, 3]\nThe notation d.p also refers to the 3rd variable. In this case, d.p is a view of the underlying data (using !) and not a copy.\nThe notation d[:,:] will refer to all rows and all columns of d, and in this case will be a copy of the data frame.\n\n\nAssignment\nSingle values can be assigned, as previously illustrated. These values are assigned to the underlying vector in the data frame, so as with vectors, the assigned value will be promoted to the column type, if necessary. Assigning a value that can’t be promoted will throw and error, as it would were such an assignment tried for a vector. To do such an assignment, the underlying vector must be changed to a wider type.\nThe row selector : or ! had different semantics when accessing the underlying data, the first returning a copy, the second a view. When used with assignment though, : refers to the underlying vector or does in-place assignment; whereas ! (and the dot access) will replace the the underlying container with the assigned container.\nTo illustrate, suppose the prices are rounded down, but actually contain an extra 99 cents. We can reassign price as following:\n\nd[!, :p] = price .+ 0.99\n\n3-element Vector{Float64}:\n 395.99\n 349.99\n 228.99\n\n\nas even though price .+ 0.99 is no longer an integer vector, the ! row indicator will instruct d to point to this new vector. Had d[:, :p] = ... been used above, an error would have been thrown as the underlying container is integer, but the new prices require floating point values to represent them.\nBroadcast assignment (using .=) will be similar. The right hand side is expanded, and then assigned. Attempts to assign the wrong value type using : will error.\n\n\nMissing values\nTrying to assign a value of missing to a data frame requires the underlying vector to allow missing values. As with vectors, the allowmissing function is useful. For example, to give the 2nd bag a price of missing, we could do:\n\nallowmissing!(d, :p)\nd[2, :p] = missing\n\nmissing\n\n\nBy using the mutating form (with the trailing !), the data frame d is mutated in the allowmissing! call. This could be observed by looking a the underlying column type.\nThe new type is a union of the old type and Missing, so we could reassign the old value:\n\nd[2, :p] = 349.99\nd\n\n\n3×6 DataFrameRowbvplaploaddStringInt64Float64?Cat…Cat…Date1Goruck GR240395.99Yfront panel2022-09-012Minaal 3.035349.99Yfront panel2022-09-013Genius25228.99Yclamshell2022-10-01\n\n\n\n\nTables.jl interface\nA table, like a data frame can be expected to loop, or iterate, over columns, say to find which columns are numeric. Or, they can be expected to iterate over rows, say to find which values are within a certain range. What should be the case with a data frame? As there are reasons to prefer either, none is specified. Rather, there is a difference based on the calling function. Data frames respect the Tables interface, which has methods to iterate over rows or columns. When iterating over columns, the individual vectors are given; when iterating over rows, a DataFrameRow object is returned. Calling copy on these rows objects will return a named tuple.\nThe basic iterators are eachrow and eachcol. For example, to see the column type for a data frame, we can broadcast eltype over the variables returned by eachcol:\n\neltype.(eachcol(d))  |> permutedims\n\n1×6 Matrix{Type}:\n String  Int64  Union{Missing, Float64}  …  Date\n\n\nTo get the data frame as a vector of named tuples, then broadcasting NamedTuple over eachrow can be used: NamedTuple.(eachrow(d)); Base.copy is an alias. Tables.rowtable(d) will produce this vector of named tuples, as well.\n\n\n\n3.1.4 Sorting\nA data frame can be sorted by specifying a permutation of the indices, such as is returned by sortperm. For example, d[sortperm(d.price), :] will sort by price in increasing order. The keyword argument rev can be passed true to reverse the default sort order.\nThe generic sort function has a method for data frames, that offers a more convenient functionality. The sort! method does in-place sorting. The simplest call is to pass in a column to sort by, as in\n\nsort(d, :p)\n\n\n3×6 DataFrameRowbvplaploaddStringInt64Float64?Cat…Cat…Date1Genius25228.99Yclamshell2022-10-012Minaal 3.035349.99Yfront panel2022-09-013Goruck GR240395.99Yfront panel2022-09-01\n\n\nThis sorts by the price (:p) variable in increasing order. To sort by decreasing order a keyword argument rev=true can be specified:\n\nsort(d, \"v\"; rev=true)\n\n\n3×6 DataFrameRowbvplaploaddStringInt64Float64?Cat…Cat…Date1Goruck GR240395.99Yfront panel2022-09-012Minaal 3.035349.99Yfront panel2022-09-013Genius25228.99Yclamshell2022-10-01\n\n\nThere can be one or more column selectors. For this, the order function can be used to reverse just one of the columns, as in the following, which first sorts the dates in reverse order (newest first) and then within a data, uses lowest price first to sort:\n\nsort(d, [order(\"d\", rev=true), :p])\n\n\n3×6 DataFrameRowbvplaploaddStringInt64Float64?Cat…Cat…Date1Genius25228.99Yclamshell2022-10-012Minaal 3.035349.99Yfront panel2022-09-013Goruck GR240395.99Yfront panel2022-09-01\n\n\n\n\n3.1.5 Filtering rows\nFiltering, or subsetting, is a means to select a specified selection of rows.\nLet’s consider the cars data set, previously defined by loading dataset(\"MASS\", \"Cars93\"). This data set consists of 27 measurements on 93 cars from the 1990s. The data set comes from R’s MASS package, and is documented there.\nTwo ready ways to filter are by using specific indices, or by using Boolean indices generated by a logical comparison.\nFor the cars data set, the latter can be used to extract the Volkswagen models. To generate the indices we compare the :Manufacturer variable with the string Volkswagen to return a vector of length 93 that indicates if the manufacturer is VW. This is done with broadcasting: cars.Manufacturer .== \"Volkswagen\". We use the easy dot access to variables in cars, alternatively cars[:, :Manufacturer] (or, more efficiently, !, as is used by the dot access) could be used were :Manufacturer coming from some variable. With this row selector, we have:\n\ncars[cars.Manufacturer .== \"Volkswagen\", :]\n\n\n4×27 DataFrameRowManufacturerModelTypeMinPricePriceMaxPriceMPGCityMPGHighwayAirBagsDriveTrainCylindersEngineSizeHorsepowerRPMRevPerMileManTransAvailFuelTankCapacityPassengersLengthWheelbaseWidthTurnCircleRearSeatRoomLuggageRoomWeightOriginMakeCat…StringCat…Float64Float64Float64Int32Int32Cat…Cat…Cat…Float64Int32Int32Int32Cat…Float64Int32Int32Int32Int32Int32Float64?Int32?Int32Cat…String1VolkswagenFoxSmall8.79.19.52533NoneFront41.88155002550Yes12.4416393633426.0102240non-USAVolkswagen Fox2VolkswagenEurovanVan16.619.722.71721NoneFront52.510945002915Yes21.17187115723834.0missing3960non-USAVolkswagen Eurovan3VolkswagenPassatCompact17.620.022.42130NoneFront42.013458002685Yes18.55180103673531.5142985non-USAVolkswagen Passat4VolkswagenCorradoSporty22.923.323.71825NoneFront62.817858002385Yes18.5415997663626.0152810non-USAVolkswagen Corrado\n\n\nThis approach lends itself to the description “find all rows matching some value” then “extract the identified rows,” – written as two steps to emphasize there are two passes through the data. Another mental model would be loop over the rows, and keep those that match the query. This is done generically by the filter function for collections in Julia or by the subset function of DataFrames.\nThe filter(predicate, collection) function is used to identify just the values in the collection for which the predicate function returns true. When a data frame is used with filter, the iteration is over the rows, so the wrapping eachrow iterator is not needed. We need a predicate function to replace the .== above. One follows. It doesn’t need .==, as r is a data frame row and access produces a value not a vector:\n\nfilter(r -> r.Manufacturer == \"Volkswagen\", cars)\n\n\n4×27 DataFrameRowManufacturerModelTypeMinPricePriceMaxPriceMPGCityMPGHighwayAirBagsDriveTrainCylindersEngineSizeHorsepowerRPMRevPerMileManTransAvailFuelTankCapacityPassengersLengthWheelbaseWidthTurnCircleRearSeatRoomLuggageRoomWeightOriginMakeCat…StringCat…Float64Float64Float64Int32Int32Cat…Cat…Cat…Float64Int32Int32Int32Cat…Float64Int32Int32Int32Int32Int32Float64?Int32?Int32Cat…String1VolkswagenFoxSmall8.79.19.52533NoneFront41.88155002550Yes12.4416393633426.0102240non-USAVolkswagen Fox2VolkswagenEurovanVan16.619.722.71721NoneFront52.510945002915Yes21.17187115723834.0missing3960non-USAVolkswagen Eurovan3VolkswagenPassatCompact17.620.022.42130NoneFront42.013458002685Yes18.55180103673531.5142985non-USAVolkswagen Passat4VolkswagenCorradoSporty22.923.323.71825NoneFront62.817858002385Yes18.5415997663626.0152810non-USAVolkswagen Corrado\n\n\nThere are some conveniences that have been developed to make the predicate functions even easier to write, which will be introduced incrementally:\nThe basic binary comparisons, like ==, are defined by functions essentially of the form (a,b) -> ==(a,b). There are partially applied versions, essentially a -> ==(a,b), with b applied, formed by ==(b). So the syntax ==(\"Volkswagen\") is a predicate taking a single argument which is then compared for equality to the string \"Volkswagen\".\nHowever, r is a data frame row, what we need to pass to ==(\"Volkswagen\") is the value of the :Manufacturer column. For this another convenience is available.\nThe pair syntax, => of the form column selector(s) => predicate function will pass just the value(s) in the column(s) to the predicate function. Combined, the above can be simplified to:\n\nfilter(:Manufacturer => ==(\"Volkswagen\"), cars)\n\n\n4×27 DataFrameRowManufacturerModelTypeMinPricePriceMaxPriceMPGCityMPGHighwayAirBagsDriveTrainCylindersEngineSizeHorsepowerRPMRevPerMileManTransAvailFuelTankCapacityPassengersLengthWheelbaseWidthTurnCircleRearSeatRoomLuggageRoomWeightOriginMakeCat…StringCat…Float64Float64Float64Int32Int32Cat…Cat…Cat…Float64Int32Int32Int32Cat…Float64Int32Int32Int32Int32Int32Float64?Int32?Int32Cat…String1VolkswagenFoxSmall8.79.19.52533NoneFront41.88155002550Yes12.4416393633426.0102240non-USAVolkswagen Fox2VolkswagenEurovanVan16.619.722.71721NoneFront52.510945002915Yes21.17187115723834.0missing3960non-USAVolkswagen Eurovan3VolkswagenPassatCompact17.620.022.42130NoneFront42.013458002685Yes18.55180103673531.5142985non-USAVolkswagen Passat4VolkswagenCorradoSporty22.923.323.71825NoneFront62.817858002385Yes18.5415997663626.0152810non-USAVolkswagen Corrado\n\n\nIt doesn’t save much for this simple example, but this specification style is the basis of the DataFrames mini language, which provides a standardized and performant means of wrangling a data frame.\nFiltering may be done on one or more values. There are different approaches.\nFor example, to first filter by the manufacturer, then by city mileage, we might write a more complicated predicate function using && to combine Boolean values:\n\npred(r) = r.Manufacturer == \"Volkswagen\" && r.MPGCity >= 20\nfilter(pred, cars)\n\n\n2×27 DataFrameRowManufacturerModelTypeMinPricePriceMaxPriceMPGCityMPGHighwayAirBagsDriveTrainCylindersEngineSizeHorsepowerRPMRevPerMileManTransAvailFuelTankCapacityPassengersLengthWheelbaseWidthTurnCircleRearSeatRoomLuggageRoomWeightOriginMakeCat…StringCat…Float64Float64Float64Int32Int32Cat…Cat…Cat…Float64Int32Int32Int32Cat…Float64Int32Int32Int32Int32Int32Float64?Int32?Int32Cat…String1VolkswagenFoxSmall8.79.19.52533NoneFront41.88155002550Yes12.4416393633426.0102240non-USAVolkswagen Fox2VolkswagenPassatCompact17.620.022.42130NoneFront42.013458002685Yes18.55180103673531.5142985non-USAVolkswagen Passat\n\n\nThe mini language can also be used. On the left hand side of (the first) => a vector of column selectors may be indicated, in which the predicate function (on the right hand side of the first =>) will get multiple arguments that can be used to implement the logic:\n\npred(m, mpg) = m == \"Volkswagen\" && mpg >= 20\nfilter([:Manufacturer, :MPGCity] => pred, cars)\n\n\n2×27 DataFrameRowManufacturerModelTypeMinPricePriceMaxPriceMPGCityMPGHighwayAirBagsDriveTrainCylindersEngineSizeHorsepowerRPMRevPerMileManTransAvailFuelTankCapacityPassengersLengthWheelbaseWidthTurnCircleRearSeatRoomLuggageRoomWeightOriginMakeCat…StringCat…Float64Float64Float64Int32Int32Cat…Cat…Cat…Float64Int32Int32Int32Cat…Float64Int32Int32Int32Int32Int32Float64?Int32?Int32Cat…String1VolkswagenFoxSmall8.79.19.52533NoneFront41.88155002550Yes12.4416393633426.0102240non-USAVolkswagen Fox2VolkswagenPassatCompact17.620.022.42130NoneFront42.013458002685Yes18.55180103673531.5142985non-USAVolkswagen Passat\n\n\nFinally, we could use filter twice:\n\ncars1 = filter(:Manufacturer => ==(\"Volkswagen\"), cars)\ncars2 = filter(:MPGCity => >=(20), cars1)\n\n\n2×27 DataFrameRowManufacturerModelTypeMinPricePriceMaxPriceMPGCityMPGHighwayAirBagsDriveTrainCylindersEngineSizeHorsepowerRPMRevPerMileManTransAvailFuelTankCapacityPassengersLengthWheelbaseWidthTurnCircleRearSeatRoomLuggageRoomWeightOriginMakeCat…StringCat…Float64Float64Float64Int32Int32Cat…Cat…Cat…Float64Int32Int32Int32Cat…Float64Int32Int32Int32Int32Int32Float64?Int32?Int32Cat…String1VolkswagenFoxSmall8.79.19.52533NoneFront41.88155002550Yes12.4416393633426.0102240non-USAVolkswagen Fox2VolkswagenPassatCompact17.620.022.42130NoneFront42.013458002685Yes18.55180103673531.5142985non-USAVolkswagen Passat\n\n\nThe above required the introduction of an intermediate data frame to store the result of the first filter call to pass to the second. This threading through of the modified data is quite common in processing pipelines. The first two approaches with complicated predicate functions can grow unwieldly, so staged modification is common. To support that, the chaining or piping operation (|>) is often used:\n\nfilter(:Manufacturer => ==(\"Volkswagen\"), cars) |>\n    d -> filter(:MPGCity => >=(20), d)\n\n\n2×27 DataFrameRowManufacturerModelTypeMinPricePriceMaxPriceMPGCityMPGHighwayAirBagsDriveTrainCylindersEngineSizeHorsepowerRPMRevPerMileManTransAvailFuelTankCapacityPassengersLengthWheelbaseWidthTurnCircleRearSeatRoomLuggageRoomWeightOriginMakeCat…StringCat…Float64Float64Float64Int32Int32Cat…Cat…Cat…Float64Int32Int32Int32Cat…Float64Int32Int32Int32Int32Int32Float64?Int32?Int32Cat…String1VolkswagenFoxSmall8.79.19.52533NoneFront41.88155002550Yes12.4416393633426.0102240non-USAVolkswagen Fox2VolkswagenPassatCompact17.620.022.42130NoneFront42.013458002685Yes18.55180103673531.5142985non-USAVolkswagen Passat\n\n\nThe right-hand side of |> expects a function for application, so an anonymous function is created. There are macros available in user-contributed packages that shorten this pattern by using a placeholder.\nA popular one is, @chain, in the Chain package, with its basic usage based on two simple principles:\n\na new line is implicitly a pipe\na _ receives the argument from the pipe. If none is given, the first argument does\n\nFor example,\n\nusing Chain\n\n\n@chain cars begin\n    filter(:Manufacturer => ==(\"Volkswagen\"), _)\n    filter(:MPGCity => >=(20), _)\nend\n\n\n2×27 DataFrameRowManufacturerModelTypeMinPricePriceMaxPriceMPGCityMPGHighwayAirBagsDriveTrainCylindersEngineSizeHorsepowerRPMRevPerMileManTransAvailFuelTankCapacityPassengersLengthWheelbaseWidthTurnCircleRearSeatRoomLuggageRoomWeightOriginMakeCat…StringCat…Float64Float64Float64Int32Int32Cat…Cat…Cat…Float64Int32Int32Int32Cat…Float64Int32Int32Int32Int32Int32Float64?Int32?Int32Cat…String1VolkswagenFoxSmall8.79.19.52533NoneFront41.88155002550Yes12.4416393633426.0102240non-USAVolkswagen Fox2VolkswagenPassatCompact17.620.022.42130NoneFront42.013458002685Yes18.55180103673531.5142985non-USAVolkswagen Passat\n\n\n\nSubset\nThe calling style filter(predicate, collection) is consistently used with several higher-order functions, for example, map and reduce and is supported by Julia’s do syntax.\nHowever, with data frames, the convention is to have action functions expect the data frame in the first position.\nThe subset function also returns a filtered copy of the data frame with rows matching a certain selection criteria. It’s usage is somewhat similar to filter with the order of its arguments reversed, but the predicate must return a vector of indices, so broadcasting or the ByRow function may be necessary.\nFor example, these produce the same data frame:\n\nd1 = subset(cars, :Manufacturer => m -> m .== \"Volkswagen\") # use .== not just ==\nd2 = subset(cars, :Manufacturer => ByRow(==(\"Volkswagen\"))) # use `ByRow` to ensure predicate is applied to each\n\n\n4×27 DataFrameRowManufacturerModelTypeMinPricePriceMaxPriceMPGCityMPGHighwayAirBagsDriveTrainCylindersEngineSizeHorsepowerRPMRevPerMileManTransAvailFuelTankCapacityPassengersLengthWheelbaseWidthTurnCircleRearSeatRoomLuggageRoomWeightOriginMakeCat…StringCat…Float64Float64Float64Int32Int32Cat…Cat…Cat…Float64Int32Int32Int32Cat…Float64Int32Int32Int32Int32Int32Float64?Int32?Int32Cat…String1VolkswagenFoxSmall8.79.19.52533NoneFront41.88155002550Yes12.4416393633426.0102240non-USAVolkswagen Fox2VolkswagenEurovanVan16.619.722.71721NoneFront52.510945002915Yes21.17187115723834.0missing3960non-USAVolkswagen Eurovan3VolkswagenPassatCompact17.620.022.42130NoneFront42.013458002685Yes18.55180103673531.5142985non-USAVolkswagen Passat4VolkswagenCorradoSporty22.923.323.71825NoneFront62.817858002385Yes18.5415997663626.0152810non-USAVolkswagen Corrado\n\n\nUnlike filter, subset allows multiple arguments to be applied:\n\nsubset(cars,\n       :Manufacturer => ByRow(==(\"Volkswagen\")),\n       :MPGCity      => ByRow(>=(20)))\n\n\n2×27 DataFrameRowManufacturerModelTypeMinPricePriceMaxPriceMPGCityMPGHighwayAirBagsDriveTrainCylindersEngineSizeHorsepowerRPMRevPerMileManTransAvailFuelTankCapacityPassengersLengthWheelbaseWidthTurnCircleRearSeatRoomLuggageRoomWeightOriginMakeCat…StringCat…Float64Float64Float64Int32Int32Cat…Cat…Cat…Float64Int32Int32Int32Cat…Float64Int32Int32Int32Int32Int32Float64?Int32?Int32Cat…String1VolkswagenFoxSmall8.79.19.52533NoneFront41.88155002550Yes12.4416393633426.0102240non-USAVolkswagen Fox2VolkswagenPassatCompact17.620.022.42130NoneFront42.013458002685Yes18.55180103673531.5142985non-USAVolkswagen Passat\n\n\nThe mini language uses a default of identity for the function so, if the columns are Boolean, they can be used to subset the data:\n\ndd = DataFrame(a = [true, false], b = [true, missing])\nsubset(dd, :a)  # just first row\n\n\n1×2 DataFrameRowabBoolBool?1truetrue\n\n\nFor data with missing values, like in the :b column of dd, the comparison operators implement 3-valued logic, meaning the response can be true, false, or missing. Using subset(dd, :b) above will error. The keyword skipmissing can be passed true to have these skipped over. The default is false.\n\nsubset(dd, :b; skipmissing=true)\n\n\n1×2 DataFrameRowabBoolBool?1truetrue\n\n\nThe dropmissing function filters out each row that has a missing value.\n\n\n\n3.1.6 Selecting columns; transforming data\nFiltering of a data frame can be viewed as accessing the data in df with a boolean set of row indices, as in df[inds, :]. (There are flags to have a “view” of the data, as in df[inds, !]). Filtering returns a data frame with a possibly reduced number of rows, and the same number of columns.\nThe select function is related in that it can be viewed as passing a Boolean vector of column indices to select specific columns or variables, as in df[:, inds]. The syntax of select also extends this allowing transformations of the data and reassignment, as could be achieved through this pattern: df.new_variable = f(other_variables_in_df).\nThe select function will return a data frame with the same number of rows (cases) as the original data frame. The variables returned are selected using column selectors. There is support for the DataFrames mini language.\nFirst, we can select on or more columns by separating column selectors with commas:\nHere, using the backpack data in the variable d, we select the first column by column number, the second using a symbol, the third by a string, and the fourth and fifth using a regular expression:2\n\nselect(d, 1, :v, \"p\", r\"^l\")\n\n\n3×5 DataFrameRowbvplaploadStringInt64Float64?Cat…Cat…1Goruck GR240395.99Yfront panel2Minaal 3.035349.99Yfront panel3Genius25228.99Yclamshell\n\n\nOne use of select is to rearrange the column order, say by moving a column to the front. The : or All() column selector will add the rest. That is this command would move “d” to the front of the other columns: select(d, :d, All()).\nSelection also allows an easy renaming of column names, using the pairs notation column name => new column name:\n\nselect(d, :v => :volume, \"p\" => \"price\")\n\n\n3×2 DataFrameRowvolumepriceInt64Float64?140395.99235349.99325228.99\n\n\nThis usage is a supported abbreviation of source_column => function => target_column_name, where the function is identity. Here the function is not necessarily a predicate function, as it is with subset, but rather a function which returns a vector of the same length as the number of columns in the data frame (as select always returns the same number of columns as the data frame it is passed.)\nThe function receives column vectors and should return a column vector. For a scalar function, it must be applied to each entry of the column vector, that is, each row. Broadcasting is one manner to do this. As used previously, the DataFrames package provides ByRow to also apply a function to each row of the column vector(s).\nFor example to compute a ratio of price to volume, we might have:\n\nselect(d, [:p, :v] => ByRow((p,v) -> p/v) => \"price to volume\")\n\n\n3×1 DataFrameRowprice to volumeFloat6419.8997529.9997139.1596\n\n\nThe above would be equivalent to select(d, [:p, :v] => ((p,v) -> p./v) => \"price to volume\") (broadcasted p ./ v and parentheses), but not select(d, [:p, :v] => (p,v) -> p./v => \"price to volume\"), as the precedence rules for => do not parse the expressions identically. That is, the use of parentheses around an anonymous function is needed.\nThe specification of a target column name is not necessary, as one will be generated. In this particular example, it would be :p_v_function. For named functions, the automatic namings are a bit more informative, as the method name replaces the “function” part. The renamecols keyword argument can be set to false to drop the addition of a function name, which in this case would leavel :p_v as the name. For a single-column selector it would not rename the column, rather replace it.\nThe above reduced the number of columns to just that specified. The : selector will select all columns (as it does with indexing) and return them:\n\nselect(d, [:p, :v] => ByRow((p,v) -> p/v) => \"price to volume\", :) # return all columns\n\n\n3×7 DataFrameRowprice to volumebvplaploaddFloat64StringInt64Float64?Cat…Cat…Date19.89975Goruck GR240395.99Yfront panel2022-09-0129.99971Minaal 3.035349.99Yfront panel2022-09-0139.1596Genius25228.99Yclamshell2022-10-01\n\n\nAs seen in this last example, the source column selectors can select more than one column. The function receives multiple arguments. It is possible that there be multiple target columns. For example, we could compute both price-to-volume and volume-to-price quantities. A function to do so would be\n\npv_and_vp(p,v) = [p/v, v/p]\n\npv_and_vp (generic function with 1 method)\n\n\nWhen we use this, we see the two values are stored in one column.\n\nselect(d, [:p, :v] => ByRow(pv_and_vp))\n\n\n3×1 DataFrameRowp_v_pv_and_vpArray…1[9.89975, 0.101013]2[9.99971, 0.100003]3[9.1596, 0.109175]\n\n\nTo split these up, we can give two names:\n\nselect(d, [:p, :v] => ByRow(pv_and_vp) => [:pv, :vp])\n\n\n3×2 DataFrameRowpvvpFloat64Float6419.899750.10101329.999710.10000339.15960.109175\n\n\nThe AsTable function has two meanings in the mini language. If on the target side, it assumes the output of the function is a vector of containers (in this example the vectors [p/v, v/p]) that should be expanded into multiple columns. It uses the keys (were the containers named tuples) or, in this case generates them, to create multiple columns in the destination:\n\nselect(d, [:p, :v] => ByRow(pv_and_vp) => AsTable)\n\n\n3×2 DataFrameRowx1x2Float64Float6419.899750.10101329.999710.10000339.15960.109175\n\n\nHad we used a named tuple in our function, pv_and_vp(p, v) = (pv = p/v, vp = v/p), then thecolumn names would come from tuples names and need not be generated.\nWhen AsTable is used on the source columns, as in AsTable([:p,:v]) then the function receives a named tuple with column vector entries. (For this, pv_and_vp would be written pv_and_vp(r) = [r.p/r.v, r.v/r.p], with r representing a row in a two-column table with names :p and :v.\n\nTransform\nExtending the columns in the data frame by select is common enough that the function transform is supplied which always keeps the columns of the original data frame, though they can also be modified through the mini language. The use of transfrom is equivalent to select(df, :, args...).\n\n\n\n\n\n\nThe DataFrames’ mini language\n\n\n\nThe mini language is well documented in the documentation string of transform and the blog post of Bogumil Kasminski “DataFrames.jl minilanguage explained.”\n\n\n\n\n\n3.1.7 Combine\nThe combine function creates a new data frame whose columns are the result of transformations applied to the source data frame. The name will be more clear after we discuss grouping or splitting of a data set.\nA typical usage of combine is to apply some reduction to the data, for example finding an average.\nIn this example, we use the mean function from the StatsBase package and apply that to the :MPGCity measurements in the cars data:\n\ncombine(cars, :MPGCity => mean)\n\n\n1×1 DataFrameRowMPGCity_meanFloat64122.3656\n\n\n(A second pair can be used to specify a name for the target, as in :MPGCity => mean => :AvgMPGCity)\nThere are several numeric variables in this data set, we may wish to find the mean of more than 1 at a time. For this, we can broadcast the columns via .=>, as in this example:\n\ncombine(cars, [:MPGCity, :MPGHighway] .=> mean)\n\n\n1×2 DataFrameRowMPGCity_meanMPGHighway_meanFloat64Float64122.365629.086\n\n\n(That is an alternate to combine(cars, :MPGCity => mean, :MPGHighway => mean).)\nBroadcasting => lends itself to succinctly applying a function to multiple columns.\nFor example, to apply mean to all the numeric values, we might select them first (using the filtering feature of names), then pass to the mini language, as follows:\n\nnms = names(cars, Real)\ncombine(cars, nms .=> mean; renamecols=false)\n\n\n1×16 DataFrameRowMinPricePriceMaxPriceMPGCityMPGHighwayEngineSizeHorsepowerRPMRevPerMileFuelTankCapacityPassengersLengthWheelbaseWidthTurnCircleWeightFloat64Float64Float64Float64Float64Float64Float64Float64Float64Float64Float64Float64Float64Float64Float64Float64117.125819.509721.898922.365629.0862.66774143.8285280.652332.216.66455.08602183.204103.94669.376338.9573072.9\n\n\nMore than one transformation at a time is possible, as already seen in the mini language. With combine the number of rows is determined by the output of the transformation. In this example, the mean reduces to a number, but unique reduces :Origin to the number of levels. The result of mean is repeated, it is not the mean for each group (a task we demonstrate shortly):\n\ncombine(cars, :MPGCity => mean, :Origin => unique)\n\n\n2×2 DataFrameRowMPGCity_meanOrigin_uniqueFloat64String122.3656non-USA222.3656USA\n\n\n\n\n3.1.8 Flatten\nAs mentioned, the repeat function is used to repeat values in a vector, or other iterable. The simplest usage looks like this:\n\nrepeat([1,2,3], 2) |> show\n\n[1, 2, 3, 1, 2, 3]\n\n\nA common use of repetition is to take two variables, say x and y, perhaps of unequal length and create a data frame with two columns: one to store the values and one to indicate if a value came from x or y. This could be achieved like\n\nx = [1,2,3]\ny = [4, 5]\n\nDataFrame(variable=vcat(repeat([:x], length(x)), repeat([:y], length(y))),\n          values = vcat(x,y))\n\n\n5×2 DataFrameRowvariablevaluesSymbolInt641x12x23x34y45y5\n\n\nThe flatten function offers an alternative. Consider the data frame:\n\nx = [1,2,3]\ny = [4, 5]\ndxy = DataFrame(variable=[:x,:y], value=[x, y])\n\n\n2×2 DataFrameRowvariablevalueSymbolArray…1x[1, 2, 3]2y[4, 5]\n\n\nThe flatten function will iterate over the indicated columns and repeat the other variables:\n\nflatten(dxy, :value)\n\n\n5×2 DataFrameRowvariablevalueSymbolInt641x12x23x34y45y5\n\n\n\n\n3.1.9 SplitApplyCombine\nThe split-apply-combine strategy for wrangling data frames was popularized in the influential paper “The Split-Apply-Combine Strategy for Data Analysis: (Wickham 2011) by H. Wickham which introduces this description:”where you break up a big problem into manageable pieces, operate on each piece independently and then put all the pieces back together.”\nThe groupby function for data frames does the breaking up, the combine function can put things together again, and the DataFrames mini language can operate on each piece.\nThe groupby function splits a data frame into pieces of type GroupedDataFrame. The basic signature is groupby(df, cols; sort=nothing, skipmissing=false). The cols are one or more column selectors. The value of sort defaults to nothing leaving the order of the groups in the result undefined, but the grouping uses the fastest identified algorithm. The skipmissing argument can instruct the skipping of rows which have missing values in the selected columns.\nFor example, grouping by loading style, returns two sub data frames.\n\ngdf = groupby(d, :load)\n\n\nGroupedDataFrame with 2 groups based on key: loadFirst Group (2 rows): load = CategoricalValue{String, UInt32} \"front panel\"RowbvplaploaddStringInt64Float64?Cat…Cat…Date1Goruck GR240395.99Yfront panel2022-09-012Minaal 3.035349.99Yfront panel2022-09-01⋮Last Group (1 row): load = CategoricalValue{String, UInt32} \"clamshell\"RowbvplaploaddStringInt64Float64?Cat…Cat…Date1Genius25228.99Yclamshell2022-10-01\n\n\nIndices may be used to get each sub data frame. Moreover, the keys method returns keys, like indices, to efficiently look up the different sub data frames in gdf. The levels for the keys are found in the property .load, as with keys(gdf)[1].load. Grouped data frames may be iterated over; the pairs iterator iterates over both keys and values.\nThe select and subset functions work over GroupedDataFrame objects, here we see the returned values are combined:\n\nsubset(gdf, :p => ByRow(<=(350)))\n\n\n2×6 DataFrameRowbvplaploaddStringInt64Float64?Cat…Cat…Date1Minaal 3.035349.99Yfront panel2022-09-012Genius25228.99Yclamshell2022-10-01\n\n\n(The command [subset(g, :p => ByRow(<=(350))) for g in gdf] avoids the combine step.)\nWe can see that combine also works over GroupedDataFrame objects. This example shows how to find the mean city mileage of groups formed by the value of :Origin in the cars data set:\n\ncombine(groupby(cars, :Origin), :MPGCity => mean)\n\n\n2×2 DataFrameRowOriginMPGCity_meanCat…Float641USA20.95832non-USA23.8667\n\n\nMore than one column can be used to group the data. This example first groups by the number of cylinders, then groups by origin. For each of the 9 resulting groups, the group mean for city mileage is taken:\n\ncombine(groupby(cars, [:Cylinders, :Origin]), :MPGCity => mean)\n\n\n9×3 DataFrameRowCylindersOriginMPGCity_meanCat…Cat…Float6413non-USA39.333324USA24.409134non-USA25.222245non-USA18.556USA18.3566non-USA18.545578USA17.088non-USA17.09rotarynon-USA17.0\n\n\nThe @chain macro may make this more readable:\n\n@chain cars begin\n    groupby([:Cylinders, :Origin])\n    combine(:MPGCity => mean)\n    describe\nend\n\n\n3×7 DataFrameRowvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64DataType1Cylinders3rotary0CategoricalValue{String, UInt8}2OriginUSAnon-USA0CategoricalValue{String, UInt8}3MPGCity_mean21.706717.018.539.33330Float64\n\n\n\nExample 3.1 (Lego sets) For an example, we download a data set from the internet describing all the sets of Legos sold during some time period. This data set is part of the data sets provided by openintro.org, an organization trying to make interesting educational products that are free and transparent.\n\ncsv_data = download(\"https://www.openintro.org/data/csv/lego_population.csv\")\nlegos = CSV.read(csv_data, DataFrame)\nfirst(legos, 2)\n\n\n2×14 DataFrameRowitem_numberset_namethemepiecespriceamazon_priceyearagespagesminifigurespackagingweightunique_piecessizeInt64StringString31String7String31String31Int64String15String7String3String31String31String7String7141916Extra Dots - Series 2DOTS1093.993.442020Ages_6+NANAFoil packNA6Small241908Extra Dots - Series 1DOTS1093.993.992020Ages_6+NANAFoil packNA6Small\n\n\nLet’s explore this data.\nThe size of the data set is printed, though suppressed above as only the first 2 rows are requested to be shown, or can be programmatically identified with size(legos).\n\nsize(legos)\n\n(1304, 14)\n\n\nEach row is a product; there are a lot of different ones.\nThe types identified by CSV.read are not perfect. The data uses NA for not-available. We read again using the argument missingstring=\"NA\":\n\nlegos = CSV.read(csv_data, DataFrame; missingstring=\"NA\")\nfirst(legos, 2)  # show first 2 rows\n\n\n2×14 DataFrameRowitem_numberset_namethemepiecespriceamazon_priceyearagespagesminifigurespackagingweightunique_piecessizeInt64StringString31?Int64?Float64?Float64?Int64String15Int64?Int64?String31?String31?Int64?String7?141916Extra Dots - Series 2DOTS1093.993.442020Ages_6+missingmissingFoil packmissing6Small241908Extra Dots - Series 1DOTS1093.993.992020Ages_6+missingmissingFoil packmissing6Small\n\n\nThe :theme and :packaging variables are categorical, so we make them so:\n\nlegos.theme = categorical(legos.theme)\nlegos.packaging = categorical(legos.packaging);\nfirst(legos, 2)\n\n\n2×14 DataFrameRowitem_numberset_namethemepiecespriceamazon_priceyearagespagesminifigurespackagingweightunique_piecessizeInt64StringCat…?Int64?Float64?Float64?Int64String15Int64?Int64?Cat…?String31?Int64?String7?141916Extra Dots - Series 2DOTS1093.993.442020Ages_6+missingmissingFoil packmissing6Small241908Extra Dots - Series 1DOTS1093.993.992020Ages_6+missingmissingFoil packmissing6Small\n\n\nWe see data on the number of pieces and the age range. Is there some relationship?\nThe :age variable should be an ordered factor, ordered by the youngest age intended for use. The :ages variable has a pattern Ages_startXXX where XXX may be + to indicate or up, or a dash to indicate a range. We use this to identify the youngest intended age.\n\n# alternative to\n# `(m = match(r\"Ages_(\\d+)\", x); m === nothing ? missing : parse(Int, m.captures[1]))`\nfunction pick_first_age(x)\n    ismissing(x) && return missing\n    nos = collect(string(0:9...))\n    out = \"\"\n    for xi in x[6:end] # drop Ages_\n        !(xi in nos) && break\n        out = out * xi\n    end\n    out == \"\" && return missing\n    parse(Int, out)\nend\n\n\ntransform!(legos, :ages => ByRow(pick_first_age) => :youngest_age)\nlegos.youngest_age = categorical(legos.youngest_age, ordered=true)\nfirst(legos[:,r\"age\"], 2)\n\n\n2×3 DataFrameRowagespagesyoungest_ageString15Int64?Cat…?1Ages_6+missing62Ages_6+missing6\n\n\nWith that ordering, an expected pattern becomes clear – kits for older users have on average more pieces – though there are unexpected exceptions:\n\n@chain legos begin\n    groupby(:youngest_age)\n    combine([:pieces, :unique_pieces] .=> mean∘skipmissing\n            .=> [:avg_pieces, :avg_unique_pieces])\nend\n\n\n16×3 DataFrameRowyoungest_ageavg_piecesavg_unique_piecesCat…?Float64Float641missing127.59240.56042137.208326.21743255.61731.297943140.041.054194.7487.285765147.71285.155776191.07987.128987265.556108.81698491.193173.801109859.182252.7731110417.52109.76812112357.86214.14313121307.29256.414141617.25349.515162454.79409.10716182073.84214.294\n\n\n(We composed the two functions skipmissing with mean using the \\circ[tab] operator and use .=> in both places to avoid defining the transformation function twice.)\n\n\n\n\n\n\n\nThe Lego Group\n\n\n\nLego, was the largest toy company in the world by 2021. By 2015 it had produced over 600 billion parts.\n\n\nLego is a mature company with an origin dating to 1934. The number of products per year should be fairly stable, though it is easy enough to check. The data set has a :year variable, so we would only need to group the data by that, then count the number of cases per each group. The nrow function will count the cases. This function is special cased in the DataFrames’ mini language and can appear by itself:\n\ncombine(groupby(legos, :year), nrow) # use `nrow => :n`, or some such, to label differently\n\n\n3×2 DataFrameRowyearnrowInt64Int64120184422201942332020439\n\n\nWe can take other summaries by year, for example, here we find the average price by year and size:\n\n@chain legos begin\n    groupby([:year, :size])\n    combine(nrow => :n,\n            :price => mean∘skipmissing => :avg_price\n            )\nend\n\n\n9×4 DataFrameRowyearsizenavg_priceInt64String7?Int64Float6412018Small32445.779222018Large2128.5932018missing9725.813542019Small32250.466352019Large1429.632962019missing8713.853672020Small33549.810482020Large1833.545692020missing8621.6959\n\n\nThe youngest age range is a bit long. We wish to collapse it to the ranges 1-6, 7-12, and 12-18. The data has been stored as a categorical array. The cut function can group numeric data easily, but to group categorical data, the cut-ranges must be promoted to a CategoricalValue. We have the following, where Ref is used to stop the broadcasting for that value:\n\nvals = CategoricalValue.([1, 6, 12, 18], Ref(legos.youngest_age))\ntransform!(legos,\n           :youngest_age => (x ->cut(x, vals; extend=true))\n           => :youngest_age_range);\n\nWe now check if the number of pieces has dramatically changed over the year. We break up the data by the age range, as we expect variation in that value so it is best to dis-aggregate over that factor:\n\n@chain legos begin\n    groupby([:youngest_age_range, :year])\n    combine(nrow => :n,\n            :pieces => mean∘skipmissing => :avg_pieces)\nend\n\n\n12×4 DataFrameRowyoungest_age_rangeyearnavg_piecesCat…?Int64Int64Float641missing201833165.3792missing20193495.77783missing202044111.84[1, 6)201898131.5615[1, 6)201992122.5336[1, 6)202092153.4137[6, 12)2018292341.8668[6, 12)2019275389.9459[6, 12)2020276365.88710[12, 18]2018191956.2111[12, 18]2019221938.9512[12, 18]2020272111.3\n\n\nThere are many different themes. The command unique(legos.theme) shows 41. Which are the most popular? To see, the data is grouped by the theme, each group is counted, missing themes are dropped, then the data frame is sorted and the top 5 rows are shown:\n\n@chain legos begin\n    groupby(:theme)\n    combine(nrow => :n)\n    dropmissing\n    sort(:n; rev=true)\n    first(5)\nend\n\n\n5×2 DataFrameRowthemenCat…Int641Star Wars™1192Friends1033City1014NINJAGO®785DUPLO®53\n\n\n\n\nThe SplitApplyCombine package\nThere is support for the split-apply-combine paradigm outside of the DataFrames package in the package SplitApplyCombine. This package is much lighter weight than DataFrames. This support targets other representations of tabular data, such as a vector of nested tuples:\n\nusing SplitApplyCombine\ntbl = [\n(b = \"Goruck GR2\", v = 40, p = 395, lap = \"Y\", load = \"front panel\", d = Date(\"2022-09-01\")),\n(b = \"Minaal 3.0\", v = 35, p = 349, lap = \"Y\", load = \"front panel\", d = Date(\"2022-09-01\")),\n(b = \"Genius    \",     v = 25, p = 228, lap = \"Y\", load = \"clamshell\",   d = Date(\"2022-10-01\"))\n]\n\n3-element Vector{NamedTuple{(:b, :v, :p, :lap, :load, :d), Tuple{String, Int64, Int64, String, String, Date}}}:\n (b = \"Goruck GR2\", v = 40, p = 395, lap = \"Y\", load = \"front panel\", d = Date(\"2022-09-01\"))\n (b = \"Minaal 3.0\", v = 35, p = 349, lap = \"Y\", load = \"front panel\", d = Date(\"2022-09-01\"))\n (b = \"Genius    \", v = 25, p = 228, lap = \"Y\", load = \"clamshell\", d = Date(\"2022-10-01\"))\n\n\nMany of the main verbs are generic functions: filter, map, reduce; group does grouping. There are some others. A few examples follow.\nFor indexing this structure, we can’t index by [row, col], rather we can use [row][col] notation, the first to extract the row, the second to access the entries in the column. For example, he we index a selected row by position, by name, and then with multiple names:\n\ntbl[2][2], tbl[2].v, tbl[2][ [:v, :p] ]\n\n(35, 35, (v = 35, p = 349))\n\n\nThe invert function reverses the access pattern, allowing in this case, [col][row] access, with:\n\nitbl = invert(tbl)\nitbl[3][1], itbl[:p][2]\n\n(395, 349)\n\n\nIn some ways, the invert function flips the viewpoint from an array of structs to a struct of arrays.\nTo filter out rows, we have the same calling style as with data frames: filter(pred, tbl). With the outer container of tbl being an array, no special method is utilized, iteration is over each row. For example,\n\nfilter(r -> r.v >= 35, tbl) |> DataFrame\n\n\n2×6 DataFrameRowbvplaploaddStringInt64Int64StringStringDate1Goruck GR240395Yfront panel2022-09-012Minaal 3.035349Yfront panel2022-09-01\n\n\n(The call to DataFrame is to take advantage of the prettier printing. DataFrame can consume a vector of named tuples for its input.)\nThe DataPipes package composes a bit more nicely with this approach, but we continue using Chain for examples3, like this one of nested calls to filter:\n\n@chain tbl begin\n    filter(r -> r.v >= 35,  _)\n    filter(r -> r.p >= 350, _)\n    DataFrame\nend\n\n\n1×6 DataFrameRowbvplaploaddStringInt64Int64StringStringDate1Goruck GR240395Yfront panel2022-09-01\n\n\nSelecting columns and adding a transformation can be achieved through map, which iterates over each named tuple in this example:\n\nmap(r -> (b=r.b, v=r.v, p=r.p, pv = round(r.p/r.v; digits=2)), tbl) |>\n    DataFrame\n\n\n3×4 DataFrameRowbvppvStringInt64Int64Float641Goruck GR2403959.882Minaal 3.0353499.973Genius    252289.12\n\n\nThe group function can group by values of a function call. This example from the package’s Readme is instructive:\n\nnms = [\"Andrew Smith\", \"John Smith\", \"Alice Baker\",\n       \"Robert Baker\", \"Jane Smith\", \"Jason Bourne\"]\ngroup(last, split.(nms))\n\n3-element Dictionaries.Dictionary{SubString{String}, Vector{Vector{SubString{String}}}}\n  \"Smith\" │ Vector{SubString{String}}[[\"Andrew\", \"Smith\"], [\"John\", \"Smith\"], […\n  \"Baker\" │ Vector{SubString{String}}[[\"Alice\", \"Baker\"], [\"Robert\", \"Baker\"]]\n \"Bourne\" │ Vector{SubString{String}}[[\"Jason\", \"Bourne\"]]\n\n\nThe names are split on spaces, and the keys of the group are determined by the last function, which here gets the last piece after splitting (aka the “last” name, but by coincidence, not intelligence).\nFor numeric operations, this is quite convenient. Here we find group by the remainder upon division by 3:\n\ngroup(x -> rem(x,3), 1:10)\n\n3-element Dictionaries.Dictionary{Int64, Vector{Int64}}\n 1 │ [1, 4, 7, 10]\n 2 │ [2, 5, 8]\n 0 │ [3, 6, 9]\n\n\nThe grouping basically creates a dictionary and adds to the key determined by the function.\nTo apply a function to the values of a base Julia dictionary is not directly supported by map, however the dictionary used by group (from the Dictionaries packages) does allow this, so we could, for example, add up the values in each group with:\n\nmap(sum, group(x -> rem(x,3), 1:10))\n\n3-element Dictionaries.Dictionary{Int64, Int64}\n 1 │ 22\n 2 │ 15\n 0 │ 18\n\n\nThe group function can do this in one call, by placing the function to apply in between the by function and the table. Here we apply first to pick off the first name. That is, [\"Andrew\", \"Smith\"] becomes just \"Andrew\", the first element of that vector.\n\ngps = group(last, first, split.(nms))\n\n3-element Dictionaries.Dictionary{SubString{String}, Vector{SubString{String}}}\n  \"Smith\" │ SubString{String}[\"Andrew\", \"John\", \"Jane\"]\n  \"Baker\" │ SubString{String}[\"Alice\", \"Robert\"]\n \"Bourne\" │ SubString{String}[\"Jason\"]\n\n\n\nExample 3.2 (A tally function) The Tally package provides a quick way to tally up numbers. Here we do a simplified version.\nTo tally, we have three steps: group by each by value, count the number in each groups, sort the values. This is implemented with this composition of functions:\n\ntally(v; by=identity) = SplitApplyCombine.sortkeys(map(length, group(by, v)))\n\ntally (generic function with 1 method)\n\n\nTo see, we have:\n\nv = rand(1:5, 50)\nd = tally(v)\n\n5-element Dictionaries.Dictionary{Int64, Int64}\n 1 │ 7\n 2 │ 6\n 3 │ 9\n 4 │ 13\n 5 │ 15\n\n\nUsing map, we can provide an alternate, oft used view for tallying:\n\ntallies = \"\\u007C\"^4\nftally = \"┼┼┼┼ \"\nfunction prison_count(x)\n    d, r = divrem(x, 5)\n    ftally^d * tallies[1:r]\nend\n\n\nmap(prison_count, d)\n\n5-element Dictionaries.Dictionary{Int64, Any}\n 1 │ \"┼┼┼┼ ||\"\n 2 │ \"┼┼┼┼ |\"\n 3 │ \"┼┼┼┼ ||||\"\n 4 │ \"┼┼┼┼ ┼┼┼┼ |||\"\n 5 │ \"┼┼┼┼ ┼┼┼┼ ┼┼┼┼ \"\n\n\n\n\nExample 3.3 (A simple stem and leaf diagram) The grouping and sorting made quite accessible by SplitApplyCombine could also be used to produce a simple stem-and-leaf type diagram. Here we don’t bother displaying stems with no associated leaves, which does change the interpretation of spread in the data, and could easily be added in, though it takes us a bit afield, as it requires some manipulation of the display:\n\nx = rand(0:100, 32)  # 32 test scores\nstem_unit = 10       # stem * 10 is value of stem\n@chain x begin\n    round.(Int, 10*_/stem_unit)\n    SplitApplyCombine.group(x -> x ÷ 10, x->rem(x,10), _)\n    SplitApplyCombine.sortkeys(_)\n    map(sort, _)\n    map(x -> parse(Int,join(x)), _)\nend\n\n10-element Dictionaries.Dictionary{Int64, Int64}\n 0 │ 17\n 1 │ 245\n 2 │ 278\n 3 │ 13578\n 4 │ 2\n 5 │ 445\n 6 │ 247\n 7 │ 19\n 8 │ 3357\n 9 │ 44469\n\n\n\n\nLet’s return to the cars data and use group to identify the average mileage per type. For this task, we group by :Typeand then apply a function to extract the mileage from a row. The copy.(eachrow(cars)) command creates a vector of named tuples.\n\ncs = copy.(eachrow(cars))  # Can also call `NamedTuple` for `copy`\ngps = group(r -> r.Type, r -> r.MPGCity, cs)\n\n6-element Dictionaries.Dictionary{CategoricalValue{String, UInt8}, Vector{Int32}}\n CategoricalValue{String, UInt8} \"Smal… │ Int32[25, 29, 23, 29, 31, 23, 46, 42,…\n CategoricalValue{String, UInt8} \"Mids… │ Int32[18, 19, 22, 22, 19, 16, 21, 21,…\n CategoricalValue{String, UInt8} \"Comp… │ Int32[20, 25, 25, 23, 22, 22, 24, 26,…\n CategoricalValue{String, UInt8} \"Larg… │ Int32[19, 16, 16, 17, 20, 20, 20, 18,…\n CategoricalValue{String, UInt8} \"Spor… │ Int32[19, 17, 18, 22, 24, 30, 24, 26,…\n  CategoricalValue{String, UInt8} \"Van\" │ Int32[18, 15, 17, 15, 18, 17, 18, 18,…\n\n\nWe now can map mean over each group:\n\nms = map(mean, gps)\n\n6-element Dictionaries.Dictionary{CategoricalValue{String, UInt8}, Float64}\n   CategoricalValue{String, UInt8} \"Small\" │ 29.857142857142858\n CategoricalValue{String, UInt8} \"Midsize\" │ 19.545454545454547\n CategoricalValue{String, UInt8} \"Compact\" │ 22.6875\n   CategoricalValue{String, UInt8} \"Large\" │ 18.363636363636363\n  CategoricalValue{String, UInt8} \"Sporty\" │ 21.785714285714285\n     CategoricalValue{String, UInt8} \"Van\" │ 17.0\n\n\nThis requires two passes through the grouped data, the first to get just the mileage values, the next to call mean. The following DataFrames syntax hides this:\n\ncombine(groupby(cars, :Type), :MPGHighway => mean)\n\n\n6×2 DataFrameRowTypeMPGHighway_meanCat…Float641Compact29.8752Large26.72733Midsize26.72734Small35.47625Sporty28.78576Van21.8889\n\n\nReductions, like sum and count which can be written easily using the higher-order function reduce, have support to combine the grouping and reduction. The groupreduce function takes the additional arguments of reduce (an operation and an optional init value). For example, to get the group sum, we could call:\n\ngroupreduce(r -> r.Type, r -> r.MPGCity, +, cs)\n\n6-element Dictionaries.Dictionary{CategoricalValue{String, UInt8}, Int32}\n   CategoricalValue{String, UInt8} \"Small\" │ 627\n CategoricalValue{String, UInt8} \"Midsize\" │ 430\n CategoricalValue{String, UInt8} \"Compact\" │ 363\n   CategoricalValue{String, UInt8} \"Large\" │ 202\n  CategoricalValue{String, UInt8} \"Sporty\" │ 305\n     CategoricalValue{String, UInt8} \"Van\" │ 153\n\n\nThe group sum and group count have shortcuts, so the mean could have been computed as:\n\ngroupsum(r -> r.Type, r -> r.MPGCity, cs) ./ groupcount(r -> r.Type, cs)\n\n6-element Dictionaries.Dictionary{CategoricalValue{String, UInt8}, Float64}\n   CategoricalValue{String, UInt8} \"Small\" │ 29.857142857142858\n CategoricalValue{String, UInt8} \"Midsize\" │ 19.545454545454547\n CategoricalValue{String, UInt8} \"Compact\" │ 22.6875\n   CategoricalValue{String, UInt8} \"Large\" │ 18.363636363636363\n  CategoricalValue{String, UInt8} \"Sporty\" │ 21.785714285714285\n     CategoricalValue{String, UInt8} \"Van\" │ 17.0\n\n\n(Unlike for a Dict instance, the dictionaries above allow such division.)\n\nThe “combine” part of the paradigm takes the pieces and reassembles. The data frames example returns a data frame, whereas with SplitApplyCombine.jl we have a dictionary. A dictionary does not map directly to a data frame. While both dictionaries and named tuples are associative arrays, we wouldn’t necessarily want to map our data to a row. Rather, here we use the pairs iterator to iterate over the keys and values to produce an array of named tuples:\n\n[ (; Type, MPG) for (Type,MPG) in pairs(ms)] |> DataFrame\n\n\n6×2 DataFrameRowTypeMPGCat…Float641Small29.85712Midsize19.54553Compact22.68754Large18.36365Sporty21.78576Van17.0\n\n\n\n\n\n3.1.10 Aggregating data\nSomewhat inverse to flattening data is to aggregate data, by which we mean combining multiple columns into a single one, or combine multiple rows into a single one. The select and combine methods can do these tasks.\nConsider the three price variables in the cars data. We can find an average of the three price numbers easily enough. This function allows the specification as different arguments, not in a container:\n\n_mean(xs...) = mean(xs)\n\n_mean (generic function with 1 method)\n\n\nWe can then use select to aggregate the three price variables. In the following, these are selected with a regular expression:\n\ncars_price = select(cars, 1:3, r\"Price\" => ByRow(_mean) => :price)\nfirst(cars_price, 3)\n\n\n3×4 DataFrameRowManufacturerModelTypepriceCat…StringCat…Float641AcuraIntegraSmall15.86672AcuraLegendMidsize33.93333Audi90Compact29.1\n\n\nTo aggregate over columns, the groupby construct can be used to specify the groupings to aggregate over. In the following this is the car type:\n\ncombine(groupby(cars_price, :Type), :price => mean => :avg_price)\n\n\n6×2 DataFrameRowTypeavg_priceCat…Float641Compact18.21042Large24.3033Midsize27.21524Small10.16675Sporty19.40246Van19.1111\n\n\n\n\n3.1.11 Tidy data; stack, unstack\nThe split-apply-combine paradigm suggest that storing data so that it can be split readily is preferable to other ways of storage. The notion of “tidy data,” (Wickham 2014) as presented in R’s tidyr package and described in this vignette suggests data sets should follow:\n\nEvery column is a variable.\nEvery row is an observation.\nEvery cell is a single value.\n\nThese terms are defined by\n\nA dataset is a collection of values, usually either numbers (if quantitative) or strings (if qualitative). Values are organised in two ways. Every value belongs to a variable and an observation. A variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units. An observation contains all values measured on the same unit (like a person, or a day, or a race) across attributes.\n\nIn addition to flatten, the DataFrames package provides the functions stack and unstack for tidying data. We illusrate with an abbreviated version of an example in the tidyr vignette.\nConsider this data from the Global Historical Climatology Network for one weather station (MX17004) in Mexico for five months in 2010. The full data has 31 days per month, this abbreviated data works with just the first 8:\n\nweather = \"\"\"\n\"id\",\"year\",\"month\",\"element\",\"d1\",\"d2\",\"d3\",\"d4\",\"d5\",\"d6\",\"d7\",\"d8\"\n\"MX17004\",\"2010\",\"1\",\"tmax\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\"\n\"MX17004\",\"2010\",\"1\",\"tmin\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\"\n\"MX17004\",\"2010\",\"2\",\"tmax\",\"---\",\"27.3\",\"24.1\",\"---\",\"---\",\"---\",\"---\",\"---\"\n\"MX17004\",\"2010\",\"2\",\"tmin\",\"---\",\"14.4\",\"14.4\",\"---\",\"---\",\"---\",\"---\",\"---\"\n\"MX17004\",\"2010\",\"3\",\"tmax\",\"---\",\"---\",\"---\",\"---\",\"32.1\",\"---\",\"---\",\"---\"\n\"MX17004\",\"2010\",\"3\",\"tmin\",\"---\",\"---\",\"---\",\"---\",\"14.2\",\"---\",\"---\",\"---\"\n\"MX17004\",\"2010\",\"4\",\"tmax\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\"\n\"MX17004\",\"2010\",\"4\",\"tmin\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\"\n\"MX17004\",\"2010\",\"5\",\"tmax\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\"\n\"MX17004\",\"2010\",\"5\",\"tmin\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\",\"---\"\n\"\"\"\n\nw = CSV.read(IOBuffer(weather), DataFrame; missingstring=\"---\")\nfirst(w, 4)\n\n\n4×12 DataFrameRowidyearmonthelementd1d2d3d4d5d6d7d8String7Int64Int64String7MissingFloat64?Float64?MissingFloat64?MissingMissingMissing1MX1700420101tmaxmissingmissingmissingmissingmissingmissingmissingmissing2MX1700420101tminmissingmissingmissingmissingmissingmissingmissingmissing3MX1700420102tmaxmissing27.324.1missingmissingmissingmissingmissing4MX1700420102tminmissing14.414.4missingmissingmissingmissingmissing\n\n\nThis example is said to have variables stored in both rows and columns. The first step to tidying the data is to stack days 1 through 8 into a column. We use the Between column selector to select the columns for d1 though d8 (an alternative to, say, r\"^d\"):\n\nw1 = @chain w begin\n    stack(Between(:d1, :d8); variable_name = :day)\n    filter(:value => !ismissing, _)\nend\n\n\n6×6 DataFrameRowidyearmonthelementdayvalueString7Int64Int64String7StringFloat64?1MX1700420102tmaxd227.32MX1700420102tmind214.43MX1700420102tmaxd324.14MX1700420102tmind314.45MX1700420103tmaxd532.16MX1700420103tmind514.2\n\n\nAs in the example being followed, missing values are skipped, requiring the reader to imply the lack of measurement on a given day.\nThe day is coded with a leading “d” and as a string, not an integer. We can strip this prefix out using a regular expression or, as here, string indexing then parse the result to an integer:\n\ntransform!(w1, :day => ByRow(x -> parse(Int, x[2:end])) => :day)\n\n\n6×6 DataFrameRowidyearmonthelementdayvalueString7Int64Int64String7Int64Float64?1MX1700420102tmax227.32MX1700420102tmin214.43MX1700420102tmax324.14MX1700420102tmin314.45MX1700420103tmax532.16MX1700420103tmin514.2\n\n\nFinally, the element column does not hold a variable, rather it stores the names of two variables (tmin and tmax). For this the data is “unstacked” splitting the longer variable value in two:\n\nw2 = unstack(w1, :element, :value)\n\n\n3×6 DataFrameRowidyearmonthdaytmaxtminString7Int64Int64Int64Float64?Float64?1MX1700420102227.314.42MX1700420102324.114.43MX1700420103532.114.2\n\n\nAs described in the vignette, now each variable is in one column, and each row describes one day.\n\n\n3.1.12 Joins\nConsider the three following assessments of some class. The students have somewhat spotty attendance:\n\nt1 = \"\"\"\nfirst,last,points\nAlice, Smith, 100\nBob, Jones, 200\n\"\"\"\n\nt2 = \"\"\"\nfirst,last,score\nCarol, Gates, 125\nChad, Gad, 200\nBob, Jones, 225\n\"\"\"\n\nt3 = \"\"\"\nfirst,last,points\nBob, Jones, 300\nCarol, Gates, 150\nErin, Dan, 100\n\"\"\"\na1, a2, a3 = [CSV.read(IOBuffer(txt), DataFrame) for txt ∈ (t1, t2, t3)];\n\nWe have several ways to combine these three data frames.\nWe can concatenate them using vcat. Before doing so, we need to align the column names, as the second data set used a different name:\n\nselect!(a2, 1:2, :score => :points);\n\nThe vcat method for data frames takes a named argument source allowing a variable to be appended indicating the source of the data frame. The following will indicate with a 1, 2, or 3 what data frame contributed what row:\n\naₛ = vcat(a1, a2, a3; source=:assignment)\ndescribe(aₛ)\n\n\n4×7 DataFrameRowvariablemeanminmedianmaxnmissingeltypeSymbolUnion…AnyUnion…AnyInt64DataType1firstAliceErin0String72last Dan Smith0String73points175.0100175.03000Int644assignment2.12512.030Int64\n\n\nSimilarly, append! could be used in combination with reduce (as in reduce(append!, (a1, a2, a3))).\nA more common alternative to the above is to join the data frames using one of several join commands. Joins allow multiple tables to be merged, or mashed. into one. We briefly illustrate the implementations in DataFrames of outer, left, right, inner, semi, and anti joins, leaving the cross join and further details to the documentation.\nWe return to the original data, with the differently named columns:\n\na1, a2, a3 = [CSV.read(IOBuffer(txt), DataFrame) for txt ∈ (t1, t2, t3)];\n\nThe first join we introduce is outerjoin. It combines two data frames with rows for keys found in any of the data frames passed in.\n\nouterjoin(a1, a2, on=[\"first\", \"last\"])\n\n\n4×4 DataFrameRowfirstlastpointsscoreString7String7Int64?Int64?1Bob Jones2002252Alice Smith100missing3Carol Gatesmissing1254Chad Gadmissing200\n\n\nThe on argument is a column name (or names as above) to join the data frames on. They need not have the same name; the values passed to on may be pairs indicating the mapping. (That is, if we were joining on the last variable, on=:points => :score would be possible.)\nThe variable to join on is often called the “key,” though in Julia that terminology is also used with associative arrays.\nThe joined data frame includes all columns from either data frame not specified in on. The resulting data frame, in this case, is in a wide, not tidy, format.\nIn the above, the columns do not have names in conflict. However, were a1 and a3 joined, they would. Passing makeunique=true instructs distinguishing names to be produced:\n\nouterjoin(a1, a3, on=[\"first\", \"last\"], makeunique=true)\n\n\n4×4 DataFrameRowfirstlastpointspoints_1String7String7Int64?Int64?1Bob Jones2003002Alice Smith100missing3Carol Gatesmissing1504Erin Danmissing100\n\n\nMore than two data frames may be passed to outerjoin, as illustrated below where all three are. The result includes a row for each person (key) in any of the data sets.\n\nouterjoin(a1, a2, a3, on=[\"first\", \"last\"], makeunique=true)\n\n\n5×5 DataFrameRowfirstlastpointsscorepoints_1String7String7Int64?Int64?Int64?1Bob Jones2002253002Carol Gatesmissing1251503Alice Smith100missingmissing4Chad Gadmissing200missing5Erin Danmissingmissing100\n\n\nFrom the outerjoin, we easily note that only the first listed student was present for all the assessments.\nThree other joins related to outerjoin are:\n\nleftjoin(df1, df2; on, ...) which includes all rows in the left data frame\nrightjoin(df1, df2; on, ...) which includes all rows in the right data frame (similar to leftjoin(df2, df1).\ninnerjoin(df1, df2; on, ...) which includes all rows with keys in both data data frames.\n\nTo illustrate:\n\nleftjoin(a1, a2, on=[\"first\", \"last\"])\n\n\n2×4 DataFrameRowfirstlastpointsscoreString7String7Int64Int64?1Bob Jones2002252Alice Smith100missing\n\n\n\nrightjoin(a1, a2, on=[\"first\", \"last\"])\n\n\n3×4 DataFrameRowfirstlastpointsscoreString7String7Int64?Int641Bob Jones2002252Carol Gatesmissing1253Chad Gadmissing200\n\n\nIn the following, we must qualify the function below, as SplitApplyCombine has a conflicting use, though this isn’t typically needed:\n\nDataFrames.innerjoin(a1, a2, on=[\"first\", \"last\"])\n\n\n1×4 DataFrameRowfirstlastpointsscoreString7String7Int64Int641Bob Jones200225\n\n\nJoins have an analog in set operations: the outer join is like a union, the inner join is like an intersection.\nThe next two joins do not join columns, as the ones above. Rather, they use the second data frame to filter out rows of the first data frame.\nThe semijoin returns the rows of the left data frame that match a key of the right data frame:\n\nsemijoin(a1, a2[3:3, :]; on=[\"first\", \"last\"])\n\n\n1×3 DataFrameRowfirstlastpointsString7String7Int641Bob Jones200\n\n\nAn antijoin includes rows in the left data frame that do not match a key in the right data frame. (Like the set difference). Here we see the row in common to a1 and a2 is not included:\n\nantijoin(a1, a2, on=[\"first\", \"last\"])\n\n\n1×3 DataFrameRowfirstlastpointsString7String7Int641Alice Smith100\n\n\n\n\n\n\nWickham, H. (2011), “The split-apply-combine strategy for data analysis,” Journal of Statistical Software, 40, 1–29. https://doi.org/10.18637/jss.v040.i01.\n\n\nWickham, H. (2014), “Tidy data,” The Journal of Statistical Software, 59."
  },
  {
    "objectID": "EDA/bivariate-julia.html#multiple-samples",
    "href": "EDA/bivariate-julia.html#multiple-samples",
    "title": "4  Bivariate data",
    "section": "4.1 Multiple samples",
    "text": "4.1 Multiple samples\nA common scenario is when data for a measurement is collected across multiple levels of a factor to see if there is something about that factor that has an effect on the measurement. As measurements typically have fluctuations, this “effect” must be understood relative to some understanding of the underlying variability. To gather that, a probability model might be used. For now, we content ourselves with common graphical comparisons, which can be used to distinguish large effects.\nFirst we graphically explore one measurement, PetalWidth, for the 3 different species. A typical question might be, is the average petal width the same across the three species?\nThe data storage in iris is tidy data, each row is an observation, the PetalWidth is the variable. Were the data in a long format with a variable for each level of the factor, the stack command would prove useful to tidy the data.\nThe graphics are created by splitting the data along the levels of the factor and then applying the graphic to the univariate data for each group. For this task, the StatsPlots package is useful.1\n\n4.1.1 Grouped dot plots\nA dot plot is a useful graphic to show the distribution of values on a number line when the data is sufficiently spread out so as not to overlap too much. The basic graphic is \\(1\\)-dimensional, adding a second dimension for the grouping variable allows easy comparisons.\nHere we discuss dot plots for different levels of a factor. In the iris data set, to create plots by each species (Figure 4.1), allowing a comparison of the distribution of the \\(x\\) variable over different levels of a factor. The following illustrates the steps done, but we will see next that such effort is unneeded as the StatsPlots recipes wrap up this data manipulation, in this case with a method for dotplot.\n\nbyspecies = groupby(iris, :Species)\np1 = StatsPlots.plot()\nfor (i, k) in enumerate(keys(byspecies)) # iterate over keys\n    species = k.Species\n    xs = byspecies[k].PetalWidth\n    ys = i .+ one.(xs)\n    dotplot!(p1, xs, ys; label=species)\nend\np1;\n\nThe identification of the species after grouping is a bit awkward, but the idea is we split the data, then create a dot plot for each level of the grouping variable, :Species.\nThis is more easily done by passing :Species as the \\(y\\) value to dotplot, in which case the categorical variable is used for grouping.\n\np2 = @df iris dotplot(:PetalWidth, :Species; legend=false);\n\nThe @df macro was used as a convenience. This replaces the symbols in the expression with the corresponding column in the specified data frame.\n\n\n\n\n\nFigure 4.1: Grouped dot plots allow an easy comparison of the distribution of a variable over the levels of a categorical variable. These two use the same data set, but were produced different ways. The left-most was done “by hand” and uses a legend to distinguish the species. The right-most was produced by dotplot with labels used to mark the species.\n\n\n\n\nPutting the categorical variable first, presents a graphic (Figure 4.2) which uses jittering to disambiguate equal measurements in a species:\n\n@df iris dotplot(:Species, :PetalWidth; legend=false)\n\n\n\n\nFigure 4.2: the categorial variable’s position determines the layout chosen for the grouped dot plot. In this figure, the categorical variable was in the x position, and the numeric data was jittered left and right.\n\n\n\n\nRegardless of how the graphic is produced, there appears to be a difference in the centers based on the species, as would be expected – different species have different sizes.\n\n\n4.1.2 Boxplots across groups\nGrouped boxplots are made in a similar manner to the last example using boxplot (Figure 4.3); we use the categorical variable indicating grouping in the x position:\n\n@df iris boxplot(:Species, :PetalWidth; legend=false)\n\n\n\n\nFigure 4.3: a basic boxplot comparing petal width across the different species.\n\n\n\n\nAn article from fivethirtyeight.com contained the boxplots in ?fig-five-thirty-eight. The graphic used does not show the traditional “1.5IQR” whiskers, rather it extends the box using shading.\n{#fig-five-thirty-eight fig-alt=” A version of a box-and-whisker plot for comparing the time that a very fast crossword puzzler takes when done on the computer and the paper. The various summary statistics, such as the median, are all less on the computer. “}\n\n\n\n\n\n\nOde to the boxplots\n\n\n\nThe box plot is a great graphic for comparing values across the levels of some factor. Each boxplot shows the center (the median), the range (the IQR), the shape (well basic symmetry about the median), the outlying values (with its identying rule of 1.5 IQRs beyond \\(Q_1\\) and \\(Q_3\\). The only drawback is boxplots can’t show multi modal data. But if the factor is the cause of that, then the collection of boxplots is great for comparison.\n\n\n\n\n4.1.3 Violin and density plots\nA violin plot is like the boxplot, only it is wider where more data is concentrated, and smaller where less is. It uses a sideways density plot to summarize the data, whereas a boxplot only uses the five-number summary. The violin function creates these (Figure 4.4):\n\np2 = @df iris violin(:Species, :PetalWidth; legend=false)\n@df iris dotplot!(p2, :Species, :PetalWidth; legend=false)\n\n\n\n\nFigure 4.4: Illustration of a violin plot with a dotplot overlay for the petal width across the different species.\n\n\n\n\nAs illustrated for univariate data, the density function makes a density plot. There is an option for grouped data.\nAs seen below, the grouping variable is a named, not positional argument. In Figure 4.5 the left plot is done without grouping, the right one with grouping by species. The left plot shows a multi-modal distribution, which suggests, as we already have seen, that there is a mixture of populations in the data. There appear to be two modes, with a hint of a third. The separation between the “setosa” species and the other two is greater than the separation between the two others.\n\np1 = @df iris density(:PetalWidth; legend=false)\np2 = @df iris density(:PetalWidth, group=:Species);\n\n\n\n\n\n\nFigure 4.5: Density plots of PetalWidth and PetalWidth by Species. The left graphic suggests multiple modes, which are revealed in the right graphic.\n\n\n\n\n\n\n4.1.4 Histograms across groups\nThe density plot is, perhaps, less familiar than a histogram, but makes a better grahpic when comparing two or more distributions, as the individual graphics don’t overlap. There are attempts to use the histogram, and they can be effective. In Figure 4.6, taken from an article on fivethirtyeight.com on the time to solve cross word puzzles broken out by day of week, we see a stacked dotplot, which is visually very similar to a histogram, presented for each day of the week. The use of color allows one to distinguish the day, but the overalpping aspect of the graphic inhibits part of the distribution of most days, and only effectively shows the longer tails as the week progresses. In StatsPlots, the grouped hist function can produce similar graphics.\n\n\n\n\n\nCrossword times by day\n\n\nFigure 4.6: Dotplots for solving times of various New York Times crossword puzzled grouped by day of the puzzle.\n\n\n\n\n4.1.5 Quantile comparisons\nA “quantile-quantile plot” plots pairs of quantiles from two data sets as a scatter plot to help gauge if the shape of the two distributions is similar. If it is, the data will fall roughly on a straight line. The previously discussed quantile-normal plot is identical, with the specialization that one of the distributions is the reference normal distribution.\nWe use this plot to check if the shape of the distribution for two different species is roughly the same. To do so, we extract the PetalWidth values for the different levels of :Species. A direct way might be indexing: iris[iris.Species .== \"versicolor\", :PetalWidth] or with subset: subset(iris, :Species => ByRow(==(\"versicolor\"))).PetalWidth. In the following we group by :Species then for each group apply a \\(z\\)-score to :PetalWidth. From here, we can compare the 1st and 2nd and then the 2nd and 3rd group with qqplot in Figure 4.7. There is not a strong reason to expect big differences in shape.\n\njitter(x, scale = 0.25) = x + scale * randn(length(x))\ngps = [select(gp, :PetalWidth => zscore∘jitter => :PetalWidth)\n       for gp in groupby(iris, :Species)]\n\n\np1 = qqplot(gps[1].PetalWidth, gps[2].PetalWidth)\np2 = qqplot(gps[2].PetalWidth, gps[3].PetalWidth);\n\n\n\n\n\n\nFigure 4.7: A qqplot of the petal width from the versicolor and virginica species. The graphic suggests similar-shaped distributions. The data is jittered to disambiguate identical values."
  },
  {
    "objectID": "EDA/bivariate-julia.html#paired-data",
    "href": "EDA/bivariate-julia.html#paired-data",
    "title": "4  Bivariate data",
    "section": "4.2 Paired data",
    "text": "4.2 Paired data\nWe have looked at a single variable over several levels of a given factor. If the data is split along the factor, there are possibly a different number of measurements for the given level. A key statistical assumption for inference will be that these different measurements are independent.\nWe might express this data as\n\\[\\begin{align*}\nx_{11}, & x_{12}, \\dots, x_{1n_1}\\\\\nx_{21}, & x_{22}, \\dots, x_{2n_2}\\\\\n    & \\vdots \\\\\nx_{m1}, & x_{m2}, \\dots, x_{mn_m}.\n\\end{align*}\\]\nNow we turn our attention to data of two measured variables for a given observation. These two variables must have the same number of values, this being equal to the number of observations. Moreover, it need not be the case that the two measurements should be “independent,” indeed, the measurements are from the same case, so, for example with the iris data, if a plant has a large petal width it will likely also have a large petal length.\nWe might express the data mathematically as:\n\\[\\begin{align*}\nx_{1}, & x_{2}, \\dots, x_{n}\\\\\ny_{1}, & y_{2}, \\dots, y_{n}\n\\end{align*}\\]\nOr – to emphasize how the data is paired off – as \\((x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\).\n\n4.2.1 Numeric summaries\nThe main numeric summary of paired data is the Pearson correlation coefficient. In terms of \\(z\\) scores this is almost the average of their respective products:\n\\[\ncor = \\frac{1}{n-1} \\sum \\left(\\frac{x_i - \\bar{x}}{s_x}\\right) \\cdot \\left(\\frac{y_i - \\bar{y}}{s_y}\\right).\n\\]\nThe cor function computes this measure.\nConsider again the iris data, only we select below just the setosa data and the petal attributes:\n\nl, w = @chain iris begin\n    subset(:Species => ByRow(==(\"setosa\")))\n    select([:PetalLength, :PetalWidth])\n    eachcol\nend;\n\nWe have the correlation computed by\n\ncor(l, w)\n\n0.3316300408041188\n\n\nThe correlation will be seen to be measure of linear associativity. When it is close to 1 or -1, the variability in the data is well described by a line; when close to 0, not so.\nThe correlation can be computed from a matrix for each pair of columns. This is done here for illustration:\n\n@chain iris begin\n    subset(:Species => ByRow(==(\"setosa\")))\n    select(_, names(_, Real))\n    Matrix\n    cor\nend\n\n4×4 Matrix{Float64}:\n 1.0       0.742547  0.267176  0.278098\n 0.742547  1.0       0.1777    0.232752\n 0.267176  0.1777    1.0       0.33163\n 0.278098  0.232752  0.33163   1.0\n\n\nThe Spearman correlation is the correlation of the data after ranking. It is a measure of monotonicity in the data. There are different ranking algorithms in StatsBase, the Spearman correlation uses tiedrank. It can be computed with this or with corspearman, as shown:\n\ncorspearman(l, w), cor(tiedrank(l), tiedrank(w))\n\n(0.2711413763783511, 0.2711413763783511)\n\n\nThe primary graphic to assess the relationship between the two, presumably, dependent variables is a scatter plot:\n\nscatter(l, w; legend=false, xlab=\"PetalLength\", ylab=\"PetalWidth\")\n\nvline!([mean(l)], linestyle=:dash)\nhline!([mean(w)], linestyle=:dash)\n\n\n\n\nFigure 4.8: Scatter plot of petal length and petal width for the setosa data. The dashed lines show the “center” of the data, \\((\\bar{x}, \\bar{y})\\).\n\n\n\n\nFigure 4.8 shows the length and width data in a scatter plot. Jittering would be helpful to show all the data, as it has been discretized and many points are overplotte. The dashed lines are centered at the means of the respective variables. If the mean is the center of a single variable, then \\((\\bar{x}, \\bar{y})\\) may be thought of as the center of the paired data. Thinking of the dashed lines meeting at the origin, four quadrants are formed. The correlation can be viewed as a measure of how much the data sits in opposite quadrants. In the figure, there seems to be more data in quadrants I and III then II and IV, which sugests a positive correlation, as confirmed numerically.\nBy writing the correlation in terms of \\(z\\)-scores, the product in that formula is positive if the point is in quadrant I or III and negative if in II or IV. So, for example, a big positive number suggests data is concentrated in quadrants I and III or that there is a strong association between the variables. The scaling by the standard deviations, leaves the mathematical constraint that the correlation is between \\(-1\\) and \\(1\\).\n\n\n\n\n\n\nCorrelation is not causation\n\n\n\nThe expression “correlation is not causation” is a common one, due to efforts by the tobacco industry to give plausible alternatives to the statistical association of cigarette smoking and health problems such as lung cancer. Indeed, in some instances correlation between two variables can be related to a lurking or confounding variable. An example might be coffee consumption is positively associated with heart disease, yet is not necessarily the cause, as smoking is associated with both variables and may be the cause.\n\n\n\n\n4.2.2 Trend lines\nWe mentioned that the Pearson correlation is a measure of a linear association and the Spearman rank correlation a measure of monotonic association. We now discuss some basic trend lines that highlight the association in paired data, where there is response variable.\nWe use the language explanatory variable to describe the \\(x\\) variable, and response variable to describe the \\(y\\) variable, with the understanding that \\(x\\) might be experimentally controlled and \\(y\\) a random measured response. The “line” is an explanation of how the \\(y\\) values vary on average as the \\(x\\) values vary by the experimenter.\nThere are many choices for how to identify a trend line, though we will show one such way that is the most commonly used. To set it in a mathematical setting, a line can be parameterized by two values, \\(y = \\beta_0 + \\beta_1 x\\). Use the value \\(\\hat{y}_i = \\beta_0 + \\beta_1 x_i\\), which is the \\(y\\) value of the point on the line corresponding to \\(x_i\\). The residual amount is usually defined by \\(e_i = y_i - \\hat{y}_i\\). That is the vertical distance from the data point to the line. See Figure 4.9 for a visualization. One could argue that the distance to the line might be better than the vertical distance, but this choice leads to a very useful mathematical characterization.\n\n\n\n\n\nFigure 4.9: Two different lines fit to the data. For each the residuals are computed, squared, then summed. The least-squares regression line is the line that makes this quantity minimal for a given data set.\n\n\n\n\nLet \\(f(x)\\) be some function, then a measure of the distance of the data to the line might be the sum \\(\\sum_i f(e_i)\\). For example, using \\(f(x) = |x|\\), we have through findmin:\n\nF(x) = abs(x)\n\nresid_sum(x, y, F, b0, b1) =\n    sum(F, yi - (b0 + b1*xi) for (xi,yi) in zip(x,y))\nj(b0, b1) = resid_sum(l, w, F, b0, b1)\n\nxs = ys = -2 : 1/100 : 2\n_, k = findmin(j.(xs, ys'))\n\n(b0 = xs[first(k.I)], b1 = ys[last(k.I)])\n\n(b0 = 0.2, b1 = 0.0)\n\n\nThis non-rigorously identifies the values of the coefficients that minimize the sum in \\([-2,2]\\times[-2,2]\\) by searching for the smallest over a grid with resolution 1/100.\nUsing a different \\(F\\) changes the answer:\n\nF(x) = x^2\n\nresid_sum(x, y, f, b0, b1) =\n    sum(f, yi - (b0 + b1*xi) for (xi,yi) in zip(x,y))\nj(b0, b1) = resid_sum(l, w, F, b0, b1)\n\nxs = ys = -2 : 1/100 : 2\n_, k = findmin(j.(xs, ys'))\n\n(b0 = xs[first(k.I)], b1 = ys[last(k.I)])\n\n(b0 = -0.06, b1 = 0.21)\n\n\nWhen \\(F(x) = x^2\\), that is we seek to minimize the sum of the squared residuals, there is a mathematical formulation that can be solved to identify values for the constants. This is known as the method of least squares. There is a mathematical framework that this problem fits within that uses linear algebra.\nConsider the \\(n\\) linear equations of the form \\(y_i = \\beta_0 + \\beta_1 x_i\\). Written in a column form these are:\n\\[\\begin{align*}\ny_1 & = \\beta_0 + \\beta_1 x_1\\\\\ny_2 & = \\beta_0 + \\beta_1 x_2\\\\\n& \\vdots \\\\\ny_n & = \\beta_0 + \\beta_1 x_n\\\\\n\\end{align*}\\]\nThere are 2 unknowns, \\(\\beta_0\\) and \\(\\beta_1\\). Were there two equations (\\(n=2\\)), we could solve by substitution to find an expression for these in terms of \\((x_1,y_1)\\) and \\((x_2, y_2)\\). We algebraically solve this below with the help of a symbolic math package:\n\nusing SymPy\n@syms x[1:2] y[1:2] β[0:1]\neqns = [one.(x) x] * β .~ y\n\n2-element Vector{Sym}:\n x₁⋅β₁ + β₀ = y₁\n x₂⋅β₁ + β₀ = y₂\n\n\n\nsolve(eqns, β)\n\nDict{Any, Any} with 2 entries:\n  β₁ => (y₁ - y₂)/(x₁ - x₂)\n  β₀ => (x₁*y₂ - x₂*y₁)/(x₁ - x₂)\n\n\nMathematically for the linear problem, this last line is equivalent to solving the equation \\(A\\beta=y\\) where \\(A\\) is a matrix with one column of 1s and the other of \\(x\\):\n\nA = [one.(x) x]\n\n2×2 Matrix{Sym}:\n 1  x₁\n 1  x₂\n\n\n\nA \\ y\n\n2-element Vector{Sym}:\n (x₁*y₂ - x₂*y₁)/(x₁ - x₂)\n       (y₁ - y₂)/(x₁ - x₂)\n\n\nHowever, it is expected that there are more than 2 data points, so the line will in general be overdetermined. Symbolically, for really small \\(n\\), this can be solved symbolically:\n\n@syms x[1:3] y[1:3] β[0:1]\nA = [one.(x) x]\nsolve(sum(e^2 for e in A*β - y), β)\n\n2-element Vector{Tuple{Sym, Sym}}:\n (-x₁*β₁/3 - x₂*β₁/3 - x₃*β₁/3 + y₁/3 + y₂/3 + y₃/3 - sqrt(2)*sqrt(-x₁^2*β₁^2 + x₁*x₂*β₁^2 + x₁*x₃*β₁^2 + 2*x₁*y₁*β₁ - x₁*y₂*β₁ - x₁*y₃*β₁ - x₂^2*β₁^2 + x₂*x₃*β₁^2 - x₂*y₁*β₁ + 2*x₂*y₂*β₁ - x₂*y₃*β₁ - x₃^2*β₁^2 - x₃*y₁*β₁ - x₃*y₂*β₁ + 2*x₃*y₃*β₁ - y₁^2 + y₁*y₂ + y₁*y₃ - y₂^2 + y₂*y₃ - y₃^2)/3, β₁)\n (-x₁*β₁/3 - x₂*β₁/3 - x₃*β₁/3 + y₁/3 + y₂/3 + y₃/3 + sqrt(2)*sqrt(-x₁^2*β₁^2 + x₁*x₂*β₁^2 + x₁*x₃*β₁^2 + 2*x₁*y₁*β₁ - x₁*y₂*β₁ - x₁*y₃*β₁ - x₂^2*β₁^2 + x₂*x₃*β₁^2 - x₂*y₁*β₁ + 2*x₂*y₂*β₁ - x₂*y₃*β₁ - x₃^2*β₁^2 - x₃*y₁*β₁ - x₃*y₂*β₁ + 2*x₃*y₃*β₁ - y₁^2 + y₁*y₂ + y₁*y₃ - y₂^2 + y₂*y₃ - y₃^2)/3, β₁)\n\n\nThe symbolic output isn’t pretty, but, with some calculus skills, the above approach algebraically yields these values for the \\(\\beta\\)s in terms of the data, \\((x_i, y_i)\\), for any \\(n \\geq 2\\):\n\\[\n\\quad \\hat{\\beta}_1 = cor(x,y) \\cdot \\frac{s_y}{s_x}, \\quad \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}.\n\\]\nNumerically, the linear algebra problem implemented in the \\ operation used above solves the same least squares problem. So for our problem, we can get the coefficients via:\n\nA = [one.(l) l]\nA \\ w\n\n2-element Vector{Float64}:\n -0.048220327513871966\n  0.20124509405873592\n\n\nFor this identified line:\n\nthe slope (\\(\\hat{\\beta}_1\\)) is proportional to the correlation coefficient, the proportion being the ratio of the standard deviation in the response variable and standard deviation of the explanatory variable.\nthe point \\((\\bar{x}, \\bar{y})\\) is on the line.\nthe coefficients can be computed efficiently through linear algebra\n\nThe underlying machinery of linear algebra is given a different interface along with generalizations in the GLM (Bates et al. n.d.) package which implements the linear model through its lm function. We also add the StatsModels package, but this is included with GLM.\nThe statistical model to fit is specified through a formula defined using an interface from StatsModels. In this case the model is basically that w linearly depends on l and is written w ~ l, modeling the width of the petal in terms of the length.\n\nusing StatsModels, GLM\nres = fit(LinearModel, @formula(y ~ x), (y=w, x=l))\n\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\ny ~ 1 + x\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      t  Pr(>|t|)   Lower 95%  Upper 95%\n───────────────────────────────────────────────────────────────────────────\n(Intercept)  -0.0482203   0.121641   -0.40    0.6936  -0.292796    0.196356\nx             0.201245    0.0826325   2.44    0.0186   0.0351012   0.367389\n───────────────────────────────────────────────────────────────────────────\n\n\nThe formula is written as a macro, so the variables do not need to be symbols, as they are not immediately evaluated because the macro transforms them first. Macros are processed before the values are known. Formulas are templates, the data specification in lm fills in that template. In the above, a standard formula for regression is specified, the data specification assigns y to w and x to l.\nThe combination of the generic fit method with the LinearModel type is implemented in the lm function, used hereafter.\nIn the following, we use names from the data frame created by subset in the formula.\n\nd = subset(iris, :Species => ByRow(==(\"setosa\")))\nlm(@formula(PetalWidth ~ PetalLength), d)\n\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nPetalWidth ~ 1 + PetalLength\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      t  Pr(>|t|)   Lower 95%  Upper 95%\n───────────────────────────────────────────────────────────────────────────\n(Intercept)  -0.0482203   0.121641   -0.40    0.6936  -0.292796    0.196356\nPetalLength   0.201245    0.0826325   2.44    0.0186   0.0351012   0.367389\n───────────────────────────────────────────────────────────────────────────\n\n\nThe output has more detail to be explained later. For now, we only need to know that the method coef will extract the coefficients (in the first column) as a vector of length 2, which we assign to the values bhat0 and bhat1 below:\n\nscatter(jitter(l), jitter(w); legend=false)  # spread out values\nbhat0, bhat1 = coef(res)                     # the coefficients\nplot!(x -> bhat0 + bhat1 * x)                # `predict` does this generically\n\n\n\n\n\n\n\n\n\n\nA constant model\n\n\n\nThe linear model has much wider applicability than the simple regression model, \\(y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\). To see one example, we can fit the model \\(y_i = \\beta_0 + \\epsilon_i\\) using least squares:\n\nlm(@formula(y ~ 1), (y=l,)) |> coef\n\n1-element Vector{Float64}:\n 1.462\n\n\nThe estimate is the mean of l.\n\n\n\n\n4.2.3 Local regression\nThe method of least squares used above is called “regression.” It was popularized by Galton (Galton 1886) along with the phrase “regression to the mean.” The method of local regression (LOESS, with SS meaning scatterplot smoothing) essentially fits a regression line (or a degree \\(d\\) polynomial) to parts of the data then stitches them together. The result is a curve that summarizes any trend in the data between the response variable and the explanatory variable. Figure 4.10 shows some locally fit regression lines on the left, and on the right an identified loess line to show that the loess line here mostly follows the local regression models. The “loess” line was added to with these commands from the Loess package, which are wrapped up into a function to call later:\n\nimport Loess   # import so as not to conflict with other loaded packages\n\nfunction loess_line!(p, x, y; normalize=true, span=0.75, degree=2, kwargs...)\n  model = Loess.loess(x, y; normalize=normalize, span=span, degree=degree)\n  us = range(extrema(x)..., length=100)\n  plot!(p, us, Loess.predict(model, us); kwargs...)\nend\n\nloess_line! (generic function with 1 method)\n\n\nThe default value of the degree keyword is 2 for a quadratic fit to the data.\n\n\n\n\n\nFigure 4.10: On left, a scatter plot with some local regression lines (based on 10 points) layered on; on right, the same with a degree 1 loess line added.\n\n\n\n\n\n\n4.2.4 Robust linear models\nThe formula for the slope of the simple regression coefficient depends on \\(s_y/s_x\\). The sample standard deviations may be influenced greatly by as few as one outlying value. That is, they are not very resistant to outliers. A theory of robust linear models has been developed which are useful in allowing a relaxation on the standard distributional assumptions, relaxations that accommodate data with more outliers. The RobustModels package implements a number of such models, of which we illustrate just MEstimator{TukeyLoss} without comment as to the implemented algorithm.\nOne way to view data sets that benefit from robust models is to envision a process where the measured response values are given by the value on a regression line plus some “normally” distributed error plus some “contaminated” error which introduces outliers.\nTo simulate that in our length and width data, we simply add in a contaminant to some of the terms below which appear in Figure 4.11 in the upper-right corner. That figure shows the least-squares regression line is influenced by the 3 contaminated points, but that robust regression line is not.\n\nw′ = jitter(w)\nl′ = jitter(l)\nidx = sortperm(l′)\nw′, l′ = w′[idx], l′[idx]\n\nsigma_e = dispersion(res.model) # sqrt(deviance(res)/dof_residual(res))\ninds = [45, 48, 50]             # some indices\n\nw′[inds] = w'[inds] .+ 5*sigma_e  # contaminate 3 values\ncolor = fill(\"blue\", 50); color[inds] .= \"red\"\n\nscatter(l′, w′; legend=:topleft, color=color)\n\nfm = @formula(w ~ l)\nres = lm(fm, (w=w′, l=l′))\n\nus = range(minimum(l′), maximum(l′), 100)\nplot!(us, GLM.predict(res, (l=us,)); linewidth=4, label=\"lm\")\n\nimport RobustModels\nestimator = RobustModels.MEstimator{RobustModels.TukeyLoss}()\nres_robust = RobustModels.rlm(fm, (w=w′, l=l′), estimator)\n\nplot!(us, RobustModels.predict(res_robust, (l=us,)); linewidth=4, label=\"rlm\")\n\n\n\n\nFigure 4.11: The three data points in the upper right of this graphic are “contaminated” data. Their outlying nature has undue influence on the least-squares regression line (“lm”) pulling it up towards the points. The robust regression line (“rlm”) is not so influenced.\n\n\n\n\n\n\n4.2.5 Prediction\nIn the definition of loess_line! and Figure 4.11 a predict method from various packages was used to plot the trend line. The pattern was similar: predict(model, (l=data,)) created \\(y\\) values for the given explanatory values.\nThe language comes from the usage of statistical models.\nThe explanatory “variable”, in general, may consist of 0, 1, or more variables. Examples with 0 or 1 for lm were given. It is not hard to envision in the iris data that the petal length might be influenced by the species, the petal width, and even details of the sepal sizes. The word explanatory hints that the \\(y\\) values should be somehow explained by the covariates. Of course, statistical data is typically subject to variability, so even if all covariates are equal, different responses need not be the same. A statistical model may account for this by expressing a relationship:\n\nresponse = predicted + error\n\nFor our purposes, the “predicted” part comes from some parameterized function of the covariates and forms the structural model for the data. On average, it is assumed the errors are \\(0\\), but that an individual error introduces some randomness.\nThe parameters in the model are fitted by some process (such as lm) leaving the related decomposition\n\nresponse = fitted + residual\n\nIn GLM and other packages within the Julia ecosystem, the aptly named generic fit method is used to fit a model.\nThe “fitted” and “residual” values are based on the data and are known quantities; the “predicted” and “error” are postulated and may be unknowable. It is common to use the identified model as estimates for predicted values.\nIn the above examples, the predict method was used to do just that. The syntax for predict may have seemed more complicated than needed, but the model may have many covariates, so the specification needs a means to identify them. A Tables compatible specification of the data used in the prediction is used, in the above that was a named tuple.\nThe modeling environment of Julia has these generic methods for the parts of a model:\n\npredict(model, newdata): Make predictions for new values of the covariate(s).\nfitted(model): The predicted values for the covariate(s) used to identify the parameters in the model.\nresponse(model): The response values\nresiduals(model): The response values minus the fitted values\n\nThe residuals are not the errors, but give a sense of them. The residuals function returns the residual values for a model fit. For example, we have these summaries:\n\nmean(residuals(res)), std(residuals(res))\n\n(3.4638958368304885e-16, 0.39759594400019677)\n\n\n(The mean of the residuals is always 0 for a linear model, a consequence of the method of least squares.)\n\n\n4.2.6 Assessing the residuals\nIn the creation of Figure 4.11 the following value was used:\n\nsigma_e = dispersion(res.model)\n\n0.40171621927012974\n\n\nThis is the square root of the sum of the squared residuals (computed by deviance) divided by the degrees of freedom (in this case \\(n-2\\), but in general given by dof_residual):\n\nmse = deviance(res) / dof_residual(res) # sense of scale\nsqrt(mse)\n\n0.40171621927012974\n\n\nFor the simple linear regression model, the value \\(\\sum e_i^2\\) is minimized by the choice of the estimates for the \\(\\beta\\)s. This value is called the SSE, or sum of squared errors (or the SSR, sum of squared residuals). It can be found from the residuals or the generic method deviance:\n\nsum(ei^2 for ei in residuals(res)), deviance(res)\n\n(7.746044199584972, 7.746044199584973)\n\n\nThe deviance gives a sense of scale, but does not reflect the size of the data set, \\(n\\). The residual degrees of freedom of a model is \\(n-k -1\\) where \\(k\\) is the number of variables. Assuming an intercept, in simple linear regression \\(k=1\\)\n\nn = length(l) # or nobs(res)\nn - 1 - 1, dof_residual(res)\n\n(48, 48.0)\n\n\n(More generally, this is \\(n\\) minus the number of columns in the model matrix, which for this model is one for the intercept and one for the \\(x\\) values.)\nThe mean squared error (MSE) is the ratio of the SSE and the degrees of freedom, or \\((\\sum e_i^2)/(n-k-1)\\). This is like a sample variance for the error terms, the square root of it, like the standard deviation, a also measure of variability.\nThe residuals can be used to graphically assess the ability of the fitted model used to describe the data. There are a few standard graphics:2\n\nA fitted versus residuals plot. The residuals are the data less the structural part of the model. If the model fits well, the residuals should not show a pattern.\nA quantile plot of the residuals. For statistical inference, there are distributional assumptions made on the error values. This plot helps assess if typical assumption on the shape is valid.\nA scale location. For statistical inference, there are distributional assumptions made on the error values. This plot helps assess if typical assumption of equal variances is valid.\nResiduals versus leverage. As regression can be sensitive to influential outliers. This is assessed with Cook’s distance which measures the change if point is used to fit the data compared to if it is not.\n\nThe support for these graphics in Julia is not as developed as in R, say. As such, we create some simple functions for generating this plots.\nWe first define several helper functions:\n\nusing LinearAlgebra: diag\nhat(res) = (X = modelmatrix(res); X * inv(crossmodelmatrix(res)) * X')\nhatvalues(res) = diag(hat(res))\nfunction rstandard(res)\n    es = [ei/sqrt(1-hi) for (ei, hi) in zip(residuals(res), hatvalues(res))]\n    s = dispersion(res)\n    es / s\nend\n\nrstandard (generic function with 1 method)\n\n\nThe hat function computes a matrix that arises in the regression model, its diagonal entries are returned by hatvalues. These are used to scale the residuals to produce the standardized residuals, computed by rstandard. The generic leverage, defined in StatsBase, is intended to compute what hatvalues does, but it currently doesn’t have a method defined in GLM.\nThe 4 basic diagnostic plots follow those returned by default in R, though the R versions offer more insight into outlying values. In Figure 4.12 the 4 graphics are shown for the “setosa” species data with PetalWidth modeled by PetalLength.\n\nfunction fitted_versus_residuals(res)\n  p = plot(; legend=false)\n  scatter!(p, fitted(res), residuals(res);\n  xlab=\"Fitted\", ylab=\"Residuals\", title=\"Residuals vs. fitted\")\n  loess_line!(p, fitted(res), residuals(res))\n  p\nend\n\nfunction quantile_residuals(res)\n    qqnorm(rstandard(res);\n       legend=false,\n       title = \"Normal Q-Q\",\n       xlab=\"Theoretical quantiles\", ylab=\"Standardized residuals\")\nend\n\nfunction scale_location_plot(res)\n    x, y = fitted(res), rstandard(res)\n    y = sqrt.(abs.(y))\n\n    p = plot(; legend=false)\n    scatter!(p, x, y;\n        title=\"Scale location\",\n        xlab=\"Fitted\", ylab = \"Standardized residuals\")\n    loess_line!(p, x, y)\n    p\nend\n\nfunction residual_leverage(res)\n   x, y = cooksdistance(res), rstandard(res)\n\n   p = plot(; legend=false)\n   scatter!(p, x, y;\n    title = \"Residuals vs. leverage\",\n    xlab=\"Leverage\", ylab=\"Standardized residuals\")\n   loess_line!(p, x, y)\n   p\nend\n\nresidual_leverage (generic function with 1 method)\n\n\n\n\n\n\n\nFigure 4.12: Four diagnostic plots, following the standard ones of R\n\n\n\n\n\n\n\n\n\n\nAlternatives\n\n\n\nIn the above, we created functions to compute some diagnostic values. These are also defined in the LinRegOutliers package (al. 2021) along with much else. However, the interface is not quite compatible with the model results of lm, so we didn’t leverage that work here.\n\n\n\n\n4.2.7 Transformations\nThe lm function, as used above, expects linear relationships between the explanatory variable and the response variable. That is, a model of the type \\(y = \\beta_0 + \\beta_1 x + \\epsilon\\). There may be data for which some mathematical transformation is necessary in order to get this structural form.\nThe animals data set, defined below, measures the body size and brain size of various species of animals. The brain size depends on the body size, but it seems to be non-linear and better modeled by \\(e^y = e^{\\beta_0 + \\beta_1 x + \\epsilon}\\). In Figure 4.13 we can see scatter plots of the two data sets before and after a log transform.\n\nanimals = dataset(\"MASS\", \"Animals\")\ntransform!(animals, [:Brain, :Body] .=> ByRow(log) .=> [:logBrain, :logBody])\n\np1 = @df animals scatter(:Body, :Brain; legend=false,\n                         title=\"Brain by body\", xlab=\"Body\", ylab=\"Brain\")\np2 = @df animals scatter(:logBody, :logBrain; legend=false,\n                         title=\"log(Brain) by log(body)\",\n                         xlab=\"log(Body)\", ylab=\"log(Brain)\")\nplot(p1, p2; layout = (@layout [a b]))#, size=fig_size_2)\n\n\n\n\nFigure 4.13: Plot of animals data before and after log transform of the body and brain variables.\n\n\n\n\nThere is an automated group of transformations called power transformations; these are implemented in the BoxCoxTrans package. Its transform function identifies an appropriate monotonic transformation. We can see the results of the transformations of both variables here along with a regression line and a robust regression line:\n\nimport BoxCoxTrans\ntransform!(animals, [:Brain,:Body] .=> BoxCoxTrans.transform .=> [:bcBrain, :bcBody])\n\np = @df animals scatter(:bcBody, :bcBrain; label=nothing,\n                        title = \"Animals, Box-Cox transformed\",\n                        xlab = \"box-cox Body\", ylab = \"box-cox Brain\")\n\nfm = @formula(bcBrain ~ bcBody)\nres = lm(fm,  animals)\n\nestimator = RobustModels.MEstimator{RobustModels.TukeyLoss}()\nres_robust = RobustModels.rlm(fm, animals, estimator)\n\nus = range(extrema(animals.bcBody)..., length=100)\nplot!(us, GLM.predict(res, (bcBody = us,)), linewidth=4, label=\"lm\")\nplot!(us, RobustModels.predict(res_robust, (bcBody = us,)), linewidth=4, label=\"rlm\")\n\n\n\n\nFigure 4.14: The animals data after both the Body and Brain variables have been transformed through the Box-Cox method. This data set has 3 outliers that pull the lm regression line downward, but do not impact the robust regression line, which more accurately captures the trend for all but the 3 outliers (which happen to be dinosaur species).\n\n\n\n\n\n\n4.2.8 Grouping by a categorical variable\nThe iris data is an example of differences between the species, one more so than the other. The group argument to scatter instructs the coloring of the markers to vary across the values of the grouping variable, effectively showing three variables at once.\n\np_scatter = @df iris scatter(:PetalWidth, :PetalLength; group=:Species,\n      legend=false, alpha=0.33);\n\nIn Figure 4.15 the basic scatter plot for petal width modeled by petal length is shown in the left figure along with a fitted regression line. We set an alpha value for the scatterplot to better see the regression lines. The line itself was prepared with:\n\nm1 = lm(@formula(PetalLength ~ PetalWidth), iris)\n\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nPetalLength ~ 1 + PetalWidth\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────\n               Coef.  Std. Error      t  Pr(>|t|)  Lower 95%  Upper 95%\n───────────────────────────────────────────────────────────────────────\n(Intercept)  1.08356   0.072967   14.85    <1e-30   0.939366    1.22775\nPetalWidth   2.22994   0.0513962  43.39    <1e-85   2.12838     2.33151\n───────────────────────────────────────────────────────────────────────\n\n\n\nus = range(extrema(iris.PetalWidth)..., 100)\nvs = predict(m1, (PetalWidth = us, ))\nplot!(p_scatter, us, vs; title=\"iris data\");\n\nThe three clusters can influence the regression line fit to the scatter plot, that is species can be an influence, as would be expected. There are different ways for this expectation to be incorporated.\nFirst, suppose we simply adjust the fitted lines up or down for each cluster. That is, there is some additive effect for each type of species. As a example of something we discuss at more length later, we can fit this model by including Species as an additive term:\n\nm2 = lm(@formula(PetalLength ~ PetalWidth + Species), iris)\n\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nPetalLength ~ 1 + PetalWidth + Species\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────────────\n                       Coef.  Std. Error      t  Pr(>|t|)  Lower 95%  Upper 95%\n───────────────────────────────────────────────────────────────────────────────\n(Intercept)          1.2114    0.0652419  18.57    <1e-39   1.08246     1.34034\nPetalWidth           1.01871   0.152242    6.69    <1e-09   0.717829    1.31959\nSpecies: versicolor  1.69779   0.180948    9.38    <1e-15   1.34018     2.05541\nSpecies: virginica   2.27669   0.281325    8.09    <1e-12   1.7207      2.83269\n───────────────────────────────────────────────────────────────────────────────\n\n\nThe second row in the output of m2 has an identical interpretation as for m1 – it is the slope of the regression line. The first line of the output in m1 is the \\(x\\)-intercept, which moves the line up or down. Whereas the first of m2 is the \\(x\\) intercept for a line that describes just one of the species, in this case setosa. (A coding for the regression model with a categorical variable chooses one reference level, in this case “setosa.”). The 3rd and 4th lines are the slopes for the other two species.\nWe can plot these individually, one-by-one, in a similar manner as before, however when we call predict we include a level for :Species. The result is the middle figure in Figure 4.15.\n\np_scatter2 = deepcopy(p_scatter)\n\ngdf = groupby(iris, :Species)\nfor (k,d) ∈ pairs(gdf)   # GroupKey, SubDataFrame\n\n    s = string(k.Species)\n\n    us = range(extrema(d.PetalWidth)..., 100)\n    vs = predict(m2, DataFrame(PetalWidth=us, Species=s))\n    plot!(p_scatter2, us, vs; linewidth=5)\n\nend\n\nNow we identify different regression lines (slope and intercepts) for each cluster. This is done throuh a multiplicative model and is specified in the model formula of StatsModels with a *:\n\nm3 = lm(@formula(PetalLength ~ PetalWidth * Species), iris)\n\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, LinearAlgebra.CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nPetalLength ~ 1 + PetalWidth + Species + PetalWidth & Species\n\nCoefficients:\n─────────────────────────────────────────────────────────────────────────────────────────────\n                                     Coef.  Std. Error      t  Pr(>|t|)  Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────────────────────────────────\n(Intercept)                       1.32756     0.130933  10.14    <1e-17   1.06877     1.58636\nPetalWidth                        0.54649     0.490003   1.12    0.2666  -0.422038    1.51502\nSpecies: versicolor               0.453712    0.373701   1.21    0.2267  -0.284935    1.19236\nSpecies: virginica                2.91309     0.406031   7.17    <1e-10   2.11054     3.71564\nPetalWidth & Species: versicolor  1.32283     0.555241   2.38    0.0185   0.225359    2.42031\nPetalWidth & Species: virginica   0.100769    0.524837   0.19    0.8480  -0.936611    1.13815\n─────────────────────────────────────────────────────────────────────────────────────────────\n\n\nThe first four coefficients are interpreted similarly to those for m2, the remaining 2 summarize the interaction between the petal width and the species type.\nWe can produce a graphic, the right-most figure in Figure 4.15:\n\np_scatter3 = deepcopy(p_scatter)\n\ngdf = groupby(iris, :Species)\nfor (k,d) ∈ pairs(gdf)   # GroupKey, SubDataFrame\n\n    s = string(k.Species)\n\n    us = range(extrema(d.PetalWidth)..., 100)\n    vs = predict(m3, DataFrame(PetalWidth=us, Species=s))\n    plot!(p_scatter3, us, vs; linewidth=5)\n\nend\n\n\n\n\n\n\nFigure 4.15: Three scatterplots of petal width by petal length in the iris data set. The right-hand figure includes the regression line fit to all the data. The middle one fits regression lines identified through an additive model that includes the species. The slopes are parallel, but the lines have an additive shift. The right figure shows a multiplicative model wherein the slopes and intercepts for each species are chosen.\n\n\n\n\n\n\n\n\nal., S. et (2021), “LinRegOutliers: A julia package for detecting outliers in linear regression,” Journal of Open Source Software, 6. https://doi.org/10.21105/joss.02892.\n\n\nBates, D., and others (n.d.). GLM.jl. https://doi.org/10.5281.\n\n\nGalton, F. (1886), “Regression towards mediocrity in hereditary stature.” The Journal of the Anthropological Institute of Great Britain and Ireland, [Royal Anthropological Institute of Great Britain; Ireland, Wiley], 15, 246–263."
  },
  {
    "objectID": "EDA/categorical-data-julia.html#univariate-categorical-data",
    "href": "EDA/categorical-data-julia.html#univariate-categorical-data",
    "title": "5  Categorical data",
    "section": "5.1 Univariate categorical data",
    "text": "5.1 Univariate categorical data\nWe first start with a single categorical variable before turning our attention to one or more.\n\n5.1.1 Tabulations\nLet’s consider a data set from R’s MASS package on a student survey; in particular the Smoke variable, which is stored as a categorical variable. The levels method reports 4 levels for this variable:\n\nusing RDatasets\nsurvey = dataset(\"MASS\", \"survey\")\nsmokes = survey.Smoke\nlevels(smokes)\n\n4-element Vector{String}:\n \"Heavy\"\n \"Never\"\n \"Occas\"\n \"Regul\"\n\n\nHow frequent is each? We would need to tabulate to answer that question. This can be done by grouping and applying the nrow function from DataFrames:\n\ncombine(groupby(survey, :Smoke), nrow)\n\n\n5×2 DataFrameRowSmokenrowCat…?Int641missing12Heavy113Never1894Occas195Regul17\n\n\nSimilarly, we can split (with group from SplitApplyCombine) and apply length:\n\nimport SplitApplyCombine: group\n@chain survey begin\n    eachrow\n    copy\n    group(r -> r.Smoke, _)\n    map(length, _)\nend\n\n5-element Dictionaries.Dictionary{Any, Int64}\n CategoricalValue{String, UInt8} \"Never\" │ 189\n CategoricalValue{String, UInt8} \"Regul\" │ 17\n CategoricalValue{String, UInt8} \"Occas\" │ 19\n CategoricalValue{String, UInt8} \"Heavy\" │ 11\n                                 missing │ 1\n\n\nThe above easily tabulates the same, and would be useful if the data were not in a data frame.\nFor tabulation, a separate package has benefits. We use the FreqTables package and its freqtables function. This returns a named vector:\n\nusing FreqTables\ntbl = freqtable(smokes)\n\n5-element Named Vector{Int64}\nDim1    │ \n────────┼────\n\"Heavy\" │  11\n\"Never\" │ 189\n\"Occas\" │  19\n\"Regul\" │  17\nmissing │   1\n\n\nNamed vectors are from the NamedArrays package and offer indexing by name, similar to a data frame.\nEither way (and there are others), this shows for this data set, even in the 90s “Never” is by far the most common response for past smoking habits, and it shows there is 1 missing response.\n\n\n5.1.2 Visualizations\nTabulations can be visualized beyond using a table.\nA bar chart represents the above table using bars proportional to the counts. The bar function makes a bar chart for the labels in x and the given data. For this example, we have either have to add “missing” to the levels, or, as is done here, excise it from the data set.\n\nbar(levels(smokes), tbl[1:end-1]; legend=false)\n\n\n\n\nFigure 5.1: Basic barplot.\n\n\n\n\nAnother common graphic is to use a dot, not a bar, to represent the value. Still another graphic is the pie charts; common in the business world, but not favorites within introductory statistics."
  },
  {
    "objectID": "EDA/categorical-data-julia.html#paired-categorical-data",
    "href": "EDA/categorical-data-julia.html#paired-categorical-data",
    "title": "5  Categorical data",
    "section": "5.2 Paired categorical data",
    "text": "5.2 Paired categorical data\nIn the survey data set we could look at pairs of data where both are categorical and ask questions about the pair. For example, the data contains information on smoking and identified gender (Sex). Is one gender more likely to smoke?\nAgain, we can use grouping and apply to see the counts:\n\ntbl = combine(groupby(survey, [:Sex, :Smoke]), nrow)\nfirst(tbl, 3) # 3 of 10 rows displayed\n\n\n3×3 DataFrameRowSexSmokenrowCat…?Cat…?Int641missingNever12FemaleHeavy53FemaleNever99\n\n\nA contingency table is the more familiar means to view two-way categorical count data. A count of all combinations of the levels of one and the levels of the other is presented in a grid.\nWith unstack we can do this within DataFrames:\n\n@chain survey begin\n    select([:Sex, :Smoke])\n    dropmissing\n    groupby([:Sex, :Smoke])\n    combine(nrow => :value)\n    unstack(:Smoke, :value)\nend\n\n\n2×5 DataFrameRowSexHeavyNeverOccasRegulCat…Int64?Int64?Int64?Int64?1Female599952Male6891012\n\n\nThe above dropped missing values; to keep them in, the allowmissing argument may be specified to unstack:\n\n@chain survey begin\n    groupby([:Sex, :Smoke])\n    combine(nrow => :value)\n    unstack(:Smoke, :value; allowmissing=true)\nend\n\n\n3×6 DataFrameRowSexNeverHeavyOccasRegulmissingCat…?Int64?Int64?Int64?Int64?Int64?1missing1missingmissingmissingmissing2Female99595missing3Male89610121\n\n\nThe missing values can be replaced with 0 using the coalesce function which scans through its arguments returning the first that is not equal to missing:\n\n@chain survey begin\n    groupby([:Sex, :Smoke])\n    combine(nrow => :value)\n    unstack(:Smoke, :value; allowmissing=true)\n    transform(Not(1) .=> ByRow(x -> coalesce(x, 0)); renamecols=false)\nend\n\n\n3×6 DataFrameRowSexNeverHeavyOccasRegulmissingCat…?Int64Int64Int64Int64Int641missing100002Female9959503Male89610121\n\n\nMore conveniently, the freqtable command will produce contingency tables:\n\ntbl = freqtable(survey, :Sex, :Smoke)\n\n3×5 Named Matrix{Int64}\nSex ╲ Smoke │ \"Heavy\"  \"Never\"  \"Occas\"  \"Regul\"  missing\n────────────┼────────────────────────────────────────────\n\"Female\"    │       5       99        9        5        0\n\"Male\"      │       6       89       10       12        1\nmissing     │       0        1        0        0        0\n\n\nThe freqtable interface allows the user to pass in two variables of data, or, as above, a tabular data set and two variable names. The freqtable method summarized them with the levels of the first variables naming the rows, and levels of the second naming the columns.\nIt is essentially this function using DataFrames:\n\nfunction xtabs(d, f, g)\n    @chain d begin\n        groupby([f, g])\n        combine(nrow => :value)\n        unstack(g, :value; allowmissing=true)\n        transform(Not(1) .=> ByRow(x ->coalesce(x, 0)); renamecols=false)\n    end\nend\n\nxtabs (generic function with 1 method)\n\n\n\n5.2.1 Conditional distributions of two-way tables\nAt first glance, there does not seem to be much difference in the smoking variable between the identified genders. As tables may have many more counts in a given row or column, it can be helpful to take proportions of the rows or columns to compare. The FreqTables package provides the prop function to do so. By default, it takes a proportion of all the data; pass in margins=1 to get proportions for each row, margins=2 to get proportions for each column. For example, to compare the distribution of Smokes for each level of Sex, we take proportions across each row:\n\nprop(tbl; margins=1)  # check `sum(prop(tbl; margins=1); dims=2)` returns 1\n\n3×5 Named Matrix{Float64}\nSex ╲ Smoke │    \"Heavy\"     \"Never\"     \"Occas\"     \"Regul\"     missing\n────────────┼───────────────────────────────────────────────────────────\n\"Female\"    │  0.0423729    0.838983   0.0762712   0.0423729         0.0\n\"Male\"      │  0.0508475    0.754237   0.0847458    0.101695  0.00847458\nmissing     │        0.0         1.0         0.0         0.0         0.0\n\n\nThere does not seem to be big differences between the rows, indicating that the gender doesn’t seem to have an effect on the smoking prevalency.\nWhat about the exercise variable?\n\ntbl = freqtable(survey, :Exer, :Smoke)\nprop(tbl; margins=1)\n\n3×5 Named Matrix{Float64}\nExer ╲ Smoke │   \"Heavy\"    \"Never\"    \"Occas\"    \"Regul\"    missing\n─────────────┼──────────────────────────────────────────────────────\n\"Freq\"       │ 0.0608696   0.756522   0.104348  0.0782609        0.0\n\"None\"       │ 0.0416667       0.75      0.125  0.0416667  0.0416667\n\"Some\"       │ 0.0306122   0.857143  0.0408163  0.0714286        0.0\n\n\nAgain, not much difference across the levels of Exer.\nFinding the row (or column) proportions as above finds the conditional distribution for a given value. (Answering the question, say, what is the distribution of the second variable given the first variables has a specific level?)\n\nRow proportions with Data Frames.\nThe task of finding row proportions with a data frame can be similarly addressed by applying a transform to each row. The following uses a percentage scale:\n\nperc(x,Σ) = round(100 * x / Σ, digits=1)\nperc(x::Missing, Σ) = x\nperc(r) = map(x -> perc(x,sum(skipmissing(r))), r)\n\nfunction perc_table(d)\n    nms = names(d, Union{AbstractString, CategoricalValue})\n    combine(d, nms, AsTable(Not(nms)) => ByRow(perc) => AsTable; renamecols=false)\nend\n\nxtabs(survey, :Exer, :Smoke) |> perc_table\n\n\n3×6 DataFrameRowExerHeavyNeverOccasRegulmissingCat…Float64Float64Float64Float64Float641Freq6.175.710.47.80.02None4.275.012.54.24.23Some3.185.74.17.10.0\n\n\n\n\n\n5.2.2 Marginal distributions of two-way tables\nA marginal distribution from a two-way table is found by adding all the values in each row, or each column. With two-way tables generated from the full data, there are more direct ways to realize these, but from a two-way table, we just need to apply sum to each row or column. The sum function takes a dims argument to specify the dimension, which, in this case, is 2 for adding along the columns (the second dimension) and 1 for adding down the rows (the first dimension):1\n\nsum(tbl, dims=1) # kinda like `freqtable(survey.Smoke)`\n\n1×5 Named Matrix{Int64}\nExer ╲ Smoke │ \"Heavy\"  \"Never\"  \"Occas\"  \"Regul\"  missing\n─────────────┼────────────────────────────────────────────\nsum(Exer)    │      11      189       19       17        1\n\n\n\nsum(tbl, dims=2) # like `freqtable(survey.Exer)`\n\n3×1 Named Matrix{Int64}\nExer ╲ Smoke │ sum(Smoke)\n─────────────┼───────────\n\"Freq\"       │        115\n\"None\"       │         24\n\"Some\"       │         98\n\n\n\n\n5.2.3 Two-way tables from summarized data\nSuppose a data set was presented in the following two-way table:\n\nA two-way contingency table of fabricated data showing counts of student grades and mode of instruction.\n\n\nGrade\nIn person\nHybrid\nAsynchronous Online\n\n\n\n\nA - C\n10\n5\n5\n\n\nD\n10\n15\n10\n\n\nF\n5\n10\n10\n\n\n\nThis table could be stored as a two-way table in different ways. Here we show how to make this a data frame, then expand it to variables, then summarize.\n\ndf = DataFrame([\n(Grade=\"A-C\", IP=10, Hybrid=5,  Asynchronous=5),\n(Grade=\"D\",   IP=10, Hybrid=15, Asynchronous=10),\n(Grade=\"F\",   IP=5,  Hybrid=10, Asynchronous=10)\n])\n\n\n3×4 DataFrameRowGradeIPHybridAsynchronousStringInt64Int64Int641A-C10552D1015103F51010\n\n\nThere are 80 students summarized here:\n\nsum([sum(r[2:end]) for r in  eachrow(df)])\n\n80\n\n\nHere we make a data frame with 80 cases:\n\nddf = @chain df begin\n    stack(Not(:Grade), variable_name=:InstructionType)\n    transform(:value => ByRow(n -> 1:n); renamecols=false)\n    flatten(:value)\n    select(Not(:value))\nend\n\ndescribe(ddf)\n\n\n2×7 DataFrameRowvariablemeanminmedianmaxnmissingeltypeSymbolNothingStringNothingStringInt64DataType1GradeA-CF0String2InstructionTypeAsynchronousIP0String\n\n\n(It isn’t the most efficient, but we utilize flatten to repeat the values easily created by the range operator, 1:n above. As these values are not of interest, we subsequently drop them.)\nTo see we could return to the original table, we first give the InstructionType the right ordering of the levels, then create a frequency table:\n\nordered_levels = [\"IP\", \"Hybrid\", \"Asynchronous\"]\nddf.InstructionType = categorical(ddf.InstructionType;\n                                       ordered=true, levels=ordered_levels)\nfreqtable(ddf, :Grade, :InstructionType)\n\n3×3 Named Matrix{Int64}\nGrade ╲ InstructionType │           \"IP\"        \"Hybrid\"  \"Asynchronous\"\n────────────────────────┼───────────────────────────────────────────────\nA-C                     │             10               5               5\nD                       │             10              15              10\nF                       │              5              10              10\n\n\n\n\n5.2.4 Graphical summaries of two-way contingency tables\nWe review a few visualizations of dependent categorical variables.\n\nGrouped bar plots\nThe bar plot for a single categorical variable shows frequency counts for each level. A grouped bar plot shows a distribution of the second variable for the grouping variable.\nA useful data structure for this graphic is found using groupby with 2 variables:\n\ntbl = @chain survey begin\n    select([:Sex, :Smoke])\n    dropmissing\n    groupby([:Sex, :Smoke])\n    combine(nrow => :value)\nend\n\np1 = @df tbl groupedbar(:Sex, :value, group=:Smoke; xlab=\"Identified gender\")\np2 = @df tbl groupedbar(:Sex, :value, group=:Smoke; xlab=\"Identified gender\",\n                        bar_position = :stack)\n\nplot(p1, p2, layout = (@layout [a b]))\n\n\n\n\nFigure 5.2: Grouped bar chart of smoking distribution for different levels of the :Sex variable. The bar_position argument can be passed a value :stack to use a stacked display.\n\n\n\n\nAs seen in the left graphic of Figure 5.2, there are groups of bars for each level of the first variable (:Sex); the groups represent the variable passed to the group keyword argument. The values are looked up in the data frame with the computed column that was named :value through the combine function.\nThe same graphic on the left – without the labeling – is also made more directly with groupedbar(freqtable(survey, :Sex, :Smoke))\n\n\nAndrews plot\nAn Andrews plot is implemented in StatsPlots showing differences in a collection of numeric variables for a given categorical variable. For each row, a trigonometric polynomial with coefficients given by the numeric values in the given row creates a function which is plotted. If the values across the categorical variable are similar, the graphs will be; if not, then the groupings will show up.\nWe first show an example with the iris data where the categorical value is :Species and the numeric ones the first 4 values. This will be shown in the left graphic of Figure 5.3:\n\niris = dataset(\"datasets\", \"iris\")\nandrews_1 = @df iris andrewsplot(:Species, cols(1:4));\n\nFor the next plot, we use the survey data. There are some efforts needed to wrangle the data: we convert the categorical variables to the numeric levels (levelcode) except for :Sex which we use for the grouping variable. We also drop any missing values:\n\niscategorical(x) = isa(x, CategoricalArray) # predicate function\ntbl = @chain survey begin\n    combine(_, :Sex, findall(iscategorical, eachcol(_))[2:end] .=> ByRow(levelcode);\n            renamecols=false)\n    dropmissing\nend\n\nandrews_2 = @df tbl andrewsplot(:Sex, cols(2:ncol(tbl)));\n\n\n\n\n\n\nFigure 5.3: Andrews plots of the iris data (left graphic) and the survey data (right graphic). The iris plot shows clear differences based on the :Species variable; the survey data does not for the :Sex variable.\n\n\n\n\n\n\nMosaic plots\nA mosaic plot presents a graphical view of a two-way contingency table. These are somewhat similar to the grouped bar plot with stacking, but the width of the bars depends on the frequency of the given level.\nThis graphic is not part of StatsPlots. We borrow with modification this implementation from OnlineStats:\n\nusing FreqTables\nmosaic_plot(f, g; kwargs...) = mosaic_plot!(Plots.Plot(), f, g; kwargs...)\nfunction mosaic_plot!(p, f, g; xrotation=-45, kwargs...)\n    tbl = freqtable(f, g)\n    a = sum(tbl, dims=2)\n    b = sum(tbl, dims=1)\n\n    a′ = [a.array...]\n    x = vcat(0, cumsum(a′)) / sum(a′)\n\n    tbl′ = convert(Matrix{Float64}, tbl.array)\n    tbl′[tbl′ .== 0] .+= 1/10  # give some width when missing\n    m = prop(tbl′, margins=1)\n    y = reverse(cumsum(prop(tbl′, margins=1), dims=2), dims=2)\n\n    bar!(p,\n         midpoints(x), y;\n         legend=false,\n         bar_width = diff(x),\n         xlims=(0,1), ylims=(0,1), linewidth = 0.5,\n         xticks =  (midpoints(x), string.(names(a,1))),\n         xrotation=xrotation, xmirror=true,\n         yticks = (midpoints(vcat(y[1,:],0)), reverse(string.(names(b,2)))),\n         kwargs...\n         )\nend\n\nmosaic_plot! (generic function with 1 method)\n\n\nFor the first variable, displayed on the \\(x\\)-axis, the relative width of the bar is proportional to the marginal proportions; for each level on the \\(x\\)-axis, the vertical bars show the relative proportions of the second variable. For example,\n\n@df survey mosaic_plot(:Exer, :Smoke, xlab=\"Exercise\", ylab=\"Smoke\")\n\n\n\n\nFigure 5.4: Mosaic plot of the Exercise and Smoke variables in the survey data set."
  },
  {
    "objectID": "EDA/makie.html#univariate-graphical-summaries",
    "href": "EDA/makie.html#univariate-graphical-summaries",
    "title": "6  The AlgebraOfGraphics package",
    "section": "6.1 Univariate graphical summaries",
    "text": "6.1 Univariate graphical summaries\nWe run through the basic graphics for univariate statistics. We shall see that the framework makes multi-variate display quite direct, and at times easier than a univariate display.\n\n6.1.1 Boxplot and violin plot\nA boxplot (Figure 6.1) for each species is created by composing a series of declarative commands:\n\np = data(penguins) *\n    visual(BoxPlot) *\n    mapping(:species, :bill_length_mm => \"Bill length (mm)\", color=:species);\n\nThis illustrates many of the idioms used in the AlgebraOfGraphics.\nThe data(penguins) command sets up the data. Here a data frame is passed, but this can be any Tables compatible structure, such as a struct of arrays such as data((;x, y)) for some pair of variables x and y.\nThe mapping call takes values in the data to positions and attributes of the graphic. It uses position to identify the x, y, and (at times) z values for the graphic. The y variable specification above illustrates a mini language nearly identical to the DataFrames mini language. For a box plot, an indicator of the groups goes in the x position, the data values in the y position. The color=:species argument uses a mapping between the levels of the :species variable and color ramp to give the graphic a distinct color for each species. Omitting this argument produces a monotone graphic with the chosen theme.\nThe visual(BoxPlot) command declares the visualization or transformation to be used to view the data. The visual function expects a type indicating the plot type to use and optional keyword argument. In this case BoxPlot is the type associated with the Makie.boxplot function. At times this type must be qualified, such as with Text, for annotations.\nBoth the mapping and visual calls can be used to set attributes:\n\nvisual is used to set attributes for each element independent of the data. For example, a box plot has the argument orientation which is not data dependent, so is adjusted within the visual call.\nmapping is used to have attributes depend on values of a variable, like color is used above.\n\nThe attributes are those for the underlying plotting function. For visual(BoxPlot), these can be seen at the help page for boxplot, displayed with the command ?boxplot.\nThe mapping calls shows two uses of the mini language for data manipulation. The basic form is source => function => target and works very much like the DataFrames mini language does for select or transform, but unlike those, the function is always applied by row. This makes some transformations, such as \\(z\\)-scores not possible within this call – transformations requiring the entire column need to be done within the values passed to data. The abbreviated forms are just source, as used with the color=:species argument; source => function; and source => target, such as :bill_length_mm => \"bill length (mm)\" used to rename the variable for labeling purposes. When the source involves more than one column selector, tuples should be used to group them.\nA few functions are provided to bypass the usual mapping of the data. (For example, color maps levels of a factor to a color ramp behind the scenes.) Among these are nonnumeric to pass a numeric variable to a value expecting a categorical variable and verbatim to avoid this mapping. The latter, => verbatim, will be necessary to add when annotating a figure.\nThe object p can be rendered to the screen with the draw method resulting in Figure 6.1. Just draw(p) will render the graphic, the following also shows how the figure keyword argument can be used to set attributes using a named tuple, in this case the figure size. Similarly axis values can be modified in this manner. In the following, we set a title attribute for the axis.\n\ndraw(p; figure=(;resolution=(600,400)),\n     axis=(; title=\"Bill length\"))\n\n\n\n\nFigure 6.1: Boxplots of bill length for the three penguin species in penguins.\n\n\n\n\nThis is the basic pattern where different choices are combined, or merged, with the * operation. The pieces can be re-purposed. In the following, we make use of this data:\n\nd = data(penguins);\n\nBox plots are very effective for quickly comparing distributions of a numeric variable across the levels of some factor. The calling syntax preferences that style, where both an x and y value are specified to mapping. To create a box plot of a single variable, without grouping, the graphic takes a bit more to construct. In the following we create a single valued x variable to produce the upper left graphic in Figure 6.2:\n\np1 = d * visual(BoxPlot) *\n    mapping(1 => one, :bill_length_mm => \"Bill length (mm)\");\n\nThe mini language is used above two different ways: with a function to create the single value for x (AlgebraOfGraphics will treat this to a factor, so one isn’t needed, just some single-valued function) and with a target for labeling the y variable. As mentioned, such transformations can also be done within the data frame before it is passed to data, which is necessary for some types of transformations.\nTo add another layer, in this case a scatter plot, we can add the plotting objects:\n\np2a = d * visual(BoxPlot) * mapping(:species, :bill_length_mm, color=:species)\np2b = d * visual(Scatter) * mapping(:species, :bill_length_mm)\np2 = p2a + p2b;\n\nThe Scatter transformation plots pairs of points in a Cartesian plane.\nCombinations with + add a layer; those with * merge layers. The algebra name also refers to algebraically desirable short cuts. For example, we repeat d and the mapping for each p2a and p2b, but these can be used just once by distributing them:\n\nm = mapping(:species, :bill_length_mm => \"bill length (mm)\", color=:species);\np3 =  d * ( visual(BoxPlot) + visual(Scatter) ) * m;\n\nBoth p2 and p3 are shown in the lower row of Figure 6.2. There is just one slight difference, the dots representing the data in p2 are not colored, as the mapping did not instruct that in forming p2b.\nSpecifying a violin plot requires just a slight modification to the above: we change the BoxPlot visual to Violin. Violin plots have an argument side that allows both sides of the violin to reflect an extra grouping variable. We use the :sex variable in the following, as it has only two levels. With this, each side of the violin plot reflects grouping by the :sex factor, the legend is used to lookup which level of the factor is represented.\n\np4 = d * visual(Violin) * mapping(:species, :bill_length_mm, color=:species, side=:sex);\n\nThe visual(Violin) call wraps the function Makie.violin whose documentation contains additional possible arguments beyond side.\nThe AlgebraOfGraphics package builds on the Makie package and can use its layout system. Makie’s layout system leverages matrix notation to specify cell position. The draw! method accepts a figure object as a first argument. In Figure 6.2 we layout 2 rows and 2 columns of figures, as follows:\n\nf = Figure()\ndraw!(f[1,1], p1)\ndraw!(f[1,2], p4)\ndraw!(f[2,1], p2)\ndraw!(f[2,2], p3)\nf\n\n\n\n\nFigure 6.2: Figure showing four different graphics displayed. In this case, a single boxplot; a violin plot; a boxplot with scatter; and a similar one with the data and mapping easily reused for each visual.\n\n\n\n\n\n\n6.1.2 Dot plot\nThe boxplot does an excellent job of summarizing a data set with a few indicators making it quite useful when there are many data points. A dot plot is useful when there are a limited number of values and advantageous as the graphic shows all the data.\nA dot plot (Figure 6.3) can be constructed easily enough by ensuring, in this case, the y variable is non-numeric:\n\nhuddle = penguins[sample(1:size(penguins,1), 50),:] # a sample\n\np1 = data(huddle) * visual(Scatter) *\n    mapping(:bill_length_mm=>\"Bill length (mm)\", :species => nonnumeric);\n\n(In this example, species is categorical, so the extra => nonnumeric is unnecessary.)\nCompare the above to a boxplot of the same sampled data:\n\np2 = data(huddle) * visual(BoxPlot) *\n    mapping(:species, :bill_length_mm => \"Bill length (mm)\"; color=:species);\n\nThe boxplot makes it easy to compare medians across the levels of the species factor to gauge graphically if there is a differentiated effect on the response.\nThe following is an enhanced dot plot which emphasizes a comparison of center by adding a line and sorting so that this line only moves to the right as the eye travels up the levels of the factor. The code is a modification of some from (Alday et al. 2022).\n\n\"`dotplot`: show values for each group as dotplot sorted by some center\"\nfunction _arrange_dotplot_data(df, value::Symbol, group::Symbol, center=mean;\n                              jitter=true)\n    transform!(df, value => Array, group => CategoricalArray;\n               renamecols=false) # set up types\n\n    sumry = combine(groupby(df, group), value => center => value)\n    sort!(sumry, value)\n    ordered_levels = string.(sumry[!, group])\n    levels!(sumry[!, group], ordered_levels) # relevel, used in plotting\n    levels!(df[!, group], ordered_levels)\n    jitter && (df = combine(groupby(df, group),\n                            value => (x -> x .+ std(x)/100), renamecols=false))\n\n\n    df, sumry\nend\n\n\ndf, sumry = _arrange_dotplot_data(huddle, :bill_length_mm, :species, median)\nmm = mapping(:bill_length_mm => \"Bill length (mm)\", :species)\np3 = data(df) * mm *\n    visual(Scatter; marker='○', markersize=12)  # use a character for a marker\np3 += data(sumry) * mm * visual(Lines);         # add summary line\n\nAll these figures appear in Figure 6.3.\n\n\n\n\n\nFigure 6.3: A basic dot plot, a comparable box plot, and an enhanced dot plot. For small data sets, the dot plot can show comparisons of spread and center quite well; reordering based on the center emphasizes the differentiated effect on the response of the grouping variable.\n\n\n\n\n\n\n6.1.3 Faceting\nThe package also supports faceting where different panels share the same scales allowing easy cross comparison. Faceting is specified through the keyword layout or either (or both) of row and col keywords. The layout keyword uses levels of the variable name it is passed and arranges the plots over these levels. A col declaration will make columns for each level of the specified variable, whereas a row declaration will create rows for each level of the specified variables. By default both the x and y axes are linked. These linkings can be decoupled when drawing by passing in values to the facet argument, along the lines of: draw(p, facet=(; linkxaxes=:none, linkyaxes=:none)).\n\n\n6.1.4 Histograms\nThe AlgebraOfGraphics has certain functions it refers to as transformations of the data. These include histogram, density, frequency, linear, smooth, and expectation; most all will be illustrated by example below.\nThese are used like visual was above, but arguments are passed directly to the transformation.\nThe histogram function plays the role of visual in this graphic. (The visual function is still useful to apply data-independent attributes.) Here we arrange to color by species:\n\np1 = d * histogram() * mapping(:bill_length_mm, color=:species);\n\nThe histograms overlap. The layout command can be used to declare one panel per level. We do this with :sex:\n\np2 = d * histogram() * mapping(:bill_length_mm, color=:species, layout=:sex);\n\nSee Figure 6.4 for the graphics.\n\n\n6.1.5 Density plot\nThe histogram function has options for overriding the default bin selection and has several options for scaling the figure through its normalization argument. We use this in the next graphic which layers a density plot over a scaled histogram using the :pdf scaling. The density transformation is qualified with the module name to prevent a conflict with one in Makie1.\n\nlayers = histogram(normalization=:pdf) + AlgebraOfGraphics.density()\np3 = d * layers * mapping(:bill_length_mm, color=:species, layout=:sex);\n\nIn this next figure we add in a scatter plot of the data on top of the density plots. For the scatter plot, we use the Scatter visual for which we create jittered \\(y\\) values to disambiguate the data, these are added as a column to the data in d1, below:\n\np4a = d *  AlgebraOfGraphics.density() *\n    mapping(:bill_length_mm, color=:species)\n\nd1 = data(transform(penguins,\n                    :bill_length_mm => ByRow(x -> 0.02 * rand()) => :ys))\n\np4b = d1 * visual(Scatter) * mapping(:bill_length_mm, :ys, color=:species)\np4 = p4a + p4b;\n\n\n\n\n\n\nFigure 6.4: Histogram and Density plots.\n\n\n\n\n\n\n6.1.6 Quantile-normal plots\nThe QQNorm and QQPlot visuals are used to make quantile-quantile plots; QQNorm expects a mapping to :x (first position) whereas QQPlot expects mappings to :x and :y (the first two positions).\nThe following will give a visual check if bill length is normally distributed, the graphic indicates slightly shorter tails than expected\n\np1 = data(penguins) * visual(QQNorm, qqline=:fit) *\n    mapping(:bill_length_mm);\n\nThe following will give a visual check if bill length has a similarly shaped distribution as bill depth, in this case with each species highlighted:\n\np2 = data(penguins) * visual(QQPlot, qqline=:fit) *\n    mapping(:bill_length_mm, :bill_depth_mm, color=:species);\n\nBoth are shown in Figure 6.5.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.5: Quantile-quantile plots. The left graphic uses a reference normal distribution (through QQNorm), the right one uses QQPlot to compare the distribution of two variables after grouping by species."
  },
  {
    "objectID": "EDA/makie.html#line-plots",
    "href": "EDA/makie.html#line-plots",
    "title": "6  The AlgebraOfGraphics package",
    "section": "6.2 Line plots",
    "text": "6.2 Line plots\nA scatter plot shows \\(x\\) and \\(y\\) pairs as points, a line plot connects these points. There are numerous ways to draw lines with the AlgebraOfGraphics including: visual(Lines), for connect-the-dots lines; visual(LinesFill), for shading; visual(HLines) and visual(VLines), for horizontal and vertical lines; visual(Rangebars) to draw vertical or horizontal line segments.\nThe graph of a function can be drawn using Lines, as in this example, where we add in different range bars to emphasize the role that the two parameters play in this function’s graph:\n\nϕ(x; μ=0, σ=1) = 1/sqrt(2*pi*σ^2) * exp(-(1/(2σ)) * (x - μ)^2)\n\nxs = range(-3, 3, length=251)\nys = ϕ.(xs)\nc = data((x=xs, y=ys)) * visual(Lines) * mapping(:x, :y)\n\nc += data(DataFrame(x=0, hi=ϕ(0), lo=0)) * visual(Rangebars) *\n    mapping(:x, :hi, :lo)\n\nc += data(DataFrame(xmin=0, xmax=1, y=ϕ(1))) * visual(Rangebars, direction=:x) *\n    mapping(:y, :xmin, :xmax)\n\nc += data((x=[1/10, 1/2], y=[0, ϕ(1)], label=[\"μ\", \"σ\"])) *\n    visual(Makie.Text) *\n    mapping(:x, :y, text = :label => verbatim)\ndraw(c)\n\n\n\n\nThe Rangebars visual has a direction argument, used above to make a horizontal range bar.\nThe annotation has two subtleties: the qualification of Makie.Text is needed, as there is a Text type in base Julia. More idiosyncratically, the use of verbatim in mapping is needed to avoid an attempt to map the labels to a glyph, such as a pre-defined marker."
  },
  {
    "objectID": "EDA/makie.html#bivariate-relationships",
    "href": "EDA/makie.html#bivariate-relationships",
    "title": "6  The AlgebraOfGraphics package",
    "section": "6.3 Bivariate relationships",
    "text": "6.3 Bivariate relationships\nScatterplots with trend lines are easily produced within the AlgebraOfGraphics framework: the Scatter visual creates scatter plots; for trend lines there is the smooth transformation to fit a loess line, and the linear transformation to fit linear models.\nThis first set of commands shows how to fit a smoother (upper left graphic in Figure 6.6). The smooth function has arguments which pass on to Loess.loess.\n\nlayers = visual(Scatter) + smooth()\np1 = d * layers * mapping(:bill_length_mm, :bill_depth_mm);\n\nThe linear function draws the fitted regression line and shades an interval automatically (the interval argument). Linear prediction under model assumptions provides a means to identify confidence intervals for the mean response (the average value were the covariates held fixed and the response repeatedly samples) and for the predicted response for a single observation. The latter are wider, as single observations have more variability than averages of observations. A value of nothing suppresses this aspect.\nThis next set of commands shows (upper-right figure of Figure 6.6) one way to add a linear regression line. As the mapping for linear does not include the grouping variable, (color) the line is based on all the data:\n\nd1 = d * mapping(:bill_length_mm, :bill_depth_mm)\np2a = d1 * visual(Scatter) * mapping(color=:species)\np2b = d1 * linear()\np2 = p2a + p2b;\n\nWhereas with this next specification, color is mapped for both the linear transformation and the Scatter visual. This groups the data and separate lines are fit to each. We can see (lower-left figure of Figure 6.6) that whereas the entire data shows a negative correlation, the cohorts are all positively correlated, an example of Simpson’s paradox.\n\nlayers = visual(Scatter) + linear()\np3 = d1 * layers *  mapping(color=:species);\n\nAdding layout=:sex shows more clearly (lower-right figure of Figure 6.6) that each group has a regression line fit, that is the multiplicative model is fit.\n\np4 = d1 * layers *  mapping(color=:species, layout=:sex);\n\n\n\n\n\n\nFigure 6.6: Scatter plots of bill depth by bill width produced by varying specifications.\n\n\n\n\n\n6.3.1 Corner plot\nA corner plot, as produced by the PairPlots package through its pairplot function, is a quick plot to show pair-wise relations amongst multiple numeric values. The graphic uses the lower part of a grid to show paired scatterplots with, by default, contour lines highlighting the relationship. On the diagonal are univariate density plots.\n\nusing PairPlots\nnms = names(penguins, 3:5)\np = select(penguins, nms .=> replace.(nms, \"_mm\" => \"\", \"_\" => \" \")) # adjust names\npairplot(p)\n\n\n\n\n\n\n6.3.2 3D scatterplots\nA 3-d scatter plot of 3 numeric variables can be readily arranged, with just one unexpected trick:\n\nThe mapping object should contain an x, y, and z variable specification with numeric variables.\nThe draw call should include an axis = (type = Axis3,) call, specifying that a 3D (Makie) axis should be used in the display.\n\n\nd = data(penguins)\np = d * mapping(:bill_length_mm => :bl,  :bill_depth_mm => :bd,  :flipper_length_mm=>:fl; color=:species,\n              row=:sex, col=:island)\ndraw(p, axis=((type=Axis3,)))\n\n\n\n\nFigure 6.7: 3D scatter plots of bill length, bill depth, and flipper length with faceting by island and sex variables."
  },
  {
    "objectID": "EDA/makie.html#categorical-data",
    "href": "EDA/makie.html#categorical-data",
    "title": "6  The AlgebraOfGraphics package",
    "section": "6.4 Categorical data",
    "text": "6.4 Categorical data\nThe distribution of the surveyed species is not the same. A bar chart can illustrate (upper-left graphic of Figure 6.8). The frequency transform does the counting:\n\np1 = d * frequency() * mapping(:species);\n\nTwo categories can be illustrated, we need dodge set here to avoid overplotting of the bars. In this example, following the AlgebraOfGraphics tutorial, we add in information about the island. This shows (upper-right graphic of Figure 6.8) that two species are found on just 1 island, whereas Adelie is found on all three.\n\np2 = d * frequency() *\n    mapping(:species, color=:island, dodge=:island);\n\nUsing stack in place of dodge presents a stacked bar chart (lower-left graphic of Figure 6.8):\n\np3 = d * frequency() *\n    mapping(:species, color=:island, stack=:island);\n\nA third category can be introduced using layout, col, or row (lower-right graphic of Figure 6.8):\n\np4 = d * frequency() *\n    mapping(:species, color=:island, stack=:island) *\n    mapping(row=:sex);\n\n\n\n\n\n\nFigure 6.8: Scatter plots of bill depth by bill width produced by varying specifications."
  },
  {
    "objectID": "EDA/makie.html#customizing-plots-through-axis",
    "href": "EDA/makie.html#customizing-plots-through-axis",
    "title": "6  The AlgebraOfGraphics package",
    "section": "6.5 Customizing plots through axis",
    "text": "6.5 Customizing plots through axis\nThere are a numerous customizations available when drawing a plot. We discuss a small handful of them here. See the PumasAI tutorial and the documentation for more details.\nThe draw command allows the passing of values to the axis mechanism of Makie. This allows customization of various features such as the title, the ticks, the aspect ration, and the grids.\nMakie plots are themeable. In the above we used set_aog_theme!(). This theme sets a number of defaults for the axis attributes:\nAxis = (\n        xgridvisible=false,\n        ygridvisible=false,\n        topspinevisible=false,\n        rightspinevisible=false,\n        bottomspinecolor=:darkgray,\n        leftspinecolor=:darkgray,\n        xtickcolor=:darkgray,\n        ytickcolor=:darkgray,\n        xticklabelfont=lightfont,\n        yticklabelfont=lightfont,\n        xlabelfont=mediumfont,\n        ylabelfont=mediumfont,\n        titlefont=mediumfont,\n    )\nTo override these or pass other attributes on to the rendering, the axis keyword argument accepts a named tuple of values. So, for example, to set the graphics title, we would see axis=(; title=\"Some title\"), to instruct the labels in a barplot on the x axis to be rotated, we would see axis=(;xticklabelrotation = pi/2). Of course these would typically combined, as above.\nThe following lists some useful attributes. A complete list is in the Makie docs for the Axis constructor.\nThe aspect ratio for a graphic is adjustable through the aspect attribute XX\nThe following labeling attributes can be adjusted: title, subtitle, xlabel, ylabel. These take a string (or an observable) for the value to display. This value can be adjusted, for example, there are titlealign, titlecolor, titlefont, titlesize, and titlevisible attributes. Similar attributes exist for the other labels.\nAn axis has ticks. These are often numbers. For the ticks on an x axis there are attributes xticks, xtickcolor, xtickformat, xticksize, and xtickwidth. Similarly with y. There are also minor ticks, adjustable with, for example, xminorticks, xminortickcolor, xminorticksize, etc.\nFor ticks representing categorical values, labels are used. Attributes for tick labels include: xticklabelalign, xticklabelcolor, xticklabelfont, xticklabelrotation, and xticklabelsize.\nThe displayed grid is adjustable through attributes like xgridcolor, xgridstyle, xgridvisible, xgridwidth, along with “minor” versions.\nFor 3 dimension plots, the Axis3 object is used for display. This has similarly named attributes for z values.\n\n\n\n\nAlday, P., Kliegl, R., and Bates, D. (2022), Embrace uncertainty: Fitting mixed-effects models with julia, https://juliamixedmodels.github.io/EmbraceUncertainty/.\n\n\nDanisch, S., and Krumbiegel, J. (2021), “Makie.jl: Flexible high-performance data visualization for julia,” Journal of Open Source Software, The Open Journal, 6, 3349. https://doi.org/10.21105/joss.03349.\n\n\nVertechi, P., and others (n.d.). AlgebraOfGraphics.jl."
  },
  {
    "objectID": "Inference/distributions.html#probability",
    "href": "Inference/distributions.html#probability",
    "title": "7  Probability distributions",
    "section": "7.1 Probability",
    "text": "7.1 Probability\nThis section quickly reviews the basic concepts of probability.\nMathematically a probability is an assignment of numbers to a collection of events (sets) of a probability space. These values may be understood from a model or through long term frequencies. For example, consider the tossing of a fair coin. By writing “fair” the assumption is implicitly made that each side (heads or tails) is equally likely to occur on a given toss. That is a mathematical assumption. This can be reaffirmed by tossing the coin many times and counting the frequency of a heads occuring. If the coin is fair, the expectation is that heads will occur in about half the tosses.\nThe mathematical model involves a formalism of sample spaces and events. There are some subtleties due to infinite sets, but we limit our use of events to subsets of finite or countably infinite sets or intervals of the real line. A probability measure is a function \\(P\\) which assigns each event \\(E\\) a number with:\n\n\\(0 \\leq P(E) \\leq 1\\)\nThe probability of the empty event is \\(P(\\emptyset) = 0\\), the probability of the the sample space is \\(P(\\Omega) = 1\\).\nIf events \\(E_1, E_2, \\dots\\) are disjoint then the probability of their union is the sum of the individual probabilities.\n\nA random variable, \\(X\\), is a function which takes an outcome (an element of an event) in a sample space and assigns a number. Random variables naturally generate events through sets of the type \\(\\{X = k\\}\\) for some \\(k\\). This event being all outcomes for which \\(X\\) would be \\(k\\). Similarly, \\(\\{X \\leq a\\}\\) for some \\(a\\) describes an event. These are the typical events of statistics.\nThe rules of probability lead to a few well used formulas: \\(P(X \\leq a) = 1 - P(X > a)\\) and for discrete random variables \\(P(X \\leq k) = \\sum_{j \\leq k} P(X = j)\\). Both illustrated in Figure 7.1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.1: Illustrations of \\(P(X \\leq a)\\) = \\(1 - P(X > a)\\) for a discrete and continuous distribution.\n\n\nA distribution of a random variable is a description of the probabilities of events generated by a random variable. For our purposes, it is sufficient to describe events of the type \\(\\{X \\leq a\\}\\), \\(\\{X < a\\}\\), or \\(\\{X = a\\}\\), others being formed through intersections and unions. A valid description of the cumulative distribution function, \\(F(a) = P(X \\leq a)\\), (abbreviated cdf) describes the distribution of a random variable.\nThere are 2 types of distributions where a related function describes the distribution, discrete and continuous distributions.\nA discrete random variable is one which has \\(P(X = k) > 0\\) for at most a finite or countably infinite set of numbers. For example, if \\(X\\) is the number of coin tosses producing heads in \\(n\\) tosses, then the finite \\(k\\)s are \\(0,1,2, \\dots, n\\). Whereas, if \\(X\\) is the number of coin tosses needed to toss one head. The \\(k\\) would be \\(1, 2, \\dots\\) (with no upper bound, as one could get extremely unlucky). For discrete random variables it is enough to describe \\(f(k) = P(X=k)\\) for the valid \\(k\\)s. The function \\(f(k)\\) is called the pdf (probability distribution function). An immediate consequence is \\(\\sum_k f(k) = 1\\) and \\(f(k) \\geq 0\\).\nA continuous random variable is described by a function \\(f(x)\\) where \\(P(X \\leq a)\\) is given by the area under \\(f(x)\\) between \\(-\\infty\\) and \\(a\\). The function \\(f(x)\\) is called the pdf (probability density function). An immediate consequence is the total area under \\(f(x)\\) is \\(1\\) and \\(f(x) \\geq 0\\).\nWhen defined, the pdf is the basic description of the distribution of a random variable. It says what is possible and how likely possible things are. For the two cases above, this is done differently. In the discrete case, the possible values are all \\(k\\) where \\(f(k) =P(X=k) > 0\\), but not all values are equally likely unless \\(f(k)\\) is a constant. For the continuous case there are no values with \\(P(X=k) > 0\\), as probabilities are assigned to area, and the corresponding area to this event, for any \\(k\\), is \\(0\\). Rather, values can only appear in itervals with positive area (\\(f(x) > 0\\) within this interval) and for equal-length intervals, those with more area above them are more likely to contain values.\nA data set in statistics, \\(x_1, x_2, \\dots, x_n\\), is typically modeled by a collection of random variables, \\(X_1, X_2, \\dots, X_n\\). That is, the random variables describe the possible values that can be collected, the values (\\(x_1, x_2,\\dots\\)) describe the actual values that were collected. Put differently, random variables describe what can happen before a measurement, the values are the result of the measurement.\nThe joint cumulative distribution is the probability \\(P(X_1 \\leq a_1, X_2 \\leq a_2, \\dots X_n \\leq a_n)\\). A common assumption made for statistics is that each of the random variables is\n\nidentically distributed, meaning \\(F(a) = P(X_i \\leq a) = P(X_j \\leq a)\\) for each pair \\((i,j)\\).\nindependent, which means that knowing the value of \\(X_i\\) does not effect the probabilities regarding values for \\(X_j\\), \\(j \\neq i\\). (If you know a first toss of a fair coin is heads, it doesn’t change the odds that the second toss will be heads.)\n\nWith these two assumptions, the random variables \\(X_1, X_2, \\cdots, X_n\\) are termed an iid random sample. For an iid random sample, this simplification applies: \\(P(X_1 \\leq a_1, X_2 \\leq a_2, \\dots X_n \\leq a_n)=F(a_1)\\cdot F(a_2) \\cdots F(a_n)\\) (independence means “multiply”).\nIn general, the distribution of a random variable may be hard to describe. For example, suppose the random variable is the sample median, \\(M\\), of \\(X_1, X_2, \\cdots, X_n\\). With the assumption of an iid random sample, a formula for the distribution of \\(M\\) can be worked out in terms of the underlying pdf, \\(f\\) and cdf, \\(F\\). But without that assumption, it becomes intractable, in general.\nWhen a distribution can not be fully identified, it can still be described. A few basic summaries of a probability distribution are:\n\nThe mean or average value. For a discrete distribution this is \\(\\sum_k k P(X=k)\\), which is a weighted average of the possible values, weighted by how likely they are. For a continuous distribution, a similar formula using calculus concepts applies. Both can be viewed from a center of mass perspective. The symbol \\(\\mu\\) or \\(E(X)\\) (expectation) is used to represent the mean.1\nThe variance of a probability distribution describes the spread; the standard deviation is the square root of the variance. For a random variable, the variance is described by the average value of the centered random variable, squared: \\(E( (X - \\mu)^2 )\\). The symbol \\(\\sigma^2\\) is used to represent the variance, \\(\\sigma\\) then is the standard deviation. \\(VAR(X)\\) is also used to represent the variance of a random variable, similarly \\(SD(X)\\) is used for the standard deviation.\n\nThe transformation \\(X - \\mu = X - E(X)\\) centers the random variable \\(X\\), so that \\(E(X-\\mu) = 0\\).\nThe transformation \\(Z = (X - \\mu)/\\sigma\\), following the \\(z\\)-score, centers and scales the random variable \\(X\\). The random variable \\(Z\\) has \\(E(Z) = 0\\) and \\(VAR(Z) = 1\\).\nFor a random sample \\(X_1, X_2, \\dots, X_n\\) the sum \\(S = \\sum_k X_k\\) has the property that \\(E(S) = \\sum_k E(X_k)\\) (“expectations add”). This is true even if the sample is not iid. For example, finding the average number of heads in \\(100\\) tosses of a fair coin is easy, it being \\(100 \\cdot (1/2) = 50\\), the \\(1/2\\) being the expectation of a single heads where \\(X_i=1\\) if heads, and \\(X_i=0\\) if tails.\nWhile \\(E(X_1 + X_2) = E(X_1) + E(X_2)\\), as expectations are linear and satisfy \\(E(aX+bY) = aE(X)+bE(Y)\\), it is not the case that \\(E(X_1 \\cdot X_2) = E(X_1) \\cdot E(X_2)\\) in general – though it is true when the two random variables are independent. As such, the variance of \\(S= \\sum_k X_k\\) is:\n\\[\nVAR(\\sum_k X_k) = \\sum_k VAR(X_k) + 2 \\sum_{i < j} COV(X_i, X_j),\n\\]\nwhere the covariance is \\(COV(X_i,X_j) = E((X_i - E(X_i)) \\cdot (X_j - E(X_j)))\\). If, the random variables in the sample are independent, then the covariances are \\(0\\) and the variance of a sum is the sum of the variances.\n\n7.1.1 Statistical language\nIn statistics we have seen a random sample is a sequence of random variables \\(X_1, X_2, \\dots, X_n\\). Assume this is an iid random sample.\nThe population is the common distribution of each random variable in the random sample. Populations have a pdf and cdf, often denoted \\(f_X(x)\\) and \\(F_X(x)\\). Populations are summarized by parameters, such as the mean (\\(\\mu\\)) or the standard deviation (\\(\\sigma\\)).\nA statistic is some summary of a random sample. For example, the median, or middle value, or the sample mean \\(\\bar{X} = (X_1 + X_2 + \\cdots X_n)/n\\). Statistics are also random variables and so are described by a distribution (when computable) or summarized by values such as the mean or standard deviation.\nFor the sample mean from an iid random sample, the above says:\n\\[\\begin{align*}\nE(\\bar{X}) &=  E\\left(\\frac{X_1 + X_2 + \\cdots + X_n}{n}\\right) \\\\\n&= \\frac{1}{n} \\sum_k E(X_k) \\\\\n&= \\frac{1}{n} \\sum_k \\mu = \\mu;\\\\\nVAR(\\bar{X}) &= \\frac{1}{n^2} \\cdot \\left(\\sum_k VAR(X_k) + 2\\sum_{i < j} COV(X_i,X_j)\\right)\\\\\n&= \\frac{1}{n^2}\\sum_k VAR(X_k) \\\\\n&= \\frac{1}{n^2} n \\cdot \\sigma^2 = \\frac{\\sigma^2}{n}.\n\\end{align*}\\]\nThe standard deviation then is \\(SD(\\bar{X}) = \\sigma/\\sqrt{n}\\).\nIn short, the expected value of the mean is the expected value of the population; the variance of the mean (of an iid random sample) is the variance of the population divided by the sample size, \\(n\\). The latter speaks to variability: there is more variability in a single random value than in an average of the random values. The distribution of \\(\\bar{X}\\) can be expressed through formulas in terms of \\(f_X(x)\\), but is well approximated as \\(n\\) gets large by a distribution characterized by its mean and standard deviation, as will be seen.\nThere are parallels between random variables and data sets:\n\nThe random sample \\(X_1, X_2, \\dots, X_n\\) is realized in a data set by values \\(x_1, x_2, ... x_n\\).\nThe distribution, \\(f_X(x)\\), is reflected in the data set.\nThe distribution, \\(f_X(x)\\) is typically described by key parameters\nThe data set is typically summarized by sample statistics.\n\nStatistical inference makes statements using the language of probability about the parameters in terms of various sample statistics.\nAn intuitive example is the tossing of a fair coin modeling heads by a \\(1\\) and tails by a \\(0\\) then we can parameterize the distribution by \\(f(1) = P(X=1) = p\\) and \\(f(0) = P(X=0) = 1 - P(X=1) = 1-p\\). This distribution is summarized by \\(\\mu=p\\), \\(\\sigma = \\sqrt{p(1-p)}\\). A fair coin would have \\(p=1/2\\). A sequence of coin tosses, say H,T,T,H,H might be modeled by a sequence of iid random variables, each having this distribution. The we might expect a few things, where \\(\\hat{p}\\) below is the proportion of heads in the \\(n\\) tosses:\n\nA given data set is not random, but it may be viewed as the result of a random process and had that process been run again would likely result in a different outcome. These different outcomes may be described probabalistically in terms of a distribution.\nIf \\(n\\) is large enough, the sample proportion \\(\\hat{p}\\) should be close to the population proportion \\(p\\).\nWere the sampling repeated, the variation in the values of \\(\\hat{p}\\) should be smaller for larger sample sizes, \\(n\\).\n\nThese imprecise statements can be made more precise, as will be seen. The right graphic in Figure 7.2 shows this for a bell-shaped population. Each of the 10 rows of dots shows a random sample of size \\(n=9\\) which is summarized by its sample mean with a red star. The boxplot summarizes the red stars. The spread of the bell shaped population is much greater than the spread of the empirical density plot summarizing the sample means. By the above, we’d expect the standard deviation of the population to be \\(\\sqrt{9}\\) times bigger than the standard deviation of the sample mean statistic.\n\n\n\n\n\nFigure 7.2: The top-left figure shows a population, the lower left parameters associated with the population. The top-right shows one sample from the population in red, other possible samples in blue with sample means summarized in the bottom most row. A density estimate of these sample means appears in the lower right figure showing how it is centered on the population, but more concentrated. Inference uses the language of probability to characterize a population parameter based on a single random sample.\n\n\n\n\nHowever, the center of the population and the center of the empirical density plot are nearly the same. This is because the mean of the population is the same as the mean of the sample-mean statistic.\nInferential statistics is similar: A population is a distribution summarized by parameters. A random sample drawn from a population is summarized by statistics. A statistic summarizes just one sample, but the language of probability is used to infer from that one sample, statements about the parameters which would be apparent were there many different random samples."
  },
  {
    "objectID": "Inference/distributions.html#the-distributions-package",
    "href": "Inference/distributions.html#the-distributions-package",
    "title": "7  Probability distributions",
    "section": "7.2 The Distributions package",
    "text": "7.2 The Distributions package\nWhile populations are in general described by a cdf, populations which are discrete or continuous are more naturally described by their pdf. There are many standard pdfs used to describe different types of randomness. Many of these are supported in the Distributions package. This section reviews several.\n\nusing Distributions\n\n\n7.2.1 Bernoulli, Binomial, Geometric\nThe simplest non-trivial distribution is that used to model a coin toss: \\(P(X=1)=p\\) and \\(P(X=0) = 1-p\\). We see this is parameterized by \\(p\\), the probability of heads (say). This also happens to be the mean of this distribution. By the rules of probability, we must have \\(0 \\leq p \\leq 1\\).\nIn the Distributions package, named distributions are given a type, in this case Bernoulli which requires a value of \\(p\\) to be formed. The following code uses the type and confirms the mean of this distribution is indeed \\(p\\):\n\np = 0.25\nBe = Bernoulli(p)\nmean(Be)\n\n0.25\n\n\nThe Bernoulli(p) distribution is very simple, but combinations of \\(Bernoulli(p)\\) random variables give rise to many interesting distributions. For example, the number of heads in 10 coin tosses could be modeled by the sum of 10 Bernoulli random variables. The distribution of this random variable is described by the Binomial distribution. It has two parameters: \\(p\\) for the Bernoulli \\(10\\) for the number of tosses and is implemented with the Binomial(n,p) type. There are several summary statistics for the types supported by Distributions, including: mean, median, std:\n\nn, p = 10, 1/2\nB = Binomial(n, p)\nmean(B),  median(B), std(B)\n\n(5.0, 5, 1.5811388300841898)\n\n\nThe mean and standard deviation for the Binomial can be easily computed when this statistic is viewed as a sample mean of 10 Bernoulli random variables (\\(np\\) and \\(\\sqrt{np(1-p)}\\)).\nMore is known. For example, the value of this random variable can only be between 0 and 10. The extrema function combines both the minimum and maximum functions to give this:\n\nextrema(B)\n\n(0, 10)\n\n\nThe insupport function is similar, returning true or false if a value, x, is in the support of the distribution, that is if x is a possible value:\n\ninsupport(B, 5), insupport(B, 15), insupport(B, 5.5)\n\n(true, false, false)\n\n\nWe see only 5 is a possible value. Its probability, \\(P(X=5)\\) for this discrete distribution, is computed by pdf:\n\npdf(B, 5)\n\n0.24609375000000022\n\n\nFigure 7.3 shows different examples of the binomial distribution. When both \\(np\\) and \\(n(1-p)\\) are \\(5\\) or more, the shape is roughly symmetric.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.3: The binomial distribution for different \\(n\\) and \\(p\\) values. The left graphic fixes \\(n=20\\) and shows different \\(p\\) values. One is more “bell-shaped” as both \\(np\\) and \\(n(1-p)\\) are bigger than \\(5\\). The right graphic shows a fixed \\(p\\) and different \\(n\\)s. As \\(n\\) gets bigger, the shape is more bell shaped.\n\n\nThe cumulative distribution function is computed by cdf with the same calling style, cdf(D, x). The complementary cumulative distribution function, 1 - F(x), is computed by ccdf(D, x).\nThe cumulative distribution, \\(F(a)\\), finds the value \\(p\\) for a given value of \\(a\\) in \\(p = P(X \\leq a)\\). It’s inverse operation, finding a value \\(a\\) in \\(p=P(X \\leq a)\\) for a given \\(p\\), is returned by quantile.2 Quantiles are computed with the calling style quantile(D, p).\nA related question is the number of coin tosses needed to get the first heads. If \\(X\\) is this distribution, the the event \\(\\{X > k\\}\\) can be described as the first \\(k\\) tosses were tails. This probability is directly computable. The distribution of this number depends only on the parameter \\(p\\) and is called the Geometric distribution.\nFor example, we can see here the mean, and standard deviation, and also that the the number is unbounded:\n\np = 1/2\nG = Geometric(p)\nmean(G), std(G), extrema(G) # 1/p, 1/√p, 0,1,...,∞\n\n(1.0, 1.4142135623730951, (0, Inf))\n\n\n\n\n7.2.2 Uniform and DiscreteNonParametric distributions\nA discrete uniform random variable on \\(a, a+1, \\dots, b\\) assigns equal weight to each value. Hence, each possible value is equally likely. The DiscreteUniform(a,b) type models this distribution. For example, to model the roll of a 6-sided die, we might have:\n\nD = DiscreteUniform(1,6)\nmean(D), std(D)\n\n(3.5, 1.707825127659933)\n\n\nMore generally, a distribution which assigns weight \\(p_k\\) to values \\(x_1, x_2, \\dots, x_n\\) is modeled by the DiscreteNonParametric(xs, ps) distribution. For example, Benford’s “law” is an observation that the first non-zero digit of many data sets follows a certain pattern. For this the possible values are \\(1\\) through \\(9\\) and their probabilities are \\(f(k) = P(X=k) = \\log_{10}(k+1) - \\log_{10}(k)\\). We can model this with:\n\nxs = 1:9\nps = log10.(xs .+ 1) - log10.(xs)\nB = DiscreteNonParametric(xs, ps)\n\nDiscreteNonParametric{Int64, Float64, UnitRange{Int64}, Vector{Float64}}(\nsupport: 1:9\np: [0.3010299956639812, 0.17609125905568124, 0.12493873660829996, 0.09691001300805646, 0.07918124604762478, 0.06694678963061318, 0.057991946977686726, 0.05115252244738133, 0.04575749056067513]\n)\n\n\nWe can answer questions like the mean, the standard deviation, and what is the probability the number is 5 or less with:\n\nmean(B), std(B), cdf(B, 5)\n\n(3.4402369671232065, 2.4609982997506656, 0.7781512503836436)\n\n\n\na = 5\n\nks = support(B)\nps = pdf.(B, ks)\nas = ifelse.(ks .<= a, \"X ≤ $a\", \"X > $a\")\n\nlayers = visual(Rangebars) + visual(Scatter)\np = data((; ks, ps, as)) * layers * mapping(:ks, :ps, :ps => zero, color=:as)\n\ndraw(p)\n\n\n\n\nFigure 7.4: Benford’s law illustrated with values \\(5\\) or less illustrated\n\n\n\n\nIn Distributions the Categorical type can alse have been used to construct this distribution, it being a special case of DiscreteNonParametric with the xs being \\(1, \\dots, k\\).\nThe multinomial distribution is the distribution of counts for a sequence of \\(n\\) iid random variables from a Categorical distribution. This generalizes the binomial distribution. Let \\(X_i\\) be the number of type \\(i\\) in \\(n\\) samples. Then \\(X_1 + X_2 + \\cdots + X_k = n\\), so these are not independent. They have mean \\(E(X_i)=np_i\\), variance \\(VAR(X_i) = np_i (1-p_i)\\), like the binomial, but covariance \\(COV(X_i, X_j) = -np_i p_j, i \\neq j\\). (Negative, as large values for \\(X_i\\) correlate with smaller values for \\(X_j\\) when \\(i \\neq j\\).)\n\n\n7.2.3 The continuous uniform distribution\nThe continuous uniform distribution models equally likely outcomes over an interval \\([a,b]\\). (The endpoints possibly open, as mathematically they have no chance of being selected.) For example, the built-in rand function for Float64 values returns random numbers which are modeled by being uniform over \\([0,1)\\). (That they can be 0 but not 1 is a reality of how computers and mathematical models aren’t always exactly the same, but rather the model is a abstraction of the implementation.)\nThe Uniform(a,b) type models these numbers. Here we see rand being used to randomly sample \\(3\\) uniform numbers using a more general interface than the use of rand in base Julia:\n\nU = Uniform(0, 1)\nrand(U, 3)\n\n3-element Vector{Float64}:\n 0.8503731914181489\n 0.3229835570193518\n 0.044463821185537156\n\n\nIf \\(U\\) is Uniform on \\([0,1]\\) then \\(Y = aU + b\\) is uniform on \\([b, b+a]\\). The difference in means is shifted by \\(b + E(U)\\), the difference in standard deviations is scaled by \\(a\\). We can do the algebra of linear transformations using overloaded operations:\n\na, b = 2, 3\nY = a * U + b\nq1 = mean(U) == mean(Y) - (b + mean(U)) == (1 + 0)/2\nq2 = std(U) == std(Y)/a  == sqrt(1/12)\nq1, q2\n\n(true, true)\n\n\n\n\n7.2.4 The normal distribution\nThe most important distribution in statistics is called the normal distribution. This is a bell-shaped distribution completely described by its mean and standard deviation, Normal(mu, sigma) (though some math books use the variance, \\(\\sigma^2\\) for the second parameter). The standard normal has \\(\\mu=0\\) and \\(\\sigma=1\\). Standard normal random variables are generically denoted by \\(Z\\):\n\nZ = Normal(0, 1)\nmean(Z), std(Z)\n\n(0.0, 1.0)\n\n\nThere are many facts about standard normals that are useful to know. First we have the three rules of thumb – \\(68\\), \\(95\\), \\(99.7\\) – describing the amount of area above \\([-1,1]\\), \\([-2,2]\\), and \\([-3,3]\\). We can see these from the cdf with:\n\nbetween(Z, a, b) = cdf(Z,b) - cdf(Z,a)\nbetween(Z, -1, 1), between(Z, -2, 2), between(Z, -3, 3)\n\n(0.6826894921370861, 0.9544997361036416, 0.9973002039367398)\n\n\nThese values are important as the \\(z\\) scores of different data sets are often assumed to be normal or approximately normal, so, for example, about 95% of such a data set should have \\(z\\) scores within \\(-2\\) and \\(2\\).\nThe transformation \\(Y = \\mu + \\sigma \\cdot Z\\) will produce a normal with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), modeled more directly by Normal(mu, sigma).\nA common question in introductory statistics is what values \\(a\\) and \\(b\\) correspond to \\((1-\\alpha)\\cdot 100\\)% of the area under the pdf of \\(Y\\)? There are infinitely many such answers, but only one pair that is symmetric about the mean, \\(\\mu\\). This can be found with the help of quantile and the observation that since the pdf of \\(Y\\) is symmetric, there will be area \\(1 - \\alpha\\) plus \\(\\alpha/2\\) to the left of \\(b\\), a question tailor-made for quantile. From \\(b\\), \\(a\\) can be found from symmetry, it being equidistant from the mean as \\(b\\) is.\nFor example:\n\nalpha = 0.05\nmu, sigma = 10, 20\nY = Normal(mu, sigma)\nb = quantile(Y, 1 - alpha + alpha/2)\na = mu - (b - mu)\na, b, between(Y, a, b)\n\n(-29.19927969080115, 49.19927969080115, 0.9500000000000004)\n\n\nAn observation of De Moivre in 1733 about the binomial proved foundational. Suppose we consider the binomial distribution with \\(n\\) and \\(p\\) such that \\(np\\) and \\(n\\cdot(1-p)\\) are both \\(10\\) or more. Then \\(\\mu = np\\) and \\(\\sigma = \\sqrt{np(1-p)}\\). The cdf of this distribution was observed to be very well approximated by the cdf of the \\(Normal(\\mu, \\sigma)\\) distribution. Furthermore, asymptotically they are equal.\nFor example, the following searches for the largest discrepancy in the cdfs:\n\nn, p = 100, 1/3\nmu, sigma = n*p, sqrt(n*p*(1-p))\nB, Y = Binomial(n,p), Normal(mu, sigma)\nks = 0:100\nfindmax(abs(cdf(B, k) - cdf(Y, k)) for k in ks)\n\n(0.046989293624614625, 34)\n\n\nThis shows an error of not more than \\(0.0469...\\) in approximating the binomial with the normal. The continuity correction adjusts for the discrete nature of the binomial by comparing \\(k\\) to \\(k+1/2\\) for the normal. The following is more accurate, as can be seen:\n\nfindmax(abs(cdf(B, k) - cdf(Y, k+1/2)) for k in ks)\n\n(0.004701503073942237, 34)\n\n\nOf course, computationally it is often just as easy to call cdf(B, k) as it is to call cdf(Y, k + 1/2), so the advantage here is theoretical for computationally tractable values of \\(n\\) and \\(k\\). This particular approximation is generalized in the central limit theorem.\n\n\n7.2.5 The Chi-squared distribution\nLet \\(Z\\) be a standard normal random variable. As seen, a linear transform of \\(Z\\): \\(Y=\\sigma Z + \\mu\\) will have distribution of \\(Normal(\\mu,\\sigma)\\), a different transform \\(Y=Z^2\\) can have its distribution computed using some tricks, e.g. (\\(P(Z^2 < a) = P(-\\sqrt{a} < Z < \\sqrt{a})\\).) If we have an iid random sample, \\(Z_1, Z_2, \\dots, Z_n\\) the distribution of \\(\\chi^2 = Z_1^2 + Z_2^2 + \\cdots + Z_n^2\\) is of interest. (It may be viewed as the distance squared of a randomly chosen point in space, say.) The distribution of \\(\\chi^2\\) is the Chi-squared distribution with \\(n\\) degrees of freedom and is implemented in the Chisq(n) type.\n\n\n7.2.6 The T and F distributions\nThere are two main distributions which arise in the distribution of many sample statistics related to the linear regression model, these are the \\(T\\)-distribution of Student and the \\(F\\)-distribution of Fischer. Both are parameterized by “degrees of freedom,” which are more naturally discussed with sampling distributions.\nConsider two independent random variables, a standard normal \\(Z\\) and a Chi-squared random variable, \\(Y\\), with \\(\\nu\\) degrees of freedom. The \\(T\\)-distribution with \\(\\nu\\) degrees of freedom is the distribution of the ratio:\n\\[\nT = \\frac{Z}{\\sqrt{Y/\\nu}}.\n\\]\nThe \\(T\\)-distribution is a symmetric, bell-shaped distribution like the normal distribution, but has wider, fatter tails hence typically produces larger values in a sample. This can be seen heuristically with a relatively small degrees of freedom as follows:\n\nT5 = TDist(5)\nmaximum(abs, rand(T5, 100)), maximum(abs, rand(Z, 100))\n\n(3.0300315074385837, 2.5216209999074044)\n\n\nFigure 7.5 shows a quantile-normal plot of \\(T(3)\\) in the lower-left graphic. The long tails cause deviations in the pattern of points from a straight line.\nThe skewness method measures asymmetry. For both the \\(T\\) and the normal distributions – both bell shaped – this is \\(0\\). The kurtosis function measures excess kurtosis a measure of the size of the tails as compared to the normal. We can see:\n\nskewness(T5), skewness(Z), kurtosis(T5), kurtosis(Z)\n\n(0.0, 0.0, 6.0, 0.0)\n\n\nA distribution with excess kurtosis exceeding the normal is called leptokurtic. Such distributions more frequently produce values with \\(z\\) scores larger than expected by the standard normal distribution. A platykurtic distribution, on the other hand, would have in more frequently fewer values in the tails and fewer values close to the mean of a standard normal. The uniform distribution is a good example of a playtkurtic distribution, both conditions have a test in Distributions:\n\nisleptokurtic(T5), isplatykurtic(U)\n\n(true, true)\n\n\nFor the \\(T\\)-distribution, when the degrees of freedom gets large, say bigger than \\(100\\), the distribution is well approximated by the normal. Here we see a similar comparison as above. For a quantile-normal plot see the upper-right graphic in Figure 7.5.\n\nT100 = TDist(100)\nxs = range(-3, 3, length=100)\nfindmax(abs(cdf(T100, x) - cdf(Z, x)) for x in xs)\n\n(0.0015794482887208222, 25)\n\n\nIn Figure 7.5 the lower-right graphic is of the uniform distribution, the short tails, lead to a deviation from a straight line in this graphic; the lower-right graphic is of the skewed exponential distribution; the skew shows up in the lack of symmetry about the fitted line.\n\n\n\n\n\nFigure 7.5: Quantile-normal plots for different distributions – \\(T_3\\) is leptokurtic; \\(T_{100}\\) is approximately normal; \\(U\\) is platykurtic; and \\(E\\) is skewed.\n\n\n\n\nConsider now two, independent Chi-squared random variables \\(Y_1\\) and \\(Y_2\\) with \\(\\nu_1\\) and \\(\\nu_2\\) degrees of freedom. The ratio\n\\[\nF = \\frac{Y_1/\\nu_1}{Y_2/\\nu_2},\n\\]\nhas a known distribution called the \\(F\\)-distribution with \\(\\nu_1\\) and \\(\\nu_2\\) degrees of freedom.\nThis distribution has different shapes for different parameter values (Figure 7.6), with the distributions becoming more normal as the two parameters get large.\n\nkurtosis(FDist(5, 10)), kurtosis(FDist(5,100)), kurtosis(FDist(100,100))\n\n(50.86153846153846, 3.1474470779483217, 0.7278883188966445)\n\n\n\n\n\n\n\nFigure 7.6: Quantile-normal plots of the \\(F\\) distribution for different degrees of freedom. As the degrees of freedom values get bigger, the distribution is more normal."
  },
  {
    "objectID": "Inference/distributions.html#sampling-distributions",
    "href": "Inference/distributions.html#sampling-distributions",
    "title": "7  Probability distributions",
    "section": "7.3 Sampling distributions",
    "text": "7.3 Sampling distributions\nThe normal distribution, \\(T\\) distribution, and \\(F\\) distribution are important in statistics as they arise as sampling distributions of certain statistics.\n\n7.3.1 The sample mean\nFor an iid random sample, from a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) we have seen that the sample mean, \\(\\bar{X} = (X_1 + \\dots + X_n)/n\\) is described by having mean \\(\\mu_{\\bar{X}} = \\mu\\) and standard deviation \\(\\sigma_{\\bar{X}} = \\sigma / \\sqrt{n}\\).\nWhat about the distribution itself? This depends on the underlying population.\nIf the underlying population is normal, then \\(\\bar{X}\\) is normal. This is true as the sum of two independent normal random variables is normally distributed. That is \\(\\bar{X}\\) is bell shaped, centered at the same place as the distribution of any of the \\(X_i\\)s but has a narrower shape, as the standard deviation is smaller by a factor of \\(1/\\sqrt{n}\\).\n\nCentral limit theorem\nDe Moivre’s observation that the binomial distribution is asymptotically normal is even more general: the central limit theorem informs us that for large enough \\(n\\) the following is true under assumptions::\n\\[\nP(\\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\leq z) \\approx P(Z \\leq z),\n\\]\nwhere \\(Z\\) is a standard normal random variable. That is centering \\(\\bar{X}\\) by its mean and then scaling by its standard deviation results in a random variable that is approximately a standard normal if \\(n\\) is large enough.\nThe central limit theorem has some assumptions on the underlying population. The assumption that a mean and standard deviation exist are sufficient for application. It is true exactly for normal populations, for non-normal populations asymptotically so. For a binomial population, a rule of thumb is \\(np\\) and \\(n(1-p)\\) should be larger than \\(5\\) (\\(10\\) to be conservative), which avoids a skewed population.\nIn general, how large \\(n\\) needs to be depends on the underlying shape of the population: if the population has long tails (leptokurtic) or is very skewed, then \\(n\\) must be much larger than if the population is symmetric and platykurtic.\nFigure 7.7 shows simulations of the distribution of \\(\\bar{X}\\) for \\(n=10\\) for different populations. The key to simulations is the ability to draw random samples from the distribution, which is provided through the rand method for a distribution type, with rand(D,n) returning a random sample of size \\(n\\).\n\nxbar(D, n, N=200) = [mean(rand(D, n)) for _ in 1:N]\nsxbar(D, n, N=200) = (xbar(D,n,N) .- mean(D)) / (std(D)/sqrt(n))\n\nDs = (normal      = Normal(0,1),\n      leptokurtic = TDist(3),\n      skewed      = FDist(2,5),\n      platykurtic = Uniform(0,1))\ndf = DataFrame(D=collect(keys(Ds)), zs = [sxbar(D, 10) for D in values(Ds)])\n\nd = data(flatten(df, :zs))\np = d * visual(QQNorm, qqline=:fit) *\n    mapping(:zs, layout = :D, main=:D, color=:D)\n\ndraw(p)\n\n\n\n\nFigure 7.7: Quantile-normal plots of different random samples of \\(\\bar{X}\\) for \\(n=10\\). The underlying popluation is either normal, short tailed (playkurtic), long tailed (leptokurtic), or skewed. For the normal and short tailed populations the distribution of \\(\\bar{X}\\) is approximately normal for this small value of \\(n\\); for the other two populations, larger samples sizes are needed to see the aymptotic normality guaranteed by the central limit theorem.\n\n\n\n\n\n\nThe \\(T\\)-statistic\nThe central limit theorem requires standardizing by the population standard deviation. At times this is neither known or assumed to be known. However, the standard error may be known:\n\nThe standard error of a statistic is the standard deviation of the statistic or an estimate of the standard deviation.\n\nFor \\(\\bar{X}\\), the standard deviation is \\(\\sigma/\\sqrt{n}\\) the standard error is \\(SD(\\bar{X})/\\sqrt{n}\\), where, for a sample, the sample standard deviation, \\(s\\), is used to estimate the population standard deviation, \\(\\sigma\\).\nThe scaling in the central limit theorem can be redone using the standard error, \\(S/\\sqrt{n}\\):\n\\[\nT = \\frac{\\bar{X} - \\mu}{SE(\\bar{X})} = \\frac{\\bar{X} - \\mu}{S / \\sqrt{n}},\n\\]\nwhere \\(S\\) is the sample standard deviation of the data set \\(X_1, X_2, \\dots, X_n\\) :\n\\[\nS = \\sqrt{ \\frac{1}{n-1} \\sum_i^n (X_i - \\bar{X_i})^2}.\n\\]\nThe variability in the sampling means that the standard error will occasionally be much smaller than the population standard deviation, as this is in the denominator, the random variable \\(T\\) will have longer tails than a standard normal. That is, the sampling distribution of \\(T\\) should be leptokurtic, and it is:\n\nFor a normal population, the sampling distribution of \\(T\\) is the \\(T\\)-distribution with \\(n-1\\) degrees of freedom.\n\nIn general, a \\(T\\)-statistic for a parameter, \\(\\beta\\), is the difference between the estimate for the parameter (\\(\\hat{\\beta}\\)) and the parameter, divided by the standard error of the estimator: \\(T = (\\hat{\\beta}-\\beta)/SE(\\hat{\\beta})\\). This is of the pattern “(observed - expected) / SE.”\n\n\n\n7.3.2 The sample variance\nFor an iid random sample from a normal distribution the distribution of the sample variance \\(S^2\\) can be identified. Consider this algebraic treatment,\n\\[\\begin{align*}\nS^2 &= \\frac{1}{n-1} \\sum_i^n (X_i - E(X_i))^2\\\\\n&= \\frac{\\sigma^2}{n-1} \\sum_i^n \\left( \\frac{X_i - \\bar{X_i}}{\\sigma} \\right)^2\n\\end{align*}\\]\nIf the sample is an iid random sample of normals and were \\(\\bar{X_i}\\) replaced by \\(\\mu=E(X_i)\\), the sum would be of \\(n\\) iid squared standard normal which would be \\(Chisq(n)\\) distributed. However, that isn’t quite the case. Cochran’s theorem shows for an iid random sample from a normal population the distribution of \\(\\sum (Z_i - \\bar{Z})^2\\) is \\(Chisq(n-1)\\) and \\(\\bar{X}\\) and \\(S^2\\) are independent, which leads to:\n\nFor an iid random sample from a normal population \\((n-1) S^2/\\sigma^2\\) has a \\(Chisq(n-1)\\) distribution.\n\n\nThe \\(F\\) statistic\nWhen there are two independent samples, there are two sample means and standard deviations. The ratio of the sample standard deviations gives insight into the question of whether the two populations have the same spread. The \\(F\\)-statistic is given by:\n\\[\nF = \\frac{(S_1^2/\\sigma_1^2)}{(S_2^2/\\sigma_2^2)} = \\frac{(\\chi_{1,n_1-1}^2 / (n_1-1))}{(\\chi_{2,n_2-1}^2/(n_2-1))}.\n\\]\nIf the two populations are normal with means \\(\\mu_i\\) and standard deviations \\(\\sigma_i\\), then the distribution of the \\(F\\) statistic is the \\(F\\) distribution with \\(\\nu_1=n_1-1\\) and \\(\\nu_2=n_2-1\\) degrees of freedom.\nA special case is the \\(T\\) statistic for a normal population which has a \\(F(1, n-1)\\) distribution owing to:\n\\[\\begin{align*}\nT^2 &= (\\frac{\\bar{X} - \\mu}{S/\\sqrt{n}})^2\\\\\n&= \\frac{(\\bar{X}-\\mu)^2/\\sigma^2}{S^2/(n\\sigma^2)}\\\\\n&= \\frac{(\\bar{X}-\\mu)^2/(\\sigma^2/n)}{S^2/\\sigma^2}\\\\\n&= \\frac{Z^2/1}{\\chi^2_{n-1}/(n-1)}\\\\\n&\\sim F(1, n-1).\n\\end{align*}\\]\nThat \\(\\bar{X}\\) and \\(S^2\\) are independent under these assumptions is necessary to identify the distribution.\n\nThe distribution of the \\(T\\) and \\(F\\) statistics for an iid random sample is known under the assumption of a normal population. What if the population is not normal, how robust is the distribution of these statistics to differences in the population? This was studied in (BOX 1953). Here we show some examples through simulation.\nIn the following we use a long-tailed population (\\(T_3\\)) and a skewed exponential population to look:\n\nT_n(D, n) = (xs = rand(D,n); (mean(xs) - mean(D)) / (std(xs) / sqrt(n)))\nF_n(D, n1, n2) = (xs = rand(D,n1); ys = rand(D, n2); (std(xs)/(n1-1)) / (std(ys)/(n2-1)))\n\nn = 10\nn1, n2 = 10, 5\nN = 100\n\n\nps = range(0.01, 0.99, length=N)\nSDs = (T=TDist(n-1), F=FDist(n1-1, n2-1))\nPops = (var\"long-tailed\"=TDist(3), skewed=Exponential(1))\n\n# use scatter to produce qqplot\ndf = DataFrame([(D=\"$SDk $Popk\", x=quantile(Popv, ps), y = quantile(rand(SDv, N), ps))\n                for (SDk,SDv) in pairs(SDs) for (Popk, Popv) in pairs(Pops)])\n\np = data(flatten(df, [:x,:y])) * (visual(Scatter) + linear(;interval=nothing)) *\n    mapping(:x, :y, main=:D, layout=:D, color=:D)\n\ndraw(p, facet=(; linkxaxes=:none, linkyaxes=:none))\n\n\n\n\nFigure 7.8: Small sample simulations of \\(T\\) and \\(F\\) statistic for non-normal populations. None are well described by the \\(T\\) or \\(F\\) distributions.\n\n\n\n\n\n\n\n7.3.3 The sample median\nThe sample median is the “middle” of an iid random sample. Like the sample mean, it is a measure of center, that, unlike the sample mean, is resistant to outliers. The central limit theorem instructs us that for most populations the distribution of the sample mean is normally distributed. The distribution of the sample median can be computed. It is asymptotically normal with mean \\(\\mu\\) and variance given by \\((4nf(m)^2)^{-1}\\), where \\(n\\) is the sample size, \\(m\\) the median of the population, and \\(f\\) the pdf of the population.\nFigure 7.9 shows through boxplots the variability of the sample median and sample mean depends on the population. For the standard normal population the ratio of the variances of \\(\\bar{X}\\) and \\(M\\) tends to \\(2/\\pi\\). This is seen in the smaller IQR for \\(\\bar{X}\\) compared to \\(M\\) in the figure for the normal population. This ratio isn’t always smaller than 1 for other populations.\n\n\n\n\n\nFigure 7.9: Comparison of spread of the sample median versus the sample mean for various populations. For skewed and long-tailed distributions, the sample median has smaller variability.\n\n\n\n\n\n\n7.3.4 The Chi-squared statistic\nFor the multinomial distribution for a fixed \\(n\\) with probabilities \\(p_1, p_2, \\dots, p_k\\) and counts \\(X_1, X_2, \\dots, X_k\\), the expected values are \\(E_i = E(X_i) = n p_i\\). The difference between the observed, \\(X_i\\), and the expected, \\(E_i\\), are often summarized in this statistic:\n\\[\n\\chi^2 = \\sum_k \\frac{(X_i - E_i)^2}{E_i}.\n\\]\nFor this scenario, the asymptotic distribution of \\(\\chi^2\\) is the Chi-squared distribution with \\(k-1\\) degrees of freedom. This number comes from the one relationship amongst the \\(k\\) variables (that they all add up to \\(1\\).) That is, \\(k-1\\) \\(p_i\\)s are freely chosen, the other a dependency.\nSuppose there are \\(s\\) values, \\(\\theta = (\\theta_1, \\theta_2, \\dots, \\theta_s)\\) with some identification \\(f(\\theta) = (p_1, p_2, \\dots, p_k)\\). If these \\(s\\) values are estimated from the data (using a best asymptotically normal estimator), then there are \\(k - s - 1\\) degrees of freedom in the asymptotic Chi-squared distribution.\nThe Chi-squared sampling distribution is asymptotic meaning for \\(n\\) large enough. A rule of thumb is each count should be expected to be \\(5\\) or more.\nA related statistic is the Cressie-Read power-divergence statistic, (Cressie and Read 1984), parameterized by real \\(\\lambda\\) and given by:\n\\[\n2nI^\\lambda = \\frac{2}{\\lambda(\\lambda +1)} \\sum_{i=1}^k X_i \\left( \\left(\\frac{X_i}{E_i}\\right)^\\lambda - 1)\\right).\n\\]\nWhen \\(\\lambda = 1\\), after some algebraic manipulations, this can be identified with the \\(\\chi^2\\) statistic above. Other values for \\(\\lambda\\) recover other named statistics, for example \\(\\lambda=0\\), for which this computes the maximum-likelihood estimate from the data. The Cressie-Read statistic also has an asymptotic \\(Chisq(k-s-1)\\) distribution for a fixed \\(\\lambda\\).\n\n\n\n\nBOX, G. E. P. (1953), “NON-NORMALITY AND TESTS ON VARIANCES,” Biometrika, 40, 318–335. https://doi.org/10.1093/biomet/40.3-4.318.\n\n\nCressie, N., and Read, T. R. C. (1984), “Multinomial goodness-of-fit tests,” Journal of the Royal Statistical Society. Series B (Methodological), [Royal Statistical Society, Wiley], 46, 440–464."
  },
  {
    "objectID": "Inference/inference.html#larger-data-sets",
    "href": "Inference/inference.html#larger-data-sets",
    "title": "8  Inference",
    "section": "8.1 Larger data sets",
    "text": "8.1 Larger data sets\nSuppose \\(x_1, x_2, \\dots, x_n\\) is sample. If we assume these are realizations from some iid random sample \\(X_1, X_2, \\dots, X_n\\) from some population then when \\(n\\) is large we expect the shape of the population to be well described.\nThe density plots in Figure 8.1 show a population, a random sample of a certain size from that population, and an density plot found from the random sample. As the sample size gets larger, the density plot resembles more the underlying population.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.1: A population pdf drawn in red, a sample of size n marked with dots, and a sample density. As n increases, the sample density is an improved estimate for the population pdf.\n\n\nTo quantify this, we use the empirical cdf defined as\n\\[\nF_n(a) = \\frac{\\#\\{i: x_i \\leq a\\}}{n}.\n\\]\nThat is, the proportion of the sample data less than or equal to \\(a\\). This estimates the cdf of the population. The ecdf function from StatsBase returns a function. For example, we have for this population given as a mixture of normal distributions:\n\nY = MixtureModel(Normal[\n    Normal(0, 1),\n    Normal(1, 2),\n    Normal(2.0, 3)], [0.5, 0.2, 0.3])\n\nxs = rand(Y, 1000)\nFₙ = ecdf(xs)\n\nfindmax(abs(cdf(Y,x) - Fₙ(x)) for x in range(-5, 5, 1000))\n\n(0.0246556183201303, 487)\n\n\nThe maximum distance between cumulative distributions functions is a useful statistic in itself, which we don’t pursue. Rather, we show in Figure 8.2 the empirical cdf and the theoretical cdf for this simulation.\n\nFₙ = ecdf(rand(Y, 1000))\nxs = range(-3, 6, 1000)\nd = data((x=xs, y=cdf.(Y,xs), y′=Fₙ.(xs)))\np = d * visual(Lines; color=:black) * mapping(:x, :y)\np = p + d * visual(Lines; color=:red) * mapping(:x, :y′)\ndraw(p)\n\n\n\n\nFigure 8.2: A cumulative distribution of a mixture model with an empirical cdf generated by a random sample of size 1000."
  },
  {
    "objectID": "Inference/inference.html#confidence-intervals",
    "href": "Inference/inference.html#confidence-intervals",
    "title": "8  Inference",
    "section": "8.2 Confidence intervals",
    "text": "8.2 Confidence intervals\nWhen there is a much smaller sample size, one can still infer things about the underlying population if additional assumptions on the population are made. In general, the stronger the assumptions, the more that can be said.\n\n8.2.1 Confidence interval for a population mean\nFor example, suppose you have a data set with \\(n\\) elements, \\(x_1, x_2, \\dots, x_n\\), and you assume that:\n\nthe data can be modeled as realizations of some iid collection of random variables, \\(X_1, X_2, \\dots, X_n\\) and\nthe population of the random variables is \\(Normal(\\mu, \\sigma)\\).\n\nWith these assumptions the basic facts of probability allow statements about \\(\\mu\\) based on the data set.\nWe know the statistic:\n\\[\nZ = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} = \\frac{\\bar{X} - \\mu}{SD(\\bar{X})}\n\\]\nhas a \\(Normal(0,1)\\) distribution. For any \\(\\alpha\\) with \\(0 < \\alpha < 1/2\\), we can solve for values \\(z_{\\alpha/2}\\) and \\(z_{1-\\alpha/2}\\) satisfying:\n\\[\nP(z_{\\alpha/2} < Z < z_{1 - \\alpha/2}) = 1 - \\alpha.\n\\]\n(That is between the \\(z\\) values lies \\((1-\\alpha)\\cdot 100\\)% of the area under the pdf for \\(Normal(0,1)\\).)\nBy the symmetry of the normal distribution, we have \\(z_{\\alpha/2} = - z_{1-\\alpha/2}\\).\nRearranging this, we have with probability \\(1 - \\alpha\\) the following inequality occurs:\n\\[\n\\bar{X} - z_{1-\\alpha/2}\\cdot SD(\\bar{X}) < \\mu < \\bar{X} + z_{1-\\alpha/2}\\cdot SD(\\bar{X})\n\\]\nNow, the data set is just one possible realization of these random variables, which may or may not be unusual, we can’t say. However, we can say the process that produced this data set will produce values where \\((1-\\alpha)\\cdot 100\\)% of the time\n\\[\n\\bar{x} - z_{1-\\alpha/2}\\cdot SD(\\bar{x}) < \\mu < \\bar{x} + z_{1-\\alpha/2}\\cdot SD(\\bar{x}).\n\\]\nSince a data set is a single realization, and probability speaks to the frequency of many realizations, we can’t say for our lone data set that there is a \\((1-\\alpha)\\cdot 100\\)% chance this occurs, rather the language adopted is to say the interval \\((\\bar{x} - z_{1-\\alpha/2}\\cdot \\sigma/\\sqrt{n}, \\bar{x} + z_{1-\\alpha/2}\\cdot \\sigma/\\sqrt{n})\\) is a \\((1-\\alpha)\\cdot 100\\)% confidence interval for an unknown parameter \\(\\mu\\).\n\nFor a data set drawn from iid random sample with a \\(Normal(\\mu,\\sigma)\\) population a \\((1-\\alpha)\\cdot 100\\)% confidence interval is given by \\(\\bar{x} \\pm z_{1-\\alpha/2}\\cdot \\sigma/\\sqrt{n}\\), where \\(z_{1-\\alpha/2}=-z_{\\alpha/2}\\) satisfies \\(P(z_{\\alpha/2} < Z < z_{1-\\alpha/2})\\), \\(Z\\) being a standard normal random variable.\n\nFigure 8.3 illustrates confidence intervals based on several independent random samples. Occasionally – with a probability controlled by \\(\\alpha\\) – the intervals do not cover the true population mean.\n\n\n\n\n\nFigure 8.3: 50 simulated confidence intervals. The true mean is \\(\\mu=0\\), the confidence level is \\(0.90\\). On average, 10 out of 100 of these confidence intervals will not cover the mean. For this simulation, we expect 5 or so to miss, but the exact count may differ, as each is random.\n\n\n\n\n\nExample 8.1 (Confidence interval for the mean) Consider the task of the beverage dispenser service person. While they may enjoy great benefits and the opportunity to work out of an office, do they get a chance to practice statistics? Well, possibly. Consider the task of calibrating an automatic coffee dispenser. Suppose the engineers have managed to control the variance of the pour so that \\(\\sigma = 0.5\\) oz. The \\(8\\) oz. cups should not overflow, but should look full when done. As such, the technician aims for a mean dispense of around \\(7\\) oz, but not including \\(7.5\\). To gauge this, they run the machine \\(6\\) times and collect data using a calibrated glass. Assume this data set is from a \\(N(\\mu,\\sigma=1/2)\\) population:\n7.9  7.2  7.1  7.0  7.0  7.1\nA \\(95\\)% confidence interval for \\(\\mu\\) is given by:\n\nxs = [7.9,  7.2,  7.1,  7.0,  7.0,  7.1]\nn = length(xs)\nσ = 1/2\nα = 0.05\nza = quantile(Normal(0, 1), 1 - α + (α/2))\nSE = σ / sqrt(n)\n(mean(xs) - za*SE, mean(xs) + za*SE)\n\n(6.816590693637058, 7.616742639696278)\n\n\nThat \\(7.5\\) is included may be problematic, as larger pours than \\(8\\) oz are possible with these assumptions, so the technician calibrates the machine a bit less aggressively.\n\nThe assumption of a normal population is used to say the distribution of \\(\\bar{X}\\) is normal. This would be true if the population weren’t normal but the sample size, \\(n\\), were sufficiently large. The important part is having assumptions that allows the sampling distribution of a useful statistic to be known.\n\nThe above assumes an unknown mean (\\(\\mu\\)) but a known standard deviation. If that assumption isn’t realistic, something similar can be said. Consider the \\(T\\)-statistic:\n\\[\nT = \\frac{\\bar{X} - \\mu}{SE(\\bar{X})},\n\\]\nUnder the assumptions above (iid sample, normal population), the standard error is \\(S/\\sqrt{n}\\) and the distribution of \\(T\\) is the \\(T\\)-distribution with \\(n-1\\) degrees of freedom.\n\nFor a data set of size \\(n\\) drawn from an iid random sample with a \\(Normal(\\mu,\\sigma)\\) population a \\((1-\\alpha)\\cdot 100\\)% confidence interval is given by \\(\\bar{x} \\pm t_{1-\\alpha/2}\\cdot s/\\sqrt{n}\\), where \\(t_{\\alpha/2} = -t_{1-\\alpha/2}\\) satisfies \\(1-\\alpha = P(t_{\\alpha/2} < T_{n-1} < t_{1-\\alpha/2})\\), \\(T_{n-1}\\) being \\(T\\) distributed with \\(n-1\\) degrees of freedom.\n\n\nExample 8.2 (Confidence interval for the mean, no assumption on \\(\\sigma\\)) Returning to the coffee-dispenser technician, a cappuccino dispenser has two sources of variance for the amount poured – the coffee and the foam. This is harder to engineer precisely, so is assumed unknown in the calibration process. Suppose the technician again took \\(6\\) samples to gauge the value of \\(\\mu\\).\nWith no assumptions on the value of \\(\\mu\\) or \\(\\sigma\\). A \\(95\\)% confidence interval for \\(\\mu\\) would be computed by:\n\nxs = [7.9,  7.2,  7.1,  7.0,  7.0,  7.1]\nn = length(xs)\ns = std(xs)\nα = 0.05\nza = quantile(TDist(n-1), 1 - α + (α/2))\nSE = s / sqrt(n)\n(mean(xs) - za*SE, mean(xs) + za*SE)\n\n(6.856683216914592, 7.576650116418743)\n\n\nThese computations – and many others – are carried out by functions in the HypothesisTests package. For example, we could have computed the values above through:\n\nusing HypothesisTests\nconfint(OneSampleTTest(xs))\n\n(6.856683216914592, 7.576650116418743)\n\n\nOf some note, while often the extra assumption of a known standard deviation will lead to smaller confidence intervals, that is not guaranteed, as seen in this data set.\n\n\nExample 8.3 (Dependent samples) A method to reduce variability between samples is to match treatments. A classic data set is shoes, which collected shoe wear data from 10 boys, each given two different shoes to wear.\n\nusing RDatasets\nshoes = dataset(\"MASS\", \"shoes\")\nfirst(shoes, 2)\n\n\n2×2 DataFrameRowABFloat64Float64113.214.028.28.8\n\n\nSome boys are assumed to be harder on shoes than others, so by matching the two types, it is expected that the difference in the shoe wear per boy could be attributed to the material. That is, if \\(X_1, \\dots, X_{n}\\) models the one material and \\(Y_1, \\dots, Y_n\\) the other, the difference \\(Z_i = X_i - Y_i\\) should model the difference between the materials. Assuming this data is an iid random sample from a \\(Normal(\\mu, \\sigma)\\) population, we can find a \\(90\\)% confidence interval for the mean difference:\n\nds = combine(shoes, [:A,:B] => ByRow(-) => :Z)\nconfint(OneSampleTTest(ds.Z); level=0.90)\n\n(-0.634426399199845, -0.18557360080015525)\n\n\nThat this does not contain \\(0\\), suggests a difference in the mean wear between shoes.\n\n\nThe above illustrates a pattern: a sampling statistic (a pivotal quantity) which includes an unknown population parameter with a known sampling distribution independent of the parameters allows the formulation of confidence interval.\n\n\n8.2.2 Confidence interval for a difference of means\nFor two iid random samples, \\(X_1, X_2, \\dots, X_{n_1}\\) and \\(Y_1, Y_2, \\dots, Y_{n_2}\\) from two normal populations \\(Normal(\\mu_1, \\sigma_1)\\) and \\(Normal(\\mu_2, \\sigma_2)\\) we may have sample data. From that data, the question of the difference between \\(\\mu_1\\) and \\(\\mu_2\\) can be considered.\nWith the assumption that the two samples are themselves independent, the standard deviation for \\(\\bar{X} - \\bar{Y}\\) can be computed as:\n\\[\nSD(\\bar{X} - \\bar{Y}) = \\sigma_{\\bar{X} - \\bar{Y}} = \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}.\n\\]\nLet the \\(T\\) statistic be\n\\[\nT = \\frac{(\\bar{X} - \\bar{Y}) - (\\mu_1 - \\mu_2)}{SE(\\bar{X} - \\bar{Y})} = \\frac{\\text{observed}-\\text{expected}}{SE}.\n\\]\nThe distribution of \\(T\\) and the formula for \\(SE\\) depends on assumptions made:\n\nIf both \\(\\sigma_1\\) and \\(\\sigma_2\\) are assumed known, \\(SE(\\bar{X} - \\bar{Y}) = SD(\\bar{X} - \\bar{Y})\\), and \\(T\\) has a normal distribution.\nIf it is assumed \\(\\sigma = \\sigma_1 = \\sigma_2\\), but if no value is assumed, then the data can be pooled to estimate \\(\\sigma\\), the common standard deviation, to get \\(SE(\\bar{X} - \\bar{Y}) = s_p\\sqrt{1/n_1 + 1/n_2}\\) and \\(T\\) has a \\(T\\) distribution with \\(n_1 + n_2 - 2\\) degrees of freedom. The pooled standard deviation is the square root of \\((s_1^2(n_1-1) + s_2^2(n_2-1))/(n_1+n_2-2)\\).\nIf it is not assumed \\(\\sigma = \\sigma_1 = \\sigma_2\\) (though it secretly could be), then the standard error is \\(\\sqrt{s_1^2/n_1 + s_2^2/n_2}\\). The distribution of \\(T\\) is approximately \\(T\\)-distributed with an effective degrees of freedom given by the Welch-Satterhwaite equation, which is always between than the smaller of \\(n_1-1\\) and \\(n_2-1\\) and \\(n_1 + n_2 - 2\\).\n\n\nExample 8.4 (Confidence interval for the difference of means) Suppose the coffee-dispenser technician is tasked with calibrating two machines. Again they take samples from the two machines, in this case:\nMachine 1: 7.0, 7.8, 7.7, 7.6, 8.3\nMachine 2: 6.2, 8.0, 6.8, 7.0, 7.3, 7.9, 7.1\nDo the machines output the same amount on average? To answer this, we consider a confidence interval for \\(\\mu_1 - \\mu_2\\).\nIf the two machines are assumed to have the same variance, we can compute a 90% confidence interval with:\n\nxs = [7.0, 7.8, 7.7, 7.6, 8.3]\nys = [6.2, 8.0, 6.8, 7.0, 7.3, 7.9, 7.1]\nconfint(EqualVarianceTTest(xs, ys); level=0.90)\n\n(-0.10761090026945175, 1.0961823288408845)\n\n\nThat \\(0\\) is in this interval gives evidence that the two means are equal. The sample means do differ:\n\nmean(xs), mean(ys)\n\n(7.6800000000000015, 7.185714285714285)\n\n\nBut the variability is such, the confidence interval makes it conceivable that the population means are the same. Perhaps were more data available, a difference would be seen, as the variablity is generally smaller for larger sample sizes.\nIf the machines are from different vendors, or dispense different beverages, perhaps the assumption of equal variances is not appropriate. The UnequalVarianceTTest method is available for this comparison. The calling pattern is identical:\n\nconfint(UnequalVarianceTTest(xs, ys); level=0.90)\n\n(-0.07723849812971084, 1.0658099267011436)\n\n\nThe default for such tests in other languages, such as R, is to not assume equal variances.\n\n\n\n\nTable 8.1: Different constructors for \\(T\\)-statistic based confidence intervals\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nOneSampleZTest\nInference on population mean for one sample, known population standard deviation\n\n\nOneSampleTTest\nInference on population mean for one sample, unknown population standard deviation\n\n\nEqualVarianceTTest\nInference on difference of population means for independent samples assuming equal population standard deviations\n\n\nUnequalVarianceTTest\nInference on difference of population means for independent samples not assuming equal population standard deviations\n\n\n\n\n\n\n8.2.3 Confidence interval for a proportion\nA very common use of confidence intervals is found in the reporting of polls, particularly political polls, where it may be typical to see a statement such as “Candidate A is ahead of her opponent in the polls by 5 percentatge points with a margin of error of 5 percent” or “The favorability rating of President B is 54% with a 5 percent margin of error.” These are descriptions of confidence intervals, though both leave the confidence level unspecified. When unspecified, it can usually be roughly inferred from the margin of error, as will be seen.\nA model for a poll where a person chose one of two options is to use a Bernoulli random variable, \\(X_i\\), to describe the response. The number of \\(1\\)s in a fixed number of \\(n\\) respondents can give a proportion. If one can assume the \\(X_i\\) are iid random samples from a \\(Bernoulli(p)\\) population, then the number of \\(1\\)s can be viewed as a realization of a \\(Binomial(n,p)\\) distribution. That is, this statistic will have a known distribution.\nBefore pursuing, let’s note that the assumption implies a few things about the sampling process carried out by the pollster:\n\nThe population \\(p\\) is the proportion of the entire voting population (supposedly of size \\(N\\)) that would respond with a code of \\(1\\). A census could find \\(p\\), but random sampling is used as censuses are typically much more work to carry out.\nThe independent assumption technically requires sampling with replacement, where a person could be asked \\(0\\), \\(1\\), or more times the question. But practically if \\(N\\) is much greater than \\(n\\) this isn’t necessary and sampling without replacement can be used.\nThe identically distributed assumption requires the sampling to be representative. For example, if it is a state-wide population, a sample concentrated on one, possibly partisan, district would not be expected to be identically distributed from the population. Such sampling would sure introduce bias.\n\nAssuming, a survey is reasonably described by a \\(Binomial(n,p)\\) distribution, the BinomialTest can be used to identify a confidence interval for a given confidence level.\nThe BinomialTest function can take either two numbers x and n representing the number of \\(1\\)s in \\(n\\) sample, or a vector of trues and falses with true being a “\\(1\\)”.\n\nExample 8.5 (A political poll) A poll was taken of 1429 likely voters, with 707 indicating support for candidate A and 722 for candidate B. Let \\(p\\) be the population proportion of voters for candidate A. Find a 95% confidence interval for \\(p\\).\nAssuming the poll was collected in such a way that a binomial model would apply to the data, we have:\n\nA = 707\nB = 722\nn = A + B\nci = confint(BinomialTest(A, n); level = 0.95)\n\n(0.4685125690214495, 0.521012196604416)\n\n\nAs \\(0.50\\) is in this interval, there is no suggestion candidate A can’t win. (And indeed, they did in this case). The report on this poll would be the sample proportion and a margin of error. The margin of error is the width of the interval divided by two:\n\n(last(ci) - first(ci)) * 1/2\n\n0.02624981379148325\n\n\nA bit under 3 percentage points. A rough guide is 3 percentage points is around 1,000 people polled, a larger margin of error (MOE) is fewer people polled, a smaller one is more than 1,000 polled. Why is expanded on in a bit.\nThe BinomialTest has several different ways to compute the margin of error, or the confidence interval. These are passed to the method argument of confint as symbols. The default is :clopper_pearson which is based on the binomial distribution. The value :wald uses a normal approximation to the binomial, which may be very inaccurate when \\(p\\) is close to \\(0\\) or \\(1\\). There are others (cf. ?confint(:BinomialTest)).\nThe default one can be understood through the quantile function, and is essentially given through:\n\nalpha = 0.05\nquantile.(Binomial(n, A/n), [alpha/2, 1 - alpha/2]),  ci .* n\n\n([670, 744], (669.5044611316513, 744.5264289477104))\n\n\nFor confidence intervals calculated through the normal approximation, an explicit formula for the margin of error, based on the standard deviation of the binomial distribution, is available:\n\\[\nMOE = z_{1-\\alpha/2} \\cdot \\sqrt{p(1-p)/n} =  z_{1-\\alpha/2} SE(\\hat{p}).\n\\]\nAs \\(p\\) is unknown, the standard error is used with \\(\\hat{p}\\) approximating \\(p\\).\n\nci = confint(BinomialTest(A, n); level = 0.95, method=:wald)\nz_a = quantile(Normal(0,1), 1-alpha/2)\nphat = A/n\nSE = sqrt(phat * (1-phat)/n)\nz_a * SE, (last(ci) - first(ci))/2  # compare to that computed with :wald\n\n(0.025922569857879177, 0.025922569857879163)\n\n\nThis formula allows one to estimate the sample size, \\(n\\), needed to achieve a certain margin of error, though an estimate for \\(p\\) can be used, taking \\(p=1/2\\) gives a conservative number as \\(p(1-p)\\) is largest for that value. For example, to see what a 3% margin of error with a 95% confidence level would need in terms of a sample, we have, solving for \\(n\\):\n\\[\nn = p(1-p)\\cdot \\left(\\frac{z_{1-\\alpha/2}}{MOE}\\right)^2 \\leq \\frac{1}{4}\\left(\\frac{z_{1-\\alpha/2}}{MOE}\\right)^2.\n\\]\n\nMOE = 0.03\nalpha = 0.05\nz_a = quantile(Normal(0,1), 1-alpha/2)\n1/2 * (1- 1/2) * (z_a / MOE)^2\n\n1067.071894637261\n\n\nA bit more than 1000 in this case ensures that the MOE will be no more than \\(0.03\\); it could be much less if it was expected that \\(p\\) is far from \\(1/2\\).\n\n\n\n8.2.4 Confidence interval for a difference of proportions\nFor two sample proportions, \\(\\hat{p}_1 = x_1/n_1\\) and \\(\\hat{p}_2=x_2/n_2\\), the \\(T\\) statistic:\n\\[\nT = \\frac{(\\hat{p}_1 - \\hat{p}_2) - (p_1 - p_2)}{SE(\\hat{p}_1 - \\hat{p}_2)}\n\\]\nhas a standard normal distribution if the \\(n\\) values are both large enough. This allows a confidence interval for \\(p_1 - p_2\\) to be given by:\n\\[\n(\\hat{p}_1 - \\hat{p}_2) - z_{1-\\alpha/2} \\cdot SE(\\hat{p}_1 - \\hat{p}_2) < p_1 - p_2 < (\\hat{p}_1 - \\hat{p}_2) + z_{1-\\alpha/2} \\cdot SE(\\hat{p}_1 - \\hat{p}_2),\n\\]\nwhere \\(SE(\\hat{p}_1 - \\hat{p}_2) = \\sqrt{\\hat{p}_1(1-\\hat{p}_1)/n_1 + \\hat{p}_2(1-\\hat{p_2})/n_2}\\).\n\nExample 8.6 (Difference of population proportions) Did some external event cause people to reexamine their choice for a political race? Suppose polls taken several weeks apart yielded:\n\n\n\n\nCandidate A\nCandidate B\n\n\n\n\nOct\n289\n259\n\n\nNov\n513\n493\n\n\n\nCompute a \\(90\\)% confidence interval.\n\nx1, x2 = 289, 513\nn1, n2 = x1 + 259, x2 + 493\nphat1, phat2 = x1/n1, x2/n2\nSE = sqrt(phat1*(1-phat1)/n1 + phat2*(1-phat2)/n2)\nalpha = 0.10\nza = quantile(Normal(0,1), 1 - alpha/2)\n(phat1 - phat2) .+ [-1,1] * za * SE\n\n2-element Vector{Float64}:\n -0.026187674280803125\n  0.06105148412248293\n\n\nThat \\(0\\) is in this, suggests the possibility that there was no change in the polls.\n\n\n\n8.2.5 Confidence interval for a population standard deviation\nUnder a normal population assumption for an iid random sample, \\((n-1)S^2/\\sigma^2\\) has a \\(Chisq(n-1)\\) distribution. Solving \\(\\chi^2_{\\alpha/2} < (n-1)S^2/\\sigma^2 < \\chi^2_{1-\\alpha/2}\\) for \\(\\sigma\\) gives a formula for a \\((1-\\alpha)\\cdot 100\\)% confidence interval for \\(\\sigma^2\\):\n\\[\n\\frac{(n-1)S^2}{\\chi^2_{1-\\alpha/2}} < \\sigma^2 < \\frac{(n-1)S^2}{\\chi^2_{\\alpha/2}}.\n\\]\nTo use this, suppose our data is\n1.2, -5.2, -8.4, 3.1, -2.1, 3.8\nWe can give a \\(90\\)% CI by:\n\nxs = [1.2, -5.2, -8.4, 3.1, -2.1, 3.8]\nn, s² = length(xs), var(xs)\nalpha = 0.10\ncl, cr = quantile.(Chisq(n-1), [alpha/2, 1 - alpha/2])\n(sqrt((n-1)*s²/cr), sqrt((n-1)*s²/cl))\n\n(3.2630536146068865, 10.144128513617867)\n\n\n\n\n8.2.6 Confidence interval for comparing population standard deviations\nThe comparison of two sample standard deviations is of interest, as seen in the two-sample \\(T\\) confidence interval, where different formulas are available should there be an assumption of equality. For two independent normally distributed iid random samples, the \\(F\\)-statistic can be used to construct a confidence interval, as a scaled ratio of the sample standard deviations will have a certain \\(F\\) distribution. So for a given \\(\\alpha\\), the following happens with probability \\(1-\\alpha\\):\n\\[\nF_{\\alpha/2; n_1-1, n_2-1} < (S_1^2/\\sigma_1^2) / (S_2^2/\\sigma_2^2) < F_{1 -\\alpha/2; n_1-1, n_2-1}\n\\]\nor, after rearrangement:\n\\[\n\\frac{S_1^2/S_2^2}{F_{1 -\\alpha/2; n_1-1, n_2-1}} < \\frac{\\sigma^2_1}{\\sigma^2_2} <\n\\frac{S_1^2/S_2^2}{F_{\\alpha/2; n_1-1, n_2-1}}.\n\\]\nFor a single sample, this is used to construct a confidence interval for the ratio of variances (or standard deviations). The VarianceFTest computes the necessary pieces, but the VarianceFTest type does not have a confint method. In the following we extend the confint generic using the above for objects created by VarianceFTest:\n\nfunction HypothesisTests.confint(F::VarianceFTest; level=0.95)\n    alpha = 1 - level\n    dof1, dof2 = F.df_x, F.df_y\n    fl, fr = quantile.(FDist(dof1, dof2), (alpha/2, 1-alpha/2))\n    F.F ./ (fr, fl)\nend\n\nTo use this, for example, suppose we have two samples:\nxs:  1.2, -5.2, -8.4,  3.1, -2.1,  3.8\nys: -1.7, -1.8, -3.3, -6.3,  8.4, -0.1, 2.5, -3.8\nThen a \\(90\\)% confidence interval for \\(\\sigma^2_1/\\sigma^2_2\\) is given by:\n\nxs =  [1.2, -5.2, -8.4, 3.1, -2.1, 3.8]\nys =  [-1.7, -1.8, -3.3, -6.3, 8.4, -0.1, 2.5, -3.8]\nconfint(VarianceFTest(xs, ys); level=0.90)\n\n(0.28992354789264896, 5.614264355299279)\n\n\nThat this includes 1 suggest no reason to think the two population variances differ.\n\n\n8.2.7 Likelihood ratio confidence intervals\nThe confidence intervals so far are based on pivotal quantities with known sampling distributions. A more generally applicable alternative is to use the likelihood function, \\(L(\\theta | x) = f(x | \\theta)\\), where the latter is the joint pdf of the data for a given parameter or parameters. The idea of maximizing the likelihood is to choose the values for \\(\\theta\\) which maximize the probability of seeing the observed data and estimating \\(\\theta\\) with those values.\nThe maximum likelihood estimate is an alternative manner to estimate a parameter. Here we apply the method to data that would otherwise be amenable to a a \\(T\\)-test. First we generate some random data:\n\nμ, σ = 10, 3\nn = 8\nD = Normal(μ, σ)\nys = rand(D, n)\nys |> show\n\n[5.781941733769936, 8.416972002794303, 9.596996150922918, 3.698283030284781, 6.060429062034207, 7.346028713978333, 7.565911447762302, 9.36009670720032]\n\n\nThe likelihood function below uses an assumption on on the data, in particular that it is normally distributed with unknown mean and standard deviation (\\(y_i \\sim Normal(\\mu, \\sigma)\\)). This just happens to match how the random data was generated, but typically is an assumption about the data. We write the log-likelihood function so it takes two values the unknown parameters and the data:\n\nfunction loglik(θ, data)\n    # yᵢ ~ N(μ, σ) is model\n    μ, σ = θ\n    D = Normal(μ, σ)\n    y = first(data)\n    ll = 0.0\n    for yᵢ ∈ y\n        ll += logpdf(D, yᵢ)\n    end\n    ll\nend\n\nloglik (generic function with 1 method)\n\n\nAbove, we use logpdf to compute the logarithm of the probability density function, as an alternative to log(pdf(D, yᵢ)).\nTo maximize the data, we will use the ProfileLikelihood package, which relies on the Optim package to perform the optimization:\n\nusing ProfileLikelihood, Optim, OptimizationOptimJL\n\nThe optimization needs an initial guess, \\(\\theta_0\\), defined below. We also specify names to the parameters, when we set up a “problem” below:\n\nθ₀ = [5.0, 2.0]\nnms = [:μ, :σ]\ndat = (ys,) # data is a container\nprob = LikelihoodProblem(loglik, θ₀;\n                         data = dat,\n                         syms = nms)\n\n\nLikelihoodProblem. In-place: true\nθ₀: 2-element Vector{Float64}\n     μ: 5.0\n     σ: 2.0\n\n\n\nThis problem is solved by mle, which needs an optimization algorithm. We use the NelderMead one, which is easier to specify, as we don’t need to set up means to take a gradient:\n\nsol = mle(prob, Optim.NelderMead())\n\n\nLikelihoodSolution. retcode: Success\nMaximum likelihood: -16.306882414667225\nMaximum likelihood estimates: 2-element Vector{Float64}\n     μ: 7.228327663444761\n     σ: 1.8578709429983578\n\n\n\nAs seen above, the sol object outputs the estimate for \\(\\mu\\). We can compare to mean(ys) and std(ys); the mean agrees (the standard deviation has a different divisor, so is systematically different, but could be aligned given the sample size):\n\nmean(ys), std(ys)\n\n(7.228332356093388, 1.9861288940459225)\n\n\nThe reason to use the ProfileLikelihood package to do the optimization, is we can then find \\(95\\)% confidence levels for the parameters using a method implemented therein. The profile generation requires a specification of upper and lower bounds for the parameters:\n\nlb = [-100.0, 0.0]\nub = [ 100.0, 100.0]  # -100 < μ < 100; 0 < σ < 100\nresolutions = [200, 200]\nparam_ranges = construct_profile_ranges(sol, lb, ub, resolutions)\nprof = profile(prob, sol; param_ranges=param_ranges)\n\n\nProfileLikelihoodSolution. MLE retcode: Success\nConfidence intervals: \n     95.0% CI for μ: (5.769721273893047, 8.686946110422952)\n     95.0% CI for σ: (1.218951098613131, 3.3343880800809163)\n\n\n\nWe can compare the confidence interval identified for \\(\\mu\\) to that identified through the \\(T\\) statistic:\n\nconfint(OneSampleTTest(ys), level = 0.95)\n\n(5.567887047617717, 8.888777664569059)\n\n\nThe two differ – they use different sampling distributions and methods – though simulations will show both manners create CIs capturing the true mean at the rate of the confidence level.\nThe above example does not showcase the advantage of the maximimum likelihood methods, but hints at a systematic way to find confidence intervals, which for some cases is optimal, and is more systematic then finding some pivotal quantity (e.g. the \\(T\\)-statistic under a normal population assumption)."
  },
  {
    "objectID": "Inference/inference.html#hypothesis-tests",
    "href": "Inference/inference.html#hypothesis-tests",
    "title": "8  Inference",
    "section": "8.3 Hypothesis tests",
    "text": "8.3 Hypothesis tests\nA confidence interval is a means to estimate a population parameter with an appreciation for the variability involved in random sampling. However, sometimes a different language is sought. For example, we might hear a newer product is better than an old one, or amongst two different treatments there is a difference. These aren’t comments about the specific value. The language of hypothesis tests affords the flexibility to incorporate this language.\nThe basic setup is similar to a courtroom trial in the United States – as seen on TV:\n\na defendant is judged by a jury with an assumption of innocence\npresentation of evidence is given\nthe jury weighs the evidence assuming the defendant is innocent.\nIf it is a civil trial a preponderence of evidence is enough for the jury to say the defendent is guilty (not innocent); if a criminal trial the standard if the evidence is “beyond a reasonable doubt” then the defendent is deemed not innocent. Otherwise the defendant is said to be “not guilty,” though really it should be that they weren’t “proven” to be guilty.\n\nIn a hypothesis or significance test for parameters, the setup is similar:\n\na null hypothesis of “no difference” is assumed; an alternative hypothesis is specified.\ndata is collected\na test statistic is used to summarize the data. Then a probability is computed that accounts for the assumption of the null hypothesis and the data, computing the probability that the random values would be as or more extreme than observed in the data assuming the null hypothesis is true\nif that probability is small, then one might conclude the null hypothesis is incorrect; otherwise there is no evidence to say it isn’t correct, small would depend on the context of application.\n\nTo illustrate hypothesis tests, suppose a company claims their new product is more effective than the current standard. Further, suppose this effectiveness can be measured by a single number, though there is randomness to any single measurement. Over time the old product is known to have a mean of \\(70\\) in this measurement.\nThen the null and alternative hypotheses would be:\n\\[\nH_0: \\mu = 70 \\quad H_A: \\mu > 70,\n\\]\nwhere a bigger number is considered better. The null is an assumption of “no change,” the alternative points in the direction of the claim (better, in this case).\nTo test this, data would be collected. Suppose in this case 8 measurements of the new product were taken with these values:\n73.0, 66.1, 76.7, 68.0, 60.8, 81.8, 73.5, 75.2\nAssume these are realizations of an iid random sample from a normal population, then the sample mean would be an estimate for the population mean of the new product:\n\nxs = [73.0, 66.1, 76.7, 68.0, 60.8, 81.8, 73.5, 75.2]\nmean(xs)\n\n71.8875\n\n\nThis is more than 70, but the skeptic says half the time a sample mean would be more than 70 if the null hypothesis were true. Is it really indicative of “better?” For that question, a sense of how extreme this observed value is under an assumption of the null hypotheses is used.\nTo test this, we characterize the probability a sample mean would be even more than our observed one, were we to repeat this sampling many times:\n\nThe \\(p\\)-value is the probability the test statistic is as or more extreme than the observed value under the null hypothesis.\n\nFor this problem the \\(p\\)-value is the probability the sample mean is mean(xs) or more assuming the null hypothesis. Centering by \\(\\mu\\) and scaling by the standard error, this is:\n\\[\nP(\\frac{\\bar{X} - \\mu}{SE(\\bar{X})} > \\frac{\\bar{x} - \\mu}{s/\\sqrt{n}} \\mid H_0).\n\\]\nThat is, large values of \\(\\bar{X}\\) are not enough to be considered statistically significant, rather large values measured in terms of the number of standard errors are.\nThis scaling and an assumption that the data is from a normal population makes this a statement about a \\(T\\)-distributed random variable, namely:\n\nn = length(xs)\nSE = std(xs) / sqrt(n)\nobs = (mean(xs) - 70) / SE\np_value = 1 - cdf(TDist(n-1), obs)  # 1 - P(T ≤ obs) = P(T > obs)\n\n0.22361115388017372\n\n\nIs this \\(p\\)-value suggestive that the null hypothesis is not a valid explanation of the variation seen in the data?\nThe jury has guidance, depending on the type of trial, here there is also guidance in terms of a stated significance level. This is done in advance, but is typically just \\(\\alpha = 0.05\\). So if the \\(p\\)-value is less than \\(\\alpha\\) the null hypothesis is suggested to be incorrect; if not there is no evidence to conclude the null hypothesis is incorrect. Since here the \\(p\\)-value is much greater than \\(\\alpha = 0.05\\), we would say there is no evidence to reject the null hypothesis.\nThe mechanics here are standard and encapsulated in the OneSampleTTest constructor. The pvalue method allows the specification of the type of “tail”. In this case, as the alternative hypothesis is \\(H_A: \\mu > 70\\), the tail is :right:\n\npvalue(OneSampleTTest(xs, 70); tail=:right)\n\n0.22361115388017377\n\n\nHad the alternative been \\(H_A: \\mu \\neq 70\\), the \\(p\\)-value would be computed differently as then very big or very small values of the observed test statistic would be evidence against the null hypothesis. In this case, the tail is :both, which is the default:\n\npvalue(OneSampleTTest(xs, 70))\n\n0.44722230776034755\n\n\n\nDesign of structures in HypothesisTests\nThe HypothesisTests package was first published in 2015 and uses a calling style where the main functions match the underlying types. In particular, the OneSampleTTest method creates a structure, also called OneSampleTTest. It is illustrative of the other tests in the HypothesisTests package. This structure does not keep the data, only sufficient statistics of the data to generate the various summaries. In this case, these are n, xbar (\\(\\bar{x}\\)), df (\\(n-1)\\), stderr (\\(SE\\)), t ((mean(xs)-mu)/stderr), and μ0 (the null hypothesis). The design is the default show method summarizes most of what may be of interest, specialized methods, like confint and pvalue allow customization and access to the specific values. The default show method for this test is:\n\nOneSampleTTest(xs)\n\nOne sample t-test\n-----------------\nPopulation details:\n    parameter of interest:   Mean\n    value under h_0:         0\n    point estimate:          71.8875\n    95% confidence interval: (66.34, 77.43)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           <1e-07\n\nDetails:\n    number of observations:   8\n    t-statistic:              30.6644467681273\n    degrees of freedom:       7\n    empirical standard error: 2.3443273098512263\n\n\nA default value for the null was chosen (h_0), a default level for the confidence interval (0.95), and a default choice of tail was made for the computed \\(p\\)-value.\n\n\nEquivalence between hypothesis tests and confidence intervals\nFor many tests, such as the one-sample \\(T\\)-test, there is an equivalence between a two-sided significance test with significance level \\(\\alpha\\) and a \\((1-\\alpha)\\cdot 100\\)% confidence interval – no surprise given the same \\(T\\)-statistic is employed by both.\nFor example, consider a two-sided significance test of \\(H_0: \\mu=\\mu_0\\). Let \\(\\alpha\\) be the level of significance: we “reject” the null hypothesis if the \\(p\\)-value is less than \\(\\alpha\\). An iid random sample is summarized by the observed value of the \\(T\\)-statistic, \\((\\bar{x} - \\mu_0)/(s/\\sqrt{n})\\). Let \\(t^* = t_{1-\\alpha/2}\\) be the quantile, then if the \\(p\\)-value is less than \\(\\alpha\\), we must have the observed value in absolute value is greater than \\(t^*\\). That is\n\\[\n| \\frac{\\bar{x} - \\mu_0}{s/\\sqrt{n}}| > t^*.\n\\]\nAlgebraically, this implies that either \\(\\mu_0 < \\bar{x} - t^* s/\\sqrt{n}\\) or \\(\\mu_0 > \\bar{x} + t^* s/\\sqrt{n}\\). That is, the value of \\(\\mu_0\\) is not in the \\((1-\\alpha)\\cdot 100\\)% CI based on the sample.\n\nRejecting the null hypothesis \\(H_0: \\mu=\\mu_0\\) is equivalent to \\(\\mu_0\\) not being in the corresponding confidence interval.\n\nThe reverse is also true: any \\(\\mu_0\\) not in the \\((1-\\alpha)\\cdot 100\\)% CI would have a two-tailed test of \\(H_0: \\mu=\\mu_)\\) rejected at the \\(\\alpha\\) significance level.\n\n\nAlternative hypotheses\nThe null hypothesis is also described as one of “no effect,” the alternative hypothesis the “research hypothesis.” The \\(p\\)-value is computed under the null hypothesis. The alternative hypothesis may take different forms: it could be a specification of a point, as will be seen in the next section on power; a one-tailed directional hypothesis, useful to capture expressions like “better;” two-tailed directional, useful to capture expressions like “different;” and non-directional, useful for indicating anything save the null hypothesis. The distinction between the last two is more clear when the null hypothesis speaks to more than one variable, such as is the case when describing, say, different probabilities for a multinomial distribution.\nIn HypothesisTests, for the \\(T\\) tests and many others, the alternative is specified to pvalue through the tail argument with a value of :both, :right, or :left. Some tests have non-drectional alternatives and no argument. In the case of a \\(T\\)-test, with a symmetric sampling distribution, symmetry relates the values, so knowing one can be used to figure out the other two.\n\n\nType-I error\nThe computation of a \\(p\\)-value is the result of a significance test. Often this value is compared to a significance level, \\(\\alpha\\): if the \\(p\\)-value is less than \\(\\alpha\\) the difference of the test statistic from the null hypothesis is statistically significant. In other language, we may “reject” the null hypothesis when the \\(p\\)-value is less than \\(\\alpha\\) and “accept” the null if not.\nThe \\(p\\)-value is computed under the null hypothesis being true. The \\(p\\)-value depends on a random sample. How often, on average, would this \\(p\\)-value be less than \\(\\alpha\\)? Why, \\(\\alpha\\cdot 100\\) percent of the time, as \\(\\alpha = P(\\text{reject} | H_0)\\).\nWe can see this through a simulation. Let’s look at the \\(T\\) statistic for a sample of size \\(10\\) from a \\(Normal(0,1)\\) population:\n\nZ = Normal(0,1)\nn = 10\nN = 5000\nps = [pvalue(OneSampleTTest(rand(Z, n))) for _ in 1:N]\nsum(ps .< 0.01)/N, sum(ps .< 0.05)/N, sum(ps .< 0.10)/N\n\n(0.0122, 0.0546, 0.1034)\n\n\nThe proportions in the 5000 samples roughly match the threshold specified (\\(\\alpha = 0.01\\), \\(0.05\\), \\(0.10\\)).\nThere are some cases where a conservative sampling distribution is used. (For example, with a two sample test of means with no assumption of equal variances, a conservative degrees of freedom is sometimes suggested.) Conservative means that “rejecting” the null (or finding \\(p\\) value less than \\(\\alpha\\)) will occur on average less than \\(\\alpha\\cdot 100\\) percent of the time.\n\n\nPower\nConsider a significance test for the population mean with this null and alternative hypothesis:\n\\[\nH_0: \\mu=0, \\quad H_A: \\mu=1.\n\\]\nThat is the alternative is fully specified, unlike, say, \\(H_A: \\mu > 0\\).\nWhen a small \\(p\\) value is found, the language of “rejecting” the null hypothesis is used, whereas for large \\(p\\) values, “accept” is used. If we “reject” when the \\(p\\)-value is less than \\(\\alpha\\), then the probability of making a mistake when the null hypothesis is actually true is \\(\\alpha\\). This is called a Type-I error. In the above, as the alternative is a fixed value of the parameter, we could also compute the probability of “accepting” when the alternative hypothesis is actually true. When this error occurs, a Type-II error happens.\nFor example, a sample of size \\(n=4\\) from a \\(Normal(\\mu, 1)\\) distribution would have a standard error of \\(1/\\sqrt{4} = 1/2\\). Without thinking too much, were the sample mean close to \\(0\\) we would accept the null, were it bigger than \\(1\\) we would reject the null. Precisely, if we set \\(\\alpha = 0.05\\), then we can compute under the null hypothesis a critical value for which if \\(\\bar{x}\\) is less we “accept” and if more we “reject”:\n\nalpha = 0.05\nsigma = 1\nn = 4\nSD = sigma / sqrt(n)\nzc = quantile(Normal(0, SD), 1 - alpha)\n\n0.8224268134757359\n\n\nThis is computed under the null hypothesis. In this case, we can compute under the alternative hypothesis, as it is fully specified, the probability of “accepting” when the alternative is true. This probability is traditional called \\(\\beta\\) and depends on the exact specification of the alternative. For this specific one we have:\n\nmu_a = 1\ncdf(Normal(mu_a,  SD), zc)\n\n0.36123996868766456\n\n\nThe left graphic in Figure 8.4 shows the sampling distribution under the null hypothesis centered at \\(0,\\) and the sampling distribution under the alternative hypothesis, centered at \\(1\\). The shaded area representing \\(\\beta\\) is much bigger than \\(\\alpha\\).\nThe value for \\(\\beta\\) is pretty large, over a third, so with such a small sample, it is difficult to detect a difference of \\(1 = \\mu_A - \\mu_0\\). The basic expectation is: the smaller the difference, the larger the sample is needed to have “\\(\\beta\\)” be small.\nThe power is \\(1-\\beta\\). It may be specified ahead of time, as in: what size \\(n\\) is needed to have power \\(0.80\\) with a difference of means being \\(1\\).\nTo solve this, we would need to solve\n1 - beta = 1 - cdf(Normal(mu_a, sigma/sqrt(n)),\n    quantile(Normal(0, sigma/sqrt(n)), 1 - alpha))\nFor this problem we could do this with the following approach, where we first define a function to get the \\(\\beta\\) value for a given \\(n\\) and set of assumptions:\n\nfunction get_beta(n)\n    H0 = Normal(0, sigma/sqrt(n))\n    HA = Normal(mu_a, sigma/sqrt(n))\n    critical_point = quantile(H0, 1 - alpha)\n    beta = cdf(HA, critical_point)\nend\n\nget_beta (generic function with 1 method)\n\n\nThis is then solved for a value of n where the power for a given n matches the desired power. To do so, we equate the values and, here, note a sign change between \\(2\\) and \\(100\\):\n\nbeta = 0.2\npower = 1 - beta\nf(n) = (1 - get_beta(n)) - power\nf(2), f(100)\n\n(-0.3912027802061293, 0.19999999999999996)\n\n\nSomewhere between \\(2\\) and \\(100\\) lies the answer, and using a zero finding algorithm, a value of \\(7\\) can be seen to produce a power greater than or equal to \\(0.80\\).\n\nusing Roots\nfind_zero(f, (2, 100))\n\n6.182557232019764\n\n\n(We round up in calculations involving sample size.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.4: Illustration of power computation. A significance level \\(\\alpha\\) is chosen; under \\(H_0\\) this allows the identification of a critical point. Under \\(H_A\\), that same critical point is used to find the area in the the “acceptance” region. This probability is \\(\\beta\\). The power is \\(1-\\beta\\). Adjusting the sample size, \\(n\\), will narrow the two bell curves allowing \\(\\beta\\) to be set to a specified value. The left-hand figure uses \\(n=4\\), the right hand one uses \\(n=7\\).\n\n\nThe computation above was made easier as we used the standard deviation in our test statistic with \\(\\sigma = 1\\). To find a sample size for a \\(T\\)-test, we turn to a package, as the underlying sampling distributions are a bit different, especially under \\(H_A\\).\nLet the effect size, or standardized mean difference, be the absolute effect size (the difference between the mean specified under \\(H_0\\) and that under \\(H_A\\)) divided by the standard deviation of the population. This value is termed “small” when around \\(0.20\\), medium when around \\(0.50\\), and large when \\(0.80\\) or more. In the above computation, it would be \\(1 = (1 - 0)/1\\).\nThe power, \\(1-\\beta\\), depends on the value of \\(\\alpha\\), the sample size, and the effect size. Specifying 3 of these 4 values allows the fourth to be numerically identified.\nThe PowerAnalyses package carries out several different power computations. For the one test we are illustrating, the OneSampleTTest, the name conflicts with a related, but different method in the HypothesisTests package, as PowerAnalyses does not extend HypothesisTests. We use import instead of using, and then qualify with the module name each function used.\n\nimport PowerAnalyses\n\nFor example, to identify the sample size needed for a right-tailed test with \\(\\alpha = 0.05,\\) \\(\\beta = 0.20,\\) and the effect size is small (\\(0.30\\)) we have:\n\neffect_size = 0.3\nalpha = 0.05\npower = 0.80\n\nT = PowerAnalyses.OneSampleTTest(PowerAnalyses.one_tail)\nPowerAnalyses.get_n(T; alpha=alpha, es=effect_size, power=power)\n\n70.06790520005853\n\n\nWe see a smaller \\(n\\) is needed to have the same power with a larger effect size.\n\neffect_size = 0.8\nPowerAnalyses.get_n(T; alpha=alpha, es=effect_size, power=power)\n\n11.144239681013328\n\n\nThere are other power computations provided in the package, but this one illustrates a key point: with large enough sample sizes, any effect size can be discerned. That is the difference is statistically significant. However, that does not mean it is practically significant.\nAn example from the NIH describes a “Physicians Health Study of aspirin to prevent myocardial infarction. In more than \\(22,000\\) subjects over an average of \\(5\\) years, aspirin was associated with a reduction in MI (although not in overall cardiovascular mortality) that was highly statistically significant: \\(p < .00001\\). The study was terminated early due to the conclusive evidence, and aspirin was recommended for general prevention. However, the effect size was … extremely small. As a result of that study, many people were advised to take aspirin who would not experience benefit yet were also at risk for adverse effects. Further studies found even smaller effects, and the recommendation to use aspirin has since been modified.\n\n\n8.3.1 One sample Z test\nThe one-sample \\(Z\\) test is similar to the one-sample \\(T\\) test though the known population standard deviation is used in the test statistic, so the standard normal distribution is the sample distribution.\nFor example, consider again the coffee-dispenser technician. They wish to test the hypothesis\n\\[\nH_0: \\mu = 7.5, \\quad H_A: \\mu < 7.5\n\\]\nassuming the distribution is \\(Normal(\\mu, 1/2)\\). The test statistic used would be \\((\\bar{x}-\\mu)/(\\sigma/\\sqrt{n})\\). The data collected is\n7.9  7.2  7.1  7.0  7.0  7.1\nThe test can be carried out through the following, which shows evidence to reject the null in favor of the alternative.\n\nxs = [7.9,  7.2,  7.1,  7.0,  7.0,  7.1]\nZ = OneSampleZTest(xs, 7.5)\npvalue(Z, tail=:left)\n\n0.02152440439927433\n\n\n(The statistician may say, if the mean is \\(7.5\\) and the standard deviation is \\(0.5\\), then anything more than 1 standard deviation from the mean will overflow an \\(8\\)oz cup. But the technicians know that the normal distribution is only a generalization and that the values are always within \\(0.5\\) oz of the calibration mean.)\n\n\n8.3.2 The sign test\nThe Wilcoxon signed-rank test is an alternative to the \\(Z\\) and \\(T\\) tests. Unlike those, there is no distribution assumption on the population except that it be symmetric. For symmetric distributions with reasonable tails, the median and mean are identical, so this is often termed a test of the median. The test is implemented in the SignedRankTest function. Unlike other tests, we use the null \\(H_0: M = 0\\), so we subtract off the tested median before proceeding.\nReturning to the coffee-dispenser technician who took 6 samples and is testing if the center of the data is centered at \\(7.5\\) or something different, we have:\n\nxs = [7.9,  7.2,  7.1,  7.0,  7.0,  7.1]\nM = SignedRankTest(xs .- 7.5)\npvalue(M, tail=:both)\n\n0.15625\n\n\nThis \\(p\\)-value is large, there is no suggestion that \\(H_0\\) does not hold.\n\n\n8.3.3 One sample test of proportions\nA test of survey data where one of two answers are possible can be carried out using the binomial model, assuming the sample data is representative.\nFor example, suppose it is known that in any given semester, 50% of matriculating students will have a lower semester GPA than their cumulative GPA. A researcher tests if college transfer students are different with \\(p\\) representing the population proportion whose lower semester GPA will be worse than their cumulative one. The test is:\n\\[\nH_0: p = 0.50, \\quad H_A: p > 0.50\n\\]\nThey collect data and find of 500 transfer students identified, 290 had lower GPAs than their accumulated GPA. Is this data statistically significant?\n\nx, n = 290, 500\np = 0.5\nB = BinomialTest(x, n, p)\npvalue(B, tail=:right)\n\n0.00020020776841988704\n\n\nThis is an example of “transfer shock,” which over time is known to go away for most transfer students.\n\n\n8.3.4 Two-sample test of center\nA two-sample test of center can test if the population means or medians are equal. The latter could be done with the signed-rank test. Here we illustrate a test of the population means based on the \\(T\\)-statistic:\n\\[\nT = \\frac{(\\bar{X}_1 - \\bar{X}_2) - (\\mu_1 - \\mu_2)}{SE(\\bar{X}_1 - \\bar{X}_2)}.\n\\]\nThe null hypothesis will be \\(H_0: \\mu_1 = \\mu_2\\), the alternative is either direction or both. The \\(SE\\) will depend on an assumption of equal population variances or not (EqualVarianceTTest or UnequalVarianceTTest). The basic pattern to find the corresponding \\(p\\)-value is identical.\nFor example, consider the coffee-dispenser technician seeing if two machines have an equal calibration. The hypotheses would be:\n\\[\nH_0: \\mu_1 = \\mu_2, \\quad H_A:  \\mu_1 \\neq \\mu_2\n\\]\nThe collected data is:\nMachine 1: 7.0, 7.8, 7.7, 7.6, 8.3\nMachine 2: 6.2, 8.0, 6.8, 7.0, 7.3, 7.9, 7.1\nWithout assuming equal variances, the test could be carried out with:\n\nxs = [7.0, 7.8, 7.7, 7.6, 8.3]\nys = [6.2, 8.0, 6.8, 7.0, 7.3, 7.9, 7.1]\nT = UnequalVarianceTTest(xs, ys)\npvalue(T)  # use default tail=:both\n\n0.14802700278100428\n\n\nThe data does not suggest a statistically significant difference between the two machines.\n\n\n8.3.5 Two-sample test of proportions\nFor a comparison of two population proportions, we do the math using the test statistic:\n\\[\nZ = \\frac{(\\hat{p}_1 - \\hat{p}_2) - (p_1 - p_2)}{SE(\\hat{p}_1 - \\hat{p}_2)},\n\\]\nwhich for large enough \\(n\\) will have a standard normal distribution, assuming the sampling is well modeled by independent, binomial random variables.\nSuppose, we are interested in comparing two randomly chosen groups of transfer students at different institutions. We are testing if the GPA on the first semester after transfer falls off from the accumulated average. The test hypotheses are:\n\\[\nH_0: p_1 = p_2, \\quad, H_A: p_1 \\neq p_2.\n\\]\nThe test statistic involves \\(SE\\) where the \\(SD\\) is \\(\\sqrt{p_1(1-p_1)/n_1 + p_2(1-p_2)/n_2}\\). The null hypothesis states \\(p_1=p_2\\), but not what that value is, so the \\(SE\\) is found by estimating \\(p_1=p_2\\) by pooling the data.\nThe collected data is:\n             x      n\nSchool A    120    200\nSchool B    100    190\nWe proceed to compute the observed value of the test statistic:\n\nx1, n1 = 120, 200\nx2, n2 = 100, 190\nphat1, phat2 = x1/n1, x2/n2\nphat = (x1 + x2)/(n1 + n2) # pooled\nSE = sqrt(phat*(1-phat) * (1/n1 + 1/n2))\nZ_obs = (phat1 - phat2)/SE\n\n1.4667724206855928\n\n\nThis is a two-tailed test, so the \\(p\\)-value incorporates potential test statistics more than Z_obs or less than -Z_obs. (The minus sign comes, as Z_obs is positive.) Since the sampling distribution is a symmetric (standard normal) we have:\n\np_value = 2 * (1 - cdf(Normal(0,1), Z_obs))\n\n0.14243797452124007\n\n\nThis being larger than \\(\\alpha = 0.05\\), suggests no reason to reject the null hypothesis.\n\n\n8.3.6 Test of variances\nThe question of two, independent, normally distributed, samples having equal variances can be tested through an \\(F\\)-test and is implemented in VarianceFTest.\nFor example, consider a test for equal variances over two levels of some treatment with hypotheses:\n\\[\nH_0: \\sigma_1 = \\sigma_2, \\quad H_A: \\sigma_1 \\neq \\sigma_2\n\\]\nThe \\(F\\) test considers the ratio \\(\\sigma_1^2/\\sigma_2^2\\), so these hypotheses may also have been specified in terms of ratios.\nSuppose two independent samples were collected from normally distributed populations, giving:\nxs: -2.1, -1.8, 1.3,  -0.9, 1.7,  -2.0, -1.6, 3.8, -0.8, 5.5\nys:  7.6,  6.4, 7.2,  16.1, 6.6,  10.7, 11.0, 9.4\nWe enter the data and then pass this to the test:\n\nxs = [-2.1, -1.8, 1.3, -0.9, 1.7, -2.0, -1.6, 3.8, -0.8, 5.5]\nys = [7.6, 6.4, 7.2, 16.1, 6.6, 10.7, 11.0, 9.4]\nF = VarianceFTest(xs, ys)\npvalue(F, tail=:both)\n\n0.566171938952384\n\n\n\n\n\n\n\n\nRobustness of the F test\n\n\n\nThe \\(F\\)-test of variance, unlike, say, tests for the population mean, are sensitive to departures from the assumption of a normally distributed population. For this case, this was first noticed by Pearson (cf. (BOX 1953)). The more robust LeveneTest constructor implements Levene’s test for comparing two or more variances for equality.\n\n\n\n\n8.3.7 Test of correlation\nSuppose \\((X_1, Y_1), (X_2, Y_2), \\dots, (X_n, Y_n)\\) are jointly normally distributed. The Pearson correlation coefficient was given by:\n\\[\nr = \\frac{1}{n-1} \\sigma \\left(\\frac{X_i-\\bar{X}}{s_X}\\right) \\cdot \\left(\\frac{Y_i - \\bar{Y}}{s_Y}\\right).\n\\]\nThe exact sampling distribution function of \\(r\\) is known and involves the parameter \\(\\rho\\), the population correlation. Under the null hypothesis \\(\\rho=0\\), the distribution is a \\(T\\)-distribution with \\(n-2\\) degrees of freedom. This allows the testing of \\(0\\) correlation, which under this assumption is a test of independence.\nFor example, to test if these two normally distributed pairs of numbers are independent:\nx: -1.41  -0.8  -1.28  0.68   0.16  -1.28\ny: -0.34  -0.24  0.29  0.83  -0.77  -0.66\nwe have:\n\nx = [-1.41,  -0.8, -1.28, 0.68, 0.16, -1.28]\ny = [-0.34, -0.24, 0.29, 0.83, -0.77, -0.66]\nn = length(x)\n\nr_obs = cor(x, y)\n\n2 * ccdf(TDist(n-2), abs(r_obs))\n\n0.7244958336991354\n\n\nhttps://www.politico.com/news/2023/01/25/george-santos-199-expenses-00079334\nFor testing other values, the Fischer approximation proves useful, namely that\n\\[\nF(r) = \\frac{1}{2}\\ln\\left(\\frac{1 +r}{1-r}\\right) = \\tanh^{-1}(r)\n\\]\nhas an approximate normal distribution with mean \\(F(\\rho)\\) and standard error \\(1/\\sqrt{n-3}\\).\nThis can be used to generate confidence intervals, leading to a \\((1-\\alpha)\\cdot 100\\)% CI for \\(\\rho\\) expressed through \\(\\tanh^{-1}(r) - z_{1-\\alpha/2} SE < \\tanh^{-1}(\\rho) < \\tanh^{-1}(r) + z_{1-\\alpha/2} SE\\).\nFor the previous data, a \\(95\\)% percent confidence interval for \\(\\rho\\) could be constructed with:\n\nr = cor(x, y)\nn = length(x)\nalpha = 0.05\nza = quantile(Normal(0, 1), 1 - alpha/2)\nSE = 1/sqrt(n-3)\nMOE = za * SE\nlo′, hi′ = atanh(r) .+ (-MOE, MOE)\nlo, hi = tanh(lo′), tanh(hi′)\n\n(-0.6252789868635364, 0.9103466950735963)\n\n\nThis is very wide, as the sample size is small, leading to an \\(SE\\) value over \\(1/2\\).\n\n\n8.3.8 Goodness of fit test\nComparing categorical counting data to a specified multinomial model is done through a Chi-squared test. The expected count in each category is \\(E_i=n p_i\\), the count modeled by \\(X_i\\) and the statistic is \\(\\sum (X_i-E_i)^2/E_i\\).\n\nExample 8.7 (Benford’s law) Data to study Benford’s law appeared in the linked article. The authors compiled data from COVID websites to get counts on the number of confirmed cases across many data sources. The first digit of each was tallied, and produced this data:\n  1    2    3    4    5    6    7    8    9\n2863 1342 1055  916  744  673  580  461  377\nTo test if the data follow Benford’s law (\\(P(X=k) = \\log_{10}(k+1) - \\log_{10}(k)\\)) we have this as the null for \\(p_k\\) with an alternative of not so.\n\nxs = [2863, 1342, 1055,  916,  744,  673,  580,  461,  377]\n\nks = 1:9\npks = log.(10, ks .+ 1) - log.(10, ks)\n\nχ² = ChisqTest(xs, pks)\n(pvalue=pvalue(χ²), tstat=χ².stat, df=χ².df)\n\n(pvalue = 2.6484734636062243e-12, tstat = 71.34745744533376, df = 8)\n\n\nThis small \\(p\\)-value suggests the law does not exactly apply, though the general form of the law (monotonically decreasing probabilities) is certainly suggested.\n\n\nExample 8.8 In (Cressie and Read 1984), data on time passage and memory recall is provided. Following Haberman, a log-linear time trend is used to estimate the \\(p_i\\): \\(\\log(p_i) = \\alpha + \\beta\\cdot i\\).\nThat is, we will test:\n\\[\nH_0: \\log(p_i) = \\alpha + \\beta \\cdot i, \\quad H_A: \\text{not so}\n\\]\nThe values for \\(\\alpha\\) and \\(\\beta\\) are estimated from the data. Haberman used \\(\\alpha \\approx -2.1873, \\beta \\approx -0.0838\\). (Their method is not quite the same, but close to, fitting a linear model to \\(\\log(\\hat{p}_i) = \\alpha + \\beta i\\).)\nUsing this we can get the \\(\\chi^2\\) statistic as follows, with the data read from the article:\n\nxs = [15, 11, 14, 17, 5, 11, 10, 4, 8, 10, 7, 9, 11, 3, 6, 1, 1, 4]\nalpha, beta = -2.1873, -0.0838\nps = exp.(alpha .+ beta*(1:18))\nps /= sum(ps) # make sum to exactly 1\nχ² = ChisqTest(xs, ps)\nχ².stat\n\n22.716840144539344\n\n\nWe don’t show the \\(p\\) value – yet – as we need to consider an adjusted degrees of freedom. There are \\(s=2\\) parameters estimated from the data, so the degrees of freedom of this test statistic is \\(18 - 2 - 1 = 15\\). The \\(p\\)-value is found by computing the area to the right of the observed value, giving:\n\n1 - cdf(Chisq(18 - 2 - 1), χ².stat)\n\n0.09033970932839053\n\n\nThis data set was used by (Cressie and Read 1984) to illustrate the power-divergence statistic, which has a varying parameter \\(\\lambda\\). The Chi-squared test is \\(\\lambda = 1\\); a maximum-likelihood test is \\(\\lambda=0\\); a value of \\(\\lambda=2/3\\) is recommended by Cressie and Read as a robust general purpose value. We can test these, as follows:\n\nDict(l => 1 - cdf(Chisq(18 - 2 - 1), PowerDivergenceTest(xs; lambda=l, theta0=ps).stat)\n     for l in [0, 2/3, 1, 5])\n\nDict{Float64, Float64} with 4 entries:\n  0.0      => 0.0560191\n  0.666667 => 0.0824898\n  5.0      => 0.00204456\n  1.0      => 0.0903397\n\n\nThe value of \\(\\lambda=5\\) was used to show that larger values increase the size of the test statistic for this data, making the \\(p\\)-values smaller under the asymptotic distribution. The value (of \\((X_i/E_i)\\)) for \\(i=13\\) is nearly \\(2\\); powers over \\(1\\) make this one term dominate the value of the statistic. Something similar happens for powers much less than \\(1\\).\n\n\nExample 8.9 (Two-way tables) Let \\(X\\) and \\(Y\\) be two categorical variables summarizing a set of observations. Their counts could be summarized in a two-way table, say, with the \\(X\\) levels across the top, and the \\(Y\\) levels down the side. If there is no relationship between \\(X\\) and \\(Y\\) we would expect the proportions in each row to be roughly the same for each level of \\(X\\). If \\(X\\) depends on \\(Y\\), this would be expected.\nPut differently, if \\(X\\) and \\(Y\\) are not related, the expected count in cell row \\(i\\) column \\(j\\) would be \\(n \\cdot p_{ij}\\) or \\((n \\cdot P(Y=i)) \\cdot P(X=j)\\), the latter first finds the expected number in the \\(i\\)th level of \\(Y\\) times the probability of the \\(j\\) level of \\(X\\). That is, under an assumption of no association, the marginal probabilities should determine the cell probabilities.\nLet there be \\(c\\) levels of \\(X\\) and \\(r\\) levels of \\(Y\\). Viewing the table in a flat manner, there are \\(k=rc\\) different cells. If a multinomial model described the table, then the Chi-squared distribution would asymptotically describe the \\(\\chi^2\\) statistic’s sampling distribution. However, as there are \\(s = (r-1) + (c-1)\\) parameters that would need estimating (\\(r-1\\) for the \\(Y\\) probabilities as all \\(r\\) must add to \\(1\\), for example), the degrees of freedom would be \\(k-s-1 = rc - (r-1 + c-1) - 1 = (r-1)(c-1)\\).\nSuppose, a survey of many individuals was taken including questions on COVID-19 contraction and primary form of mitigation with the following data:\nMitigation/Contracted\n                 Yes    No\nVaccination       10   500\nIsolation          8   250\nFace mask         15   600\nIvermectin        20    40\nnone              20   300\nWe enter this two-way table in as a matrix, and then call ChisqTest to do the analysis:\n\ncnts = [10 500;\n        8  250;\n        15 600\n        20  40\n        20 300] # r=5, c=2 -> 5 df.\nχ² = ChisqTest(cnts)\n(pvalue=pvalue(χ²), tstat=χ².stat, df = χ².df)\n\n(pvalue = 4.5421259053037854e-30, tstat = 143.7051915805636, df = 4)\n\n\nThe small \\(p\\)-value for this made-up data suggests an association between primary mitigation and chance of contraction.\n\n\nExample 8.10 (Two-sample test of proportions) The methodology for two-way tables can be used for a two-sample test of proportions. Letting \\(X\\) count the successes and failures and \\(Y\\) the two different surveys.\nSeveral news articles of the time discussed a divide between “red” and “blue” states and their covid rates based on a politicalization of vaccinations in the United States. Suppose data was collected on whether a person with COVID-19 needed hospitalization was cross-tabulated with their county, classified broadly as “red” or “blue”. The data is\nType / Hospitalization\n       Yes    No\nRed    100  1000\nBlue    50   750\nTest the data under the hypothesis of no association against an alternative of an association. Again, the contingency table is entered as a matrix and ChisqTest called to perform the analysis:\n\ncnts = [100 1000;\n         50  750]\nχ² = ChisqTest(cnts)\n(pvalue=pvalue(χ²), tstat=χ².stat, df = χ².df)\n\n(pvalue = 0.023371321878153804, tstat = 5.140692640692601, df = 1)\n\n\nWe compare this to the following direct computation:\n\nn1, n2 = ns = map(sum, eachrow(cnts))\nphat1, phat2 = cnts[:, 1] ./  ns\nphat = sum(cnts[:,1]) / sum(cnts)\nSE = sqrt(phat*(1-phat) * (1/n1 + 1/n2))\n\nZobs = abs(phat1 - phat2)/SE  # use abs and double right tail\n\np_value = 2 * (1 - cdf(Normal(0, 1), Zobs))\n\n0.023371321878153273\n\n\nOne other way to analyze this question is with Fishers exact test. For a \\(2\\times 2\\) table, the bottom row divides into the top and the ratio of the two resulting numbers is considered (\\((a/c) / (b/d)\\)). If there is no association, this ratio should be close to \\(1\\). For this data, the Fisher exact test yields a \\(p\\)-value in the same range:\n\npvalue(FisherExactTest(100, 1000, 50, 750)) # a,b,c,d\n\n0.027803693593425226\n\n\nEither way, all produced a \\(p\\)-value smaller than the nominal \\(\\alpha=0.05\\) level for this made-up data.\n\n\n\n8.3.9 Likelihood ratio tests\nConsider again a hypothesis test with a concrete alternative:\n\\[\nH_0: \\mu = \\mu_0, \\quad H_A: \\mu = \\mu_1\n\\]\nSuppose the population is \\(Normal(\\mu, \\sigma)\\). We had a similar setup in the discussion on power, where for a \\(T\\)-test specifying three of a \\(\\alpha\\), \\(\\beta\\), \\(n\\), or an effect size allows the solving of the fourth using known facts about the \\(T\\)-statistic. The Neyman-Pearson lemma speaks to the uniformly most powerful test under this scenario with a single unknown parameter (the mean above, but could be the standard devation, etc.).\nThis test can be realized as a likelihood ratio test, which also covers tests of more generality. Suppose the parameters being tested are called \\(\\theta\\) which sit in some subset \\(\\Theta_0 \\subset \\Theta\\). The non-directional alternative would be \\(\\theta\\) is in \\(\\Theta \\setminus \\Theta_0\\).\nA test for this can be based on the likelihood ratio statistic:\n\\[\n\\lambda = -2 \\ln \\frac{\\sup_{\\theta \\in \\Theta_0} L(\\theta, x)}{\\sup_{\\theta \\in \\Theta} L(\\theta, x)} =\n-2(\\ln \\sup_{\\theta \\in \\Theta_0} L(\\theta, x) - \\ln \\sup_{\\theta \\in \\Theta} L(\\theta, x)).\n\\]\nThe \\(\\sup\\) is essentially the maximum value of the function over the set, which above is either the specific set of values in the null or all possible values. The ratio here is in \\([0,1]\\), the logarithm is then negative, the factor \\(-2\\) means \\(\\lambda\\) is in \\([0, \\infty)\\). Under some assumptions, \\(\\lambda\\) will have an asymptotically Chi-square distribution with the degrees of freedom given by the dimensionality of \\(\\Theta\\).\nWhile only in the simple case is the likelihood ratio test guaranteed to be the most powerful, the approach is much more general than the ad hoc tests described previously and mostly agrees with them.\nLet’s consider the case of a survey modeled by a binomial. We test \\(H_0: p=1/2\\) against a two-sided alternative.\nThe log-likelihood function, \\(l(p,x)\\), for the data is simply:\n\nfunction loglik(θ, data) # both θ, data are containers\n    p = first(θ)\n    x, n = data\n    logpdf(Binomial(n, p), x)\nend\n\nloglik (generic function with 1 method)\n\n\nThe test statistic is found by computing \\(-2 (l(1/2) - l(\\hat{p}))\\), where \\(\\hat{p}\\) maximizes \\(l(p,x)\\).\nWe use findmax to identify the maximum.\nSuppose our data is 60 of \\(100\\).\n\ndat = (x=60, n=100)\nps = range(0, 1, length=201)\nls = loglik.(ps, Ref(dat))  # don't broadcast over data\nl̂, i = findmax(ls)\np̂ = ps[i]\n\n0.6\n\n\nWe see \\(\\hat{p} = x/n\\) (which could have been shown mathematically). The value of the log-likelihood ratio statistic can be found through:\n\np₀ = 1/2\nll_obs = -2*(loglik(p₀, dat) - l̂)\n\n4.027102710137768\n\n\nThe \\(Chisq(1)\\) distribution describes this statistic asymptotically. Supposing that to be valid, the \\(p\\)-value is computed by looking at the probability of a more extreme value under the null hypothesis, which are values larger:\n\n1 - cdf(Chisq(1), ll_obs)\n\n0.04477477232924443\n\n\n\nExample 8.11 (The linear-regression model) The simple linear regression model was given by \\(y_i = \\beta_0 + \\beta_1 x_i + e_i\\), where for purposes of modeling, we will take \\(e_i\\) to have a \\(Normal(0, \\sigma)\\) distribution. We discuss this model more formally in the subsequent chapter, but here we see we can approach the model using maximum likelihood.\nFor a given data set, we test whether \\(\\beta_1=0\\) against a two-sided alternative.\nSuppose the data is measured dosages of ivermectin with recovery time for COVID-19 infected patients.\ndose (μg/kg): 100 100 200 200 400 400 600 800\ntime (days) :   5   5   6   4   5   8   6   6\nThe model has \\(3\\) parameters \\(\\beta_0, \\beta_1\\) and \\(\\sigma\\), the standard deviation. We have the log-likelihood function given by:\n\nfunction loglik(θ, data)\n    # yᵢ ~ N(β₀ + β₁⋅x, σ) is model\n    β₀, β₁, σ = θ\n    y, x = data\n\n    ll = 0.0\n    for (yᵢ,xᵢ) ∈ zip(y, x)\n        μᵢ = β₀ + β₁ * xᵢ\n        D = Normal(μᵢ, σ)\n        ll += logpdf(D, yᵢ)\n    end\n    ll\nend\n\nloglik (generic function with 1 method)\n\n\nWe have the data entered as:\n\nx = [100, 100, 200, 200, 400, 400, 600, 800]\ny = [5, 5, 6, 4, 5, 8, 6, 6];\n\nWe use the ProfileLikelihood package to model this. First we set up the model using an initial guess at the maximum likelihood estimators:\n\ndat = (y, x)\nθ₀ = [5.0, 0.0, 1.0]\nnms = [:β₀, :β₁, :σ]\nprob = LikelihoodProblem(loglik, θ₀;\n                         data = dat,\n                         syms = nms)\n\n\nLikelihoodProblem. In-place: true\nθ₀: 3-element Vector{Float64}\n     β₀: 5.0\n     β₁: 0.0\n     σ: 1.0\n\n\n\nWe solve the problem through:\n\nsol = mle(prob, Optim.NelderMead())\n\n\nLikelihoodSolution. retcode: Success\nMaximum likelihood: -11.466321890403758\nMaximum likelihood estimates: 3-element Vector{Float64}\n     β₀: 4.948881523162539\n     β₁: 0.0019318326444597892\n     σ: 1.0144493565817676\n\n\n\nWe see a small estimated value for \\(\\beta_1\\). Is it statistically significant? For this, we choose to profile the value, and rely on the relationship between confidence intervals and significance tests: if the 95% CI for \\(\\beta_1\\) includes \\(0\\), then the significance test would not reject the null.\n\nlb = [-100.0, -100.0, 0.0]\nub = [ 100.0,  100.0, 100.0]  # -100 < β₀, β₁ < 100; 0 < σ < 100\nresolutions = [200, 200, 200]\nparam_ranges = construct_profile_ranges(sol, lb, ub, resolutions)\nprof = profile(prob, sol; param_ranges=param_ranges)\nprof[:β₁]\n\n\nProfile likelihood for parameter β₁. MLE retcode: Success\nMLE: 0.0019318326444597892\n95.0% CI for β₁: (-0.009936997524490673, 0.01380030329166489)\n\n\n\nWe can visualize the log-likelihood over the value in the \\(95\\)% confidence interval with the following:\n\nxs = range(-0.02, 0.02, length=100); ys = prof[:β₁].(xs)\np = data((x=xs, y=ys)) * visual(Lines) * mapping(:x, :y)\np += mapping([sol[:β₁]]) * visual(VLines)\nci = [extrema(prof.confidence_intervals[2])...]\np += data((x=ci, y = prof[:β₁].(ci))) * visual(Lines) * mapping(:x, :y)\ndraw(p)\n\n\n\n\nWere a significance test desired, the test statistic requires one more optimization calculuation, this time the maximum log likelihood under \\(H_0\\), which assumes a fixed value of \\(\\beta_1\\):\n\nfunction loglik₀(θ, data)\n    # yᵢ ~ N(β₀ + β₁⋅x, σ) is model\n    β₀, σ = θ\n    β₁ = 0.0     # H₀: β₁ = 0\n    y, x = data\n\n    ll = 0.0\n    for (yᵢ,xᵢ) ∈ zip(y, x)\n        μᵢ = β₀ + β₁ * xᵢ\n        D = Normal(μᵢ, σ)\n        ll += logpdf(D, yᵢ)\n    end\n    ll\nend\n\nloglik₀ (generic function with 1 method)\n\n\n\nprob₀ = LikelihoodProblem(loglik₀, [5.0, 1.0];\n                         data = (y, x),\n                         syms = [:β₀, :σ])\nsol₀ = mle(prob₀, Optim.NelderMead())\n\n\nLikelihoodSolution. retcode: Success\nMaximum likelihood: -12.193767351012552\nMaximum likelihood estimates: 2-element Vector{Float64}\n     β₀: 5.62501607088573\n     σ: 1.1110596750702828\n\n\n\nThe likelihood ratio statistic is computed with the difference of the respective maximums, available through the maximum property of the solution:\n\nl = -2 * (sol₀.maximum - sol.maximum)\n\n1.4548909212175865\n\n\nThis observed value can be turned into a \\(p\\)-value using the asymptotically correct \\(Chisq(1)\\) distribution:\n\n1 - cdf(Chisq(1), l)\n\n0.22774478131827325\n\n\nThere is no evidence in the data to reject the null hypothesis of no effect.\n\n\n\n\n\nBOX, G. E. P. (1953), “NON-NORMALITY AND TESTS ON VARIANCES,” Biometrika, 40, 318–335. https://doi.org/10.1093/biomet/40.3-4.318.\n\n\nCressie, N., and Read, T. R. C. (1984), “Multinomial goodness-of-fit tests,” Journal of the Royal Statistical Society. Series B (Methodological), [Royal Statistical Society, Wiley], 46, 440–464."
  },
  {
    "objectID": "LinearModels/linear-regression.html#multiple-linear-regression",
    "href": "LinearModels/linear-regression.html#multiple-linear-regression",
    "title": "9  The linear regression model",
    "section": "9.1 Multiple linear regression",
    "text": "9.1 Multiple linear regression\nThe simple linear regression model related a covariate variable, \\(x\\), to a response variable \\(Y\\) through a formulation:\n\\[\nY_i = \\beta_0 + \\beta_1 \\cdot x_i + e_i,\n\\]\nwhere \\(\\beta_0, \\beta_1\\) are the parameters for the model that describe the average value of the \\(Y\\) variable for a given \\(x\\) value and the random errors, \\(e_i\\), are assumed to be described to be a random sample from some distribution, usually \\(Normal(0, \\sigma)\\). Some inferential results require this sample to be iid.\nThe model for multiple regression is similar, though there are \\(k = 0, 1\\), or more covariates accounted for in the notation:\n\\[\nY_i = \\beta_0 + \\beta_1 \\cdot x_{1i} + \\beta_2 \\cdot x_{2i} + \\cdots + \\beta_k \\cdot x_{ki} + e_i.\n\\]\nFollowing (Wackerly et al. 2008) we formulate the regression model using matrix algebra and quickly review their main results.\nIf there are \\(n\\) groups of data, then the main model matrix is the \\(n\\times (k+1)\\) matrix:1\n\\[\nX =\n\\begin{bmatrix}\n1      & x_{11} & x_{12} & \\cdots & x_{1k}\\\\\n1      & x_{21} & x_{22} & \\cdots & x_{2k}\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{nk}\n\\end{bmatrix}.\n\\]\nThe response, parameters, and errors are recorded in column vectors:\n\\[\nY = [y_1, y_2, \\cdots, y_n], \\quad \\beta = [\\beta_0, \\beta_1, \\dots, \\beta_k], \\quad e= [e_1, e_2, \\dots, e_n].\n\\]\nThe collection of equations can then be written as \\(Y = X\\beta + e\\).2\nThe notation \\(\\hat{Y} = X\\beta\\) is common, as the values \\(X\\beta\\) are used to predict the average values of the response. The least squares problem to estimate the parameters is then to find \\(\\beta\\) that minimizes:\n\\[\n\\lVert Y - \\hat{Y} \\rVert^2 = \\lVert Y - X\\beta \\rVert^2.\n\\]\nUsing calculus, it can be seen that any minimizer, \\(\\hat{\\beta}\\), will satisfy \\(X'X\\hat{\\beta}=X'Y\\), with \\(X'\\) denoting the transpose3. When written out these are called the normal equations, which, when the matrix \\(X\\) is non-degenerate (rank \\(k+1\\) in this case) can be solved algebraically by\n\\[\n\\hat{\\beta} = (X'X)^{-1}X' Y.\n\\]\nWe now assume the errors are an iid random sample from a distribution with mean \\(0\\) and variance \\(\\sigma^2\\). The matrix of covariances, \\(COV(\\beta_i, \\beta_j)\\), is called the covariance matrix and denoted \\(\\Sigma_{\\hat{\\beta}\\hat{\\beta}}\\). Under these assumptions, it can be shown to satisfy:\n\\[\n\\Sigma_{\\hat{\\beta}\\hat{\\beta}} = \\sigma^2 (X'X)^{-1}.\n\\]\nThe parameter \\(\\sigma^2\\) can be estimated. We have the residuals are given in vector form by \\(\\hat{e} = Y - \\hat{Y}\\). Here \\(\\hat{Y} = \\hat{H}Y\\) where \\(\\hat{H} = X(X'X)^{-1}X'\\) is called the “hat” matrix and comes from \\(\\hat{Y} = X\\hat{\\beta} = X((X'X)^{-1}X'Y) = (X(X'X)^{-1}X')Y\\).\nThe hat matrix has the property of a projection matrix taking values in an \\(n\\) dimensional space and projecting onto a subspace described by the columns of the \\(X\\) matrix. This allows a geometric interpretation of the least-squares formulation.4\nThe sum of squared residuals is:\n\\[\n\\sum (Y_i - \\hat{Y_i})^2 = \\lVert Y - \\hat{Y} \\rVert^2 = \\lVert (1-\\hat{H})Y\\rVert^2 = Y'(I-\\hat{H})Y,\n\\]\nwhere \\(I\\) is the diagonal matrix of all ones that acts like an identity under multiplication.\nThe expected value can be computed to get \\(E(\\lVert Y - \\hat{Y} \\rVert^2) = (n - 1 - k)\\sigma^2\\), which is used to estimate \\(\\sigma^2\\):\n\\[\ns^2 = \\frac{\\lVert Y - \\hat{Y} \\rVert^2}{n-1-k} = \\frac{\\sum (Y_i - \\hat{Y_i})^2}{n-1-k}.\n\\]\n(When \\(k=0\\), this is the same as the sample standard deviation with \\(\\hat{Y_i}\\) simply \\(\\bar{Y}\\).)\nMore is known of the \\(\\hat{\\beta}\\), in particular the distribution of:\n\\[\n\\frac{(\\hat{\\beta}-\\beta) \\cdot (X'X) \\cdot (\\hat{\\beta} - \\beta)}{\\sigma^2},\n\\]\nis \\(Chisq(k+1)\\) and, if the errors are from an iid random sample with population \\(Normal(0, \\sigma)\\), is independent of \\(s^2\\). In which case, the ratio is \\(F\\) distributed, leading to the following: a \\((1-\\alpha)\\cdot 100\\)% joint confidence interval for \\(\\beta\\) is found with:\n\\[\n(\\hat{\\beta}-\\beta) \\cdot (X'X) \\cdot (\\hat{\\beta}-\\beta) \\leq (1+k)s^2 F_{1-\\alpha; 1 + k, n-1-k}.\n\\]\nWhile the error terms, \\(e\\), are assumed to be independent, the residuals, \\(\\hat{e}\\), are not so, as one large residual must be offset by other smaller ones due to the minimization of the squared residuals. The matrix of their covariances can be expressed as \\(\\Sigma_{\\hat{e}\\hat{e}} = \\sigma^2 (I-\\hat{H})\\). The standardized residuals account for the \\((I-\\hat{H})\\) and are given by: \\(e_i/(s\\sqrt{1 - \\hat{H}_{ii}})\\).\nWhen the errors are an iid sample then the fitted values, \\(\\hat{Y}\\), are uncorrelated with the residuals.\nIf it is assumed the error population is normal, then the least-square estimates for \\(\\beta\\), given by \\(\\hat{\\beta} = (X'X)^{-1}X Y\\), are linear combinations of independent normal random variables, and consequently are normally distributed. (This is assuming the covariates are not random, or the \\(Y\\) values are conditionally independent.) For each \\(i\\), we have \\(E(\\hat{\\beta}_i) = \\beta_i\\) and \\(SE(\\hat{\\beta}_i) = s_{\\hat{\\beta}_i} = s \\sqrt{c_{ii}}\\), where \\(c_{ii}\\) is the diagonal entry of \\((X'X)^{-1}\\). Moreover, the \\(T\\)-statistic:\n\\[\nT = \\frac{\\hat{\\beta}_i - \\beta_i}{SE(\\hat{\\beta}_{ii})},\n\\]\nwill have a \\(T\\)-distribution with \\(n-k-1\\) degrees of freedom, when the errors are iid and normally distributed. For a single estimate, \\(\\hat{\\beta}_i \\pm t_{1 - \\alpha/2, n-1-k} s \\sqrt{((X'X)^{-1})_{ii}}\\) forms a \\((1 - \\alpha)\\cdot 100\\)% confidence interval for \\(\\beta_i\\).\nWhen the regression model is used for predicting the mean response for a given set of covariates, \\(x_0\\), the predictor would be \\(\\hat{\\mu}_0 = x_0 \\cdot \\hat{\\beta}\\) (with the first value for \\(x_0\\) being \\(1\\)). The variance can be computed to give \\(VAR(\\hat{\\mu}_0) = \\sigma^2 x_0'(X'X)^{-1}x_0\\), which depends on the value of \\(x_0\\). Confidence bands drawn by the linear() visualization for a scatterplot of data use this formula and a related one to estimate a single value, not an average value. The dependence on \\(x_0\\) gives the curve away from the center, \\(\\bar{x}\\).\nA measure of how much variation in the response is explained by the dependence on the respective covariates (the coefficient of determination) is given by \\(R^2\\) which is computed by\n\\[\nR^2 = 1 - \\frac{SSR}{SST},\n\\]\nwhere \\(SSR = \\sum \\hat{e}_i^2 = \\sum (y_i - \\hat{y}_i)^2\\) is the sum of the squared residuals and \\(SST = \\sum (y_i - \\bar{y})^2\\) is the total sum of the squares. When the ratio \\(SSR/SST\\) is close to \\(1\\), then the model (\\(\\hat{y}\\)) doesn’t explain much of the variation compared to the null model with all the \\(\\beta_i\\)’s, \\(i \\geq 1\\) being \\(0\\) and the sample mean of the \\(y\\), \\(\\bar{y}\\), used for prediction. Conversely, when the ratio is close to \\(0\\), then the model explains much of the variation. By subtracting this from \\(1\\), as is customary, we have the interpretation that \\(R^2\\) explains \\(R^2 \\cdot 100\\)% of the variation in the \\(y\\) values.\nThe value of \\(R^2\\) can be made equal to \\(1\\) with enough variables; the adjusted \\(R^2\\) value is a modification that weights \\(SSR/SST\\) by \\((n-1)/(n-k-1)\\) so using more variables (bigger \\(k\\)) is penalized.\n\n9.1.1 Generic methods for statistical models\nThe StatsBase package defines methods for the above calculations and more. These are generic methods with similar usage for other models than the linear regression model discussed here. Table 9.1 lists several.\n\n\nTable 9.1: Generic methods for statistical models defined in StatsBase. Those marked with a \\(*\\) are defined on the model property of the lm output.\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\ncoef\nLeast squares estimates for \\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\dots, \\hat{\\beta}_k\\)\n\n\ncoefnames\nNames of coefficients\n\n\nstderror\nStandard errors of coefficients\n\n\nresiduals\nResiduals, \\(y_i - \\hat{y}_i\\)\n\n\nfitted\n\\(\\hat{y}_i\\) values\n\n\npredict\nPredict future values using \\(\\hat{\\beta}\\)s\n\n\nconfint\nConfindence interval for estimated parameters\n\n\nmodelmatrix\nMatrix (\\(X\\)) used in computations\n\n\nnobs\n\\(n\\)\n\n\ndof\nConsumed degrees of freedom (matrix rank plus \\(1\\))\n\n\ndof_residual\nresidual degrees of freedom. \\(n-k-1\\)\n\n\nr2\nCoefficient of determination, \\(1 - SSR/SST\\)\n\n\nadjr2\nAdjusted \\(R^2\\), \\(1 - SSR/SST \\cdot (n-1)/(n-1-k)\\)\n\n\nvcov\nVariance/Covariance matrix for the \\(\\hat{\\beta}\\)s\n\n\ndeviance\nResidual sum of squares, \\(SSR\\)\n\n\ndispersion\\(^*\\)\nEstimate for \\(\\sigma\\), \\(\\hat{\\sigma} = \\sqrt{SSR/(n-1-k)}\\) or dispersion(res)/dof_residual(res)\n\n\nnulldeviance\nTotal sum of squares \\(SST = \\sum(y_i - \\bar{y})^2\\).\n\n\nloglikelihood\nLog-likelihood of the model\n\n\nnullloglikelihood\nLog-likelihood of null model\n\n\nftest\\(^*\\)\nCompute \\(F\\)-test of two or more nested models\n\n\naic\nAkaike’s Information Criterion, \\(-2\\log(L) + 2(k+2)\\)\n\n\nbic\nBayesian Information Criterion, \\(-2\\log(L) + (k+2)\\log(n)\\)\n\n\n\n\n\nExample 9.1 (Example of simple linear regression) Consider some fabricated data on dosage amount of Ivermectin and days to recovery of COVID-19 fit by a simple linear model:\n\nx = [100, 100, 200, 200, 400, 400, 600, 800]\ny = [5, 5, 6, 4, 5, 8, 6, 6];\nres = lm(@formula(y ~ x), (; x, y))  # uses named tuple to specify data\n\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\ny ~ 1 + x\n\nCoefficients:\n────────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error     t  Pr(>|t|)    Lower 95%   Upper 95%\n────────────────────────────────────────────────────────────────────────────\n(Intercept)  4.94886     0.744004    6.65    0.0006   3.12835     6.76938\nx            0.00193182  0.00176594  1.09    0.3159  -0.00238928  0.00625292\n────────────────────────────────────────────────────────────────────────────\n\n\nThe output shows the estimated coefficients, \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\). These are computed by \\((X'X)^{-1}XY\\):\n\nX = modelmatrix(res)\ninv(X' * X) * X' * y\n\n2-element Vector{Float64}:\n 4.948863636363637\n 0.0019318181818181804\n\n\nThese are also returned by the coef method, as in coef(res).\nThe default output also computes confidence intervals and performs two-sided tests of whether the parameter is \\(0\\). Focusing on \\(\\hat{\\beta}_1\\), we can find its standard error from \\(s \\sqrt{c_{ii}}\\). First we compute \\(s\\):\n\nn, k = length(x), 1\ns² = sum(eᵢ^2 for eᵢ in residuals(res)) / (n - k - 1)\ns = sqrt(s²)  # also dispersion(res.model)\nC = inv(X'*X)\nsbetas = s * sqrt.(diag(C))\n\n2-element Vector{Float64}:\n 0.7440036635973516\n 0.0017659398031727332\n\n\nMore conveniently, these are returned by the stederror method:\n\nstderror(res)\n\n2-element Vector{Float64}:\n 0.7440036635973516\n 0.0017659398031727338\n\n\nThese are also the square root of the diagonal of the covariance matrix, \\(\\Sigma_{\\hat{\\beta}\\hat{\\beta}}\\), computed by the vcov method:\n\nΣᵦᵦ = vcov(res)\n\n2×2 Matrix{Float64}:\n  0.553541    -0.00109149\n -0.00109149   3.11854e-6\n\n\n\nsqrt.(diag(Σᵦᵦ))\n\n2-element Vector{Float64}:\n 0.7440036635973516\n 0.0017659398031727338\n\n\nThe \\(T\\)-statistic for \\(H_0: \\beta_1 = 0\\) is then\n\nβ̂₁ = coef(res)[2]\nSE₁ = stderror(res)[2]\nT_obs = (β̂₁ - 0) / SE₁\n\n1.0939320685492355\n\n\nThe \\(p\\)-value is then found directly with:\n\n2 * ccdf(TDist(n-k-1), T_obs)\n\n0.315945937293384\n\n\nThis computation is needed were there different assumed values than \\(\\beta_1 = 0\\) for the null, though this particular \\(p\\)-value is included in the default display of res.\nThe confidence intervals are of the form \\(\\hat{\\beta}_i \\pm t_{1-\\alpha/2; n-k-1} \\cdot SE(\\hat{\\beta}_i)\\). We find one for the intercept term, \\(\\beta_0\\):\n\nalpha = 0.05\nta = quantile(TDist(n-k-1), 1 - alpha/2)\nβ̂₀ = coef(res)[1]\nSE₀ = stderror(res)[1]\nβ̂₀ .+ ta * [-SE₀, SE₀]\n\n2-element Vector{Float64}:\n 3.128352254612003\n 6.769375018115272\n\n\nThe confint method will also compute these, returning the values as rows in a matrix:\n\nconfint(res)\n\n2×2 Matrix{Float64}:\n  3.12835     6.76938\n -0.00238928  0.00625292\n\n\nWe compute the confidence interval for \\(\\hat{\\mu}\\) when \\(x=500\\) using the variance formula above.\n\nx0 = [1, 500]\nμ̂ = predict(res, (x=[500],))[1]  # also  dot(inv(X'*X)*X'*y, x0)\nSE = s * sqrt(x0' * inv(X' * X) * x0)\nci = μ̂ .+ ta * [-SE, SE]\n\n2-element Vector{Float64}:\n 4.711829667120805\n 7.117715787424649\n\n\nWe can visualize (Figure 9.1) with the following commands:\n\nlayers = (visual(Scatter) + linear(; interval=:confidence))\np = data((;x, y)) * layers * mapping(:x, :y)\np += data((x=[500, 500], y=ci)) * visual(Lines) * mapping(:x, :y)\ndraw(p)\n\n\n\n\nFigure 9.1: Illlustration of linear regression model with confidence band drawn. The vertical line is computed directly for the value of \\(x=500\\).\n\n\n\n\nThe value of \\(R^2\\) can be computed directly:\n\n1 - sum(eᵢ^2 for eᵢ in residuals(res)) / sum((yᵢ - mean(y))^2 for yᵢ in y)\n\n0.166283084004603\n\n\nThis can also be computed using several of the methods defined for model outputs by GLM:\n\nr2(res), 1 - deviance(res)/nulldeviance(res)\n\n(0.166283084004603, 0.166283084004603)\n\n\nWhichever way, for this model a low \\(R^2\\) implies the model does not explain much of the variance in the response.\n\n\nExample 9.2 (Multiple regression example) We give an example of multiple linear regression using a data set on various cereal boxes in a US grocery store.\n\ncereal = dataset(\"MASS\", \"UScereal\")\nfirst(cereal, 2)\n\n\n2×12 DataFrameRowBrandMFRCaloriesProteinFatSodiumFibreCarboSugarsShelfPotassiumVitaminsStringCat…Float64Float64Float64Float64Float64Float64Float64Int32Float64Cat…1100% BranN212.12112.12123.0303393.93930.30315.151518.18183848.485enriched2All-BranK212.12112.12123.0303787.87927.272721.212115.15153969.697enriched\n\n\nThe data set collected numerous variables, here we consider numeric ones:\n\nnames(cereal, Real) |> permutedims\n\n1×9 Matrix{String}:\n \"Calories\"  \"Protein\"  \"Fat\"  \"Sodium\"  …  \"Sugars\"  \"Shelf\"  \"Potassium\"\n\n\nThe initial model we consider has Calories as a response, and several covariates:\n\nfm = @formula(Calories ~ Protein + Fat + Sodium + Carbo + Sugars)\nres = lm(fm, cereal)\n\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nCalories ~ 1 + Protein + Fat + Sodium + Carbo + Sugars\n\nCoefficients:\n────────────────────────────────────────────────────────────────────────────────\n                    Coef.  Std. Error      t  Pr(>|t|)    Lower 95%    Upper 95%\n────────────────────────────────────────────────────────────────────────────────\n(Intercept)  -18.9515       3.61524    -5.24    <1e-05  -26.1856     -11.7175\nProtein        3.99092      0.598282    6.67    <1e-08    2.79376      5.18808\nFat            8.86319      0.803833   11.03    <1e-15    7.25472     10.4717\nSodium         0.00266192   0.0107093   0.25    0.8046   -0.0187674    0.0240913\nCarbo          4.91706      0.162811   30.20    <1e-36    4.59128      5.24284\nSugars         4.20214      0.216049   19.45    <1e-26    3.76983      4.63446\n────────────────────────────────────────────────────────────────────────────────\n\n\nThe output shows what might have been anticipated: there appears to be no connection between Sodium and Calories, though were this data on dinner foods that might not be the case. The \\(T\\)-test displayed for Sodium is a test of whether the slope based on Sodium is \\(0\\) – holding the other variables constant – and the large \\(p\\)-value would lead us to accept that hypotheses.\nWe drop this variable from the model and refit:\n\nres = lm(@formula(Calories ~ Protein + Fat + Carbo + Sugars), cereal)\n\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nCalories ~ 1 + Protein + Fat + Carbo + Sugars\n\nCoefficients:\n─────────────────────────────────────────────────────────────────────────\n                 Coef.  Std. Error      t  Pr(>|t|)  Lower 95%  Upper 95%\n─────────────────────────────────────────────────────────────────────────\n(Intercept)  -18.7698     3.51272   -5.34    <1e-05  -25.7963   -11.7433\nProtein        4.05056    0.543752   7.45    <1e-09    2.9629     5.13823\nFat            8.85889    0.797339  11.11    <1e-15    7.26398   10.4538\nCarbo          4.92466    0.158656  31.04    <1e-37    4.6073     5.24202\nSugars         4.21069    0.211619  19.90    <1e-27    3.78739    4.634\n─────────────────────────────────────────────────────────────────────────\n\n\nHow to interpret this? Each coefficient (save the intercept) measures the predicted change in mean number of calories for a \\(1\\)-unit increase holding the other variables fixed. For example, it is suggested that adding 1 additional unit of protein holding the other variables constant would add nearly 4 calories per serving, on average.\n\n\nExample 9.3 (Polynomial regression) Dickey provides an analysis of Galileo’s falling ball data. Galileo rolled a ball down an elevated ramp at certain distances, the ball then jumped down a certain distance that varied depending on the height of the ramp. The collected data is:\n\nrelease = [1000, 800, 600, 450, 300, 200, 100]\nhorizontal_distance = [573, 534, 495, 451, 395, 337, 253]\ngalileo = DataFrame(; release, horizontal_distance)\nfirst(galileo, 3)\n\n\n3×2 DataFrameRowreleasehorizontal_distanceInt64Int641100057328005343600495\n\n\nWith an assumption that the horizontal distance was related to \\(v_xt\\) and \\(t\\) was found by solving for \\(0 = h - (1/2)gt^2\\), we might expect \\(h\\) and \\(t\\) to be quadratically related. We consider, somewhat artifcially, the release height modeled linearly by the horizontal distance:\n\nres = lm(@formula(y ~ x), (y=galileo.release, x=galileo.horizontal_distance))\n\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\ny ~ 1 + x\n\nCoefficients:\n─────────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error      t  Pr(>|t|)    Lower 95%   Upper 95%\n─────────────────────────────────────────────────────────────────────────────\n(Intercept)  -713.262    156.449     -4.56    0.0061  -1115.43     -311.098\nx               2.77908    0.350392   7.93    0.0005      1.87837     3.67979\n─────────────────────────────────────────────────────────────────────────────\n\n\nWithout much effort, the small \\(p\\)-value would lead one to conclude the linear term is statistically significant. But Galileo might have expected a quadratic relationship and a modern reader might, as well, viewing Figure 9.2, such as modeled by the following:\n\nres₂ = lm(@formula(y ~ x + x^2), (y=galileo.release, x=galileo.horizontal_distance))\n\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\ny ~ 1 + x + :(x ^ 2)\n\nCoefficients:\n────────────────────────────────────────────────────────────────────────────────────\n                    Coef.     Std. Error      t  Pr(>|t|)     Lower 95%    Upper 95%\n────────────────────────────────────────────────────────────────────────────────────\n(Intercept)  600.054       110.543         5.43    0.0056  293.136       906.971\nx             -4.00484       0.55582      -7.21    0.0020   -5.54805      -2.46164\nx ^ 2          0.00818074    0.000665952  12.28    0.0003    0.00633176    0.0100297\n────────────────────────────────────────────────────────────────────────────────────\n\n\nThe rules of @formula parse the above as adding a variable x^2 to the model. Alternatively, the data frame could have been transformed to produce that variable. The output shows the test of \\(\\beta_2=0\\) would be rejected for reasonable values of \\(\\alpha\\).\n\n\n\n\n\nFigure 9.2: Scatter plot of falling-ball data of Galileo with a linear model fit. The curve suggests a quadratic model."
  },
  {
    "objectID": "LinearModels/linear-regression.html#categorical-covariates",
    "href": "LinearModels/linear-regression.html#categorical-covariates",
    "title": "9  The linear regression model",
    "section": "9.2 Categorical covariates",
    "text": "9.2 Categorical covariates\nThe linear regression model is more flexible than may appear on first introduction through simple regression.\nFor example, the regression model when there are no covariates is just a one-sample \\(T\\)-test, as seen from this example where a two-sided test of \\(0\\) mean is carried out.\n\ny = [-0.2, 1.9, 2.7, 2.6, 1.5, 0.6]\nlm(@formula(y ~ 1), (;y)) # using a named tuple for the data\n\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\ny ~ 1\n\nCoefficients:\n──────────────────────────────────────────────────────────────────────\n               Coef.  Std. Error     t  Pr(>|t|)  Lower 95%  Upper 95%\n──────────────────────────────────────────────────────────────────────\n(Intercept)  1.51667    0.465773  3.26    0.0225   0.319359    2.71397\n──────────────────────────────────────────────────────────────────────\n\n\nThe formula usually has an implicit intercept, but here with no covariates listed, it must be made explicit. Compare the values with the following:\n\nOneSampleTTest(y)\n\nOne sample t-test\n-----------------\nPopulation details:\n    parameter of interest:   Mean\n    value under h_0:         0\n    point estimate:          1.51667\n    95% confidence interval: (0.3194, 2.714)\n\nTest summary:\n    outcome with 95% confidence: reject h_0\n    two-sided p-value:           0.0225\n\nDetails:\n    number of observations:   6\n    t-statistic:              3.2562360146885347\n    degrees of freedom:       5\n    empirical standard error: 0.46577295374940403\n\n\nFurther, the two-sample \\(T\\)-test (with equal variances assumed) can be performed through the regression model. After tidying the data, we fit a model:\n\ny1 = [5, 4, 6, 7]\ny2 = [7, 6, 5, 4, 5, 6, 7]\ndf = DataFrame(group=[\"g1\", \"g2\"], value=[y1, y2])\nd = flatten(df, [:value])\n\nres = lm(@formula(value ~ group), d)\n\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nvalue ~ 1 + group\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────\n                Coef.  Std. Error     t  Pr(>|t|)  Lower 95%  Upper 95%\n───────────────────────────────────────────────────────────────────────\n(Intercept)  5.5         0.58757   9.36    <1e-05    4.17083    6.82917\ngroup: g2    0.214286    0.736558  0.29    0.7777   -1.45192    1.88049\n───────────────────────────────────────────────────────────────────────\n\n\nWe can compare the computed values to those computed a different way:\n\nEqualVarianceTTest(y2, y1)\n\nTwo sample t-test (equal variance)\n----------------------------------\nPopulation details:\n    parameter of interest:   Mean difference\n    value under h_0:         0\n    point estimate:          0.214286\n    95% confidence interval: (-1.452, 1.88)\n\nTest summary:\n    outcome with 95% confidence: fail to reject h_0\n    two-sided p-value:           0.7777\n\nDetails:\n    number of observations:   [7,4]\n    t-statistic:              0.2909286827258563\n    degrees of freedom:       9\n    empirical standard error: 0.7365575380122867\n\n\nHowever, some comments are warranted. We would have found a slightly different answer (a different sign) had we done EqualVarianceTTest(y1, y2). This is because a choice is made if we consider \\(\\bar{y}_1-\\bar{y}_2\\) or \\(\\bar{y}_2 - \\bar{y}_1\\) in the statistic.\nIn the use of the linear model, there is a new subtlety – the group variable is categorical and not numeric. A peek at the model matrix (modelmatrix(res)) will show that the categorical variable was coded with a \\(0\\) for each g1 and \\(1\\) for each g2. The details are handled by the underlying StatsModels package which first creates a ModelFrame which takes a formula and the data; ModelMatrix then creates the matrix, \\(X\\). The call to ModelFrame allows a specification of contrasts. The above uses the DummyCoding, which picks a base level (\"g1\" in this case) and then creates a variable for each other level, these variables having values either being 0 or 1, and 1 only when the factor has that level. Using the notation \\(1_{j}(x_i)\\) for this, we have the above call to lm fits the model \\(y_i = \\beta_0 + \\beta_1 \\cdot 1_{\\text{g2}}(x_i) + e_i\\) and the model matrix shows this (2nd row below):\n\nmodelmatrix(res) |> permutedims # turned on side to save page space\n\n2×11 Matrix{Float64}:\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n 0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n\n\nThe model can’t be \\(y_i = \\beta_0 + \\beta_1 \\cdot 1_{\\text{g2}}(x_i) + \\beta_2 \\cdot 1_{\\text{g1}}(x_i) + e_i\\), as there wouldn’t be a unique solution (the model \\(y_i = (\\beta_0 + \\beta_2) + \\beta_2\\cdot 1_{\\text{g2}}(x_i)\\) would also fit, so the parameters would not be identifiable). More mathematically, the model matrix, \\(X\\), would have 3 columns, but one of them could be expressed as a sum of the other 2. This would mean \\(X\\) would not have full rank and the least-squares formula wouldn’t have the form it does.\nTo fit a model with different contrasts, the lm function has a contrast keyword argument.\nThe above formulation does not require the factor to have just 2 levels; if there are \\(k\\) levels, then \\(k-1\\) variables are formed in the model.\n\nExample 9.4 (Categorical covariates example) Consider the cereal data set. The Shelf variable is numeric, but really it should be considered categorical for any study using a linear model, as differences between shelf 1 and 2 and shelf 2 and 3 should not be expected to be uniform (as they would were the values treated numerically). The following first ensures shelf is categorical, then fits a model on how the shelf placement impacts the number of calories:\n\ncereal.shelf = categorical(cereal.Shelf)\nres = lm(@formula(Calories ~ shelf), cereal)\n\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nCalories ~ 1 + shelf\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────\n                Coef.  Std. Error     t  Pr(>|t|)  Lower 95%  Upper 95%\n───────────────────────────────────────────────────────────────────────\n(Intercept)  119.477      13.3488  8.95    <1e-12    92.7936   146.161\nshelf: 2      10.3388     18.878   0.55    0.5859   -27.3979    48.0754\nshelf: 3      60.6692     16.9939  3.57    0.0007    26.6989    94.6394\n───────────────────────────────────────────────────────────────────────\n\n\nThe \\(p\\)-value for shelf 2 is consistent with there being no difference between shelf 1 and 2, but that of shelf 3 (counting from the floor) is significantly different from shelf 1 and would be interpreted as having 60 additional calories over shelf 1. (Which fits the expectation that the lowest-shelf traditionally holds the least sold cereals, hence the most healthy in 1993 when this data was collected).\nWe can check that the model matrix has \\(2\\) variables a few ways: directly from the size (with first column being the intercept), and indirectly by the residual degrees of freedom:\n\nsize(modelmatrix(res)), nobs(res) - dof_residual(res) - 1 # dof_residual = n - k - 1\n\n((65, 3), 2.0)\n\n\nThe omnibus \\(F\\)-test is a statistical test for a null hypothesis that \\(\\beta_i=0\\) for all \\(i\\) except \\(i=0\\). It is implemented in the ftest method of GLM. It requires fitting the null model of just a constant, which we do with:\n\nres₀ = lm(@formula(Calories ~ 1), cereal)  # null model\n\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nCalories ~ 1\n\nCoefficients:\n───────────────────────────────────────────────────────────────────────\n               Coef.  Std. Error      t  Pr(>|t|)  Lower 95%  Upper 95%\n───────────────────────────────────────────────────────────────────────\n(Intercept)  149.408     7.74124  19.30    <1e-27    133.943    164.873\n───────────────────────────────────────────────────────────────────────\n\n\nThe test takes the model, which is stored in the model property:\n\nftest(res.model, res₀.model)\n\nF-test: 2 models fitted on 65 observations\n────────────────────────────────────────────────────────────────────────\n     DOF  ΔDOF          SSR        ΔSSR      R²      ΔR²      F*   p(>F)\n────────────────────────────────────────────────────────────────────────\n[1]    4        198860.3072              0.2023                         \n[2]    2    -2  249295.4943  50435.1871  0.0000  -0.2023  7.8623  0.0009\n────────────────────────────────────────────────────────────────────────\n\n\nIgnoring for now all but the bottom right number which gives the \\(p\\)-value, we see that this null model would be rejected."
  },
  {
    "objectID": "LinearModels/linear-regression.html#interactions",
    "href": "LinearModels/linear-regression.html#interactions",
    "title": "9  The linear regression model",
    "section": "9.3 Interactions",
    "text": "9.3 Interactions\nAn interaction is when the effect of one explanatory variable depends on the values of a different explanatory variable. We see such a case in the following example.\n\nExample 9.5 The ToothGrowth data set is included in base R and summarizes an experiment on the effect of vitamin C on tooth growth in guinea pigs. Each of the 60 animals in the study received one of three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of two delivery methods, orange juice or ascorbic acid. We load the data set using RDatasets:\n\nToothGrowth = dataset(\"datasets\", \"ToothGrowth\")\nfirst(ToothGrowth, 2)\n\n\n2×3 DataFrameRowLenSuppDoseFloat64Cat…Float6414.2VC0.5211.5VC0.5\n\n\nDose is seen to be stored as a numeric variable (Float64), but we treat it as a categorical variable in the following. The table shows that the 6 different treatment pairs were tested on 10 animals.\n\nToothGrowth.Dose = categorical(ToothGrowth.Dose)\ncombine(groupby(ToothGrowth, 2:3), nrow)\n\n\n6×3 DataFrameRowSuppDosenrowCat…Cat…Int641OJ0.5102OJ1.0103OJ2.0104VC0.5105VC1.0106VC2.010\n\n\nFigure 9.3 shows a violinplot with sides reflecting the distribution of the :Supp variable. A quick glance suggests that there may be some effect due to the dosage amount and a difference between the OJ and VC delivery.\n\n\n\n\n\nFigure 9.3: ToothGrowth data set\n\n\n\n\nWe proceed to fit the additive model where Supp introduces one variable, and Dose two:\n\nres = lm(@formula(Len ~ Supp + Dose), ToothGrowth)\n\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nLen ~ 1 + Supp + Dose\n\nCoefficients:\n──────────────────────────────────────────────────────────────────────\n              Coef.  Std. Error      t  Pr(>|t|)  Lower 95%  Upper 95%\n──────────────────────────────────────────────────────────────────────\n(Intercept)  12.455     0.98828  12.60    <1e-17   10.4752    14.4348\nSupp: VC     -3.7       0.98828  -3.74    0.0004   -5.67976   -1.72024\nDose: 1.0     9.13      1.21039   7.54    <1e-09    6.7053    11.5547\nDose: 2.0    15.495     1.21039  12.80    <1e-17   13.0703    17.9197\n──────────────────────────────────────────────────────────────────────\n\n\nThe small \\(p\\)-values support the visual observation that there are differences. The value for “Supp: VC”, for instance, indicates that holding the dose equal, administering the dosage through citamin C and not ascorbic acid had a negative effect of \\(-3.7\\) units on the predicted average tooth length.\nVisually, the distribution of the VC variable seems to depend on the dosage. Perhaps there is an interaction.\nFor this data we can fit a model\n\\[\\begin{align*}\ny_i & = \\beta_0 + \\\\\n    & \\beta_1 \\cdot 1_{\\text{VC}}(\\text{Supp}_i) + \\\\\n    & \\beta_2 \\cdot 1_{1.0}(\\text{Dose}_i) + \\beta_3 \\cdot 1_{2.0}(\\text{Dose}_i) + \\\\\n    & \\beta_4 \\cdot 1_{\\text{VC}, 1.0}(\\text{Supp}_i, \\text{Dose}_i) + \\beta_5 \\cdot 1_{\\text{VC}, 2.0}(\\text{Supp}_i, \\text{Dose}_i) + e_i\n\\end{align*}\\]\nThe additional terms account for cases where, say, Supp = VC and Dose = 1.0.\nInteractions are specified in the modeling formula through *. (Which when used also includes the additive terms without interactions. Plain interactions are specified with &.). The model is:\n\nresᵢ = lm(@formula(Len ~ Supp * Dose), ToothGrowth)\n\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nLen ~ 1 + Supp + Dose + Supp & Dose\n\nCoefficients:\n──────────────────────────────────────────────────────────────────────────────\n                      Coef.  Std. Error      t  Pr(>|t|)  Lower 95%  Upper 95%\n──────────────────────────────────────────────────────────────────────────────\n(Intercept)           13.23     1.14835  11.52    <1e-15  10.9277     15.5323\nSupp: VC              -5.25     1.62402  -3.23    0.0021  -8.50596    -1.99404\nDose: 1.0              9.47     1.62402   5.83    <1e-06   6.21404    12.726\nDose: 2.0             12.83     1.62402   7.90    <1e-09   9.57404    16.086\nSupp: VC & Dose: 1.0  -0.68     2.29671  -0.30    0.7683  -5.28462     3.92462\nSupp: VC & Dose: 2.0   5.33     2.29671   2.32    0.0241   0.725381    9.93462\n──────────────────────────────────────────────────────────────────────────────\n\n\nAs expected from the graph, the \\(p\\)-value for the “Supp: VC & Dose: 2.0” case is significant.\nAs before, an \\(F\\) test can test the difference between the model with and without the interaction:\n\nftest(res.model, resᵢ.model)\n\nF-test: 2 models fitted on 60 observations\n───────────────────────────────────────────────────────────────────\n     DOF  ΔDOF       SSR       ΔSSR      R²     ΔR²      F*   p(>F)\n───────────────────────────────────────────────────────────────────\n[1]    5        820.4250             0.7623                        \n[2]    7     2  712.1060  -108.3190  0.7937  0.0314  4.1070  0.0219\n───────────────────────────────────────────────────────────────────\n\n\nThe small \\(p\\)-value suggests the interaction is statistically significant."
  },
  {
    "objectID": "LinearModels/linear-regression.html#f-test",
    "href": "LinearModels/linear-regression.html#f-test",
    "title": "9  The linear regression model",
    "section": "9.4 F test",
    "text": "9.4 F test\nConsider the linear regression model with parameters \\(\\beta\\) and a significance test with some constraint on the parameters (e.g. \\(\\beta_1 = 0\\) or \\(\\beta_1 = \\beta_2\\)). Suppose the error terms are an iid random sample from a \\(Normal(0, \\sigma)\\) distribution. The a test of \\(H_0\\) against an alternative of not \\(H_0\\) can be carried out by considering the likelihood ratio statistic. The likelihood function for a set of parameters \\(\\beta\\) is:\n\\[\\begin{align*}\nL(\\beta, \\sigma, x) &= \\prod_{i=1}^n \\frac{1}{(2\\pi\\sigma^2)^{n/2}} e^{-\\frac{1}{2\\sigma^2}(y_i - \\hat{y}_i)^2}\n\\\\\n&=\\frac{1}{(2\\pi)^{n/2}} \\frac{1}{\\sigma^n} e^{-\\frac{1}{2\\sigma^2}\\sum_i(y_i - \\hat{y}_i)^2},\n\\end{align*}\\]\nwhere \\(\\hat{y}_i = X\\beta\\) for some \\(X\\) related to the data. As \\(e^{-x}\\) is decreasing, \\(L\\) is maximized in \\(\\beta\\) when \\(\\sum_i (y_i - \\hat{y}_i)^2\\) is minimimized (a least squares estimate), say at \\(m(\\hat{\\beta})\\). In terms of \\(\\sigma\\) we take a logarithm and seek to maximize:\n\\[\n-\\frac{n}{\\sigma}\\ln(2\\pi)  - n \\ln(\\sigma) - \\frac{1}{2\\sigma^2} m(\\hat{\\beta}).\n\\]\nThis occurs at\n\\[\n\\hat{\\sigma^2} = \\frac{m(\\hat{\\beta})}{n} = \\frac{1}{n} SSR,\n\\]\nwhere \\(SSR\\) indicates the sum of the squared residuals, \\(y_i - \\hat{y}_i\\). (This is a biased estimate, as the divisor does not account for the degrees of freedom.)\nThe log-likelihood ratio statistic considers the two models: the restricted one under \\(H_0\\) and the unrestricted one. This simplifies to\n\\[\n\\lambda = -2 \\ln \\frac{L_0}{L} = n \\ln(\\frac{SSR_0}{SSR}),\n\\]\nwith \\(SSR_0\\) being the sum of the squared residuals under \\(H_0\\) and \\(SSR\\) the sum of the squared residuals under the full model, which necessarily is smaller than \\(SSR_0\\). The asymptotic distribution is \\(Chisq(k-p)\\) where \\(p\\) variables are free in \\(H_0\\).\nThe above says if \\(SSR_0/SSR\\) is sufficiently large it is statistically significant. Algebraically, the same thing can be said about\n\\[\nF = \\frac{n-k-1}{k-p} \\cdot \\frac{SSR_0 - SSR}{SSR} =\n\\frac{(SSR_0 - SSR)/(k-p)}{SSR/(n-k-1)}.\n\\]\nThe distribution of \\(SSR/(n-k-1)\\) is \\(Chisq(n-k-1)\\). However, under these assumptions and under the null hypothesis, by Cochran’s theorem \\(SSR_0- SSR\\) is independent of \\(SSR\\) and \\((SSR_0-SSR)/(k-p)\\) is \\(Chisq(k-p)\\). That is \\(F\\) has a \\(FDist(n-k-1, k-p)\\) distribution. (Not asymptotically.)\nThis statistic is used by ftest to compare two nested models. Nested means the parameters in the reduced model are related to those in the full model; no new ones are introduced.\n\nExample 9.6 Consider again the output of the last call to ftest which checked for an interaction between the Supp and Dose variables in the ToothGrowth data:\n\nres₀ = lm(@formula(Len ~ Supp + Dose), ToothGrowth)\nres = lm(@formula(Len ~ Supp * Dose), ToothGrowth)\nftest(res₀.model, res.model)\n\nF-test: 2 models fitted on 60 observations\n───────────────────────────────────────────────────────────────────\n     DOF  ΔDOF       SSR       ΔSSR      R²     ΔR²      F*   p(>F)\n───────────────────────────────────────────────────────────────────\n[1]    5        820.4250             0.7623                        \n[2]    7     2  712.1060  -108.3190  0.7937  0.0314  4.1070  0.0219\n───────────────────────────────────────────────────────────────────\n\n\nThe null hypothesis is \\(H_0: \\beta_4 = \\beta_5 = 0\\). The full model has \\(k=5\\), the null has \\(p=3\\). The reported degrees of freedom is the consumed degrees of freedom which is this number of (linearly independent) columns in the model matrix (\\(k+1\\)) plus \\(1\\). The sum of squares can be computed directly or through the deviance method:\n\nSSR₀, SSR = deviance(res₀), deviance(res) # or, say, sum(residuals(res₀).^2)\n\n(820.4250000000001, 712.1059999999999)\n\n\nThe difference between the two is the numerator of the \\(F\\) statistic when divided by \\(2 = 5-3\\) (or \\(7-5\\)). The denominator should be \\(SSR/(n-k-1)\\):\n\n((SSR₀ - SSR)/(5 - 3)) / (SSR / (60 - 1 - 5))\n\n4.1069910940225265\n\n\nThe degrees of freedom (\\(n-1-k\\)) is also calculated by\n\ndof_residual(res)\n\n54.0\n\n\nThe ftest can test more than two models. For example, suppose we test the null model with just an intercept, as in:\n\nresᵢ = lm(@formula(Len ~ 1), ToothGrowth)\nftest(resᵢ.model, res₀.model, res.model)\n\nF-test: 3 models fitted on 60 observations\n──────────────────────────────────────────────────────────────────────\n     DOF  ΔDOF        SSR        ΔSSR      R²     ΔR²       F*   p(>F)\n──────────────────────────────────────────────────────────────────────\n[1]    2        3452.2093              0.0000                         \n[2]    5     3   820.4250  -2631.7843  0.7623  0.7623  59.8795  <1e-16\n[3]    7     2   712.1060   -108.3190  0.7937  0.0314   4.1070  0.0219\n──────────────────────────────────────────────────────────────────────\n\n\nThe output here has two \\(p\\)-values, the first testing if the additive model is statistically significant (with a very small \\(p\\)-value), the second testing, as mentioned, if the model with interaction is statistically significant compared to the additive model.\n\n\nExample 9.7 We borrow an example from (Faraway 2004) to illustrate how the \\(F\\)-test can be used to test a null hypothesis of \\(H_0: \\beta_i = \\beta_j\\).\nThe dataset is in the datasets package of R:\n\nsavings = dataset(\"datasets\", \"LifeCycleSavings\")\nfirst(savings, 2)\n\n\n2×6 DataFrameRowCountrySRPop15Pop75DPIDDPIString15Float64Float64Float64Float64Float641Australia11.4329.352.872329.682.872Austria12.0723.324.411507.993.93\n\n\nWe fit the full model for SR through:\n\nres = lm(@formula(SR ~ Pop15 + Pop75 + DPI + DDPI), savings)\n\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nSR ~ 1 + Pop15 + Pop75 + DPI + DDPI\n\nCoefficients:\n─────────────────────────────────────────────────────────────────────────────────\n                    Coef.   Std. Error      t  Pr(>|t|)    Lower 95%    Upper 95%\n─────────────────────────────────────────────────────────────────────────────────\n(Intercept)  28.5661       7.35452       3.88    0.0003  13.7533      43.3788\nPop15        -0.461193     0.144642     -3.19    0.0026  -0.752518    -0.169869\nPop75        -1.6915       1.0836       -1.56    0.1255  -3.87398      0.490983\nDPI          -0.000336902  0.000931107  -0.36    0.7192  -0.00221225   0.00153844\nDDPI          0.409695     0.196197      2.09    0.0425   0.0145336    0.804856\n─────────────────────────────────────────────────────────────────────────────────\n\n\nA test of \\(H_0: \\beta_1 = \\beta_2\\) is done by preparing a variable Pop15 + Pop75 (rather than a modification to the formula):\n\nres1575 = lm(@formula(SR ~ Pop1575 + DPI + DDPI),\n             transform(savings, [:Pop15, :Pop75] => (+) => :Pop1575))\n\nStatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, CholeskyPivoted{Float64, Matrix{Float64}, Vector{Int64}}}}, Matrix{Float64}}\n\nSR ~ 1 + Pop1575 + DPI + DDPI\n\nCoefficients:\n──────────────────────────────────────────────────────────────────────────────────\n                    Coef.   Std. Error      t  Pr(>|t|)    Lower 95%     Upper 95%\n──────────────────────────────────────────────────────────────────────────────────\n(Intercept)  21.6093       4.88336       4.43    <1e-04  11.7796      31.439\nPop1575      -0.333633     0.103868     -3.21    0.0024  -0.542708    -0.124558\nDPI          -0.000845101  0.000844351  -1.00    0.3221  -0.00254469   0.000854489\nDDPI          0.390965     0.196871      1.99    0.0530  -0.00531671   0.787247\n──────────────────────────────────────────────────────────────────────────────────\n\n\nThe ftest then can be applied:\n\nftest(res.model, res1575.model)\n\nF-test: 2 models fitted on 50 observations\n──────────────────────────────────────────────────────────────────\n     DOF  ΔDOF       SSR     ΔSSR      R²      ΔR²      F*   p(>F)\n──────────────────────────────────────────────────────────────────\n[1]    6        650.7130           0.3385                         \n[2]    5    -1  673.6275  22.9145  0.3152  -0.0233  1.5847  0.2146\n──────────────────────────────────────────────────────────────────\n\n\nThe large \\(p\\) value suggests no reason to reject this null.\n\n\n\n\n\nFaraway, J. J. (2004), Linear models with r, hapman & Hall/CRC.\n\n\nWackerly, D., Mendenhall, W., and Scheaffer, R. L. (2008), Mathematical statistics with applications, Cengage."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "al., S. et (2021), “LinRegOutliers: A julia package for detecting\noutliers in linear regression,” Journal of Open Source\nSoftware, 6. https://doi.org/10.21105/joss.02892.\n\n\nAlday, P., Kliegl, R., and Bates, D. (2022), Embrace uncertainty:\nFitting mixed-effects models with julia,\nhttps://juliamixedmodels.github.io/EmbraceUncertainty/.\n\n\nBates, D., and others (n.d.). GLM.jl. https://doi.org/10.5281.\n\n\nBezanson, J., Edelman, A., Karpinski, S., and Shah, V. B. (2017),\n“Julia: A fresh\napproach to numerical computing,” SIAM review, SIAM,\n59, 65–98.\n\n\nBOX, G. E. P. (1953), “NON-NORMALITY AND TESTS ON\nVARIANCES,” Biometrika, 40, 318–335. https://doi.org/10.1093/biomet/40.3-4.318.\n\n\nCressie, N., and Read, T. R. C. (1984), “Multinomial goodness-of-fit\ntests,” Journal of the Royal Statistical Society. Series\nB (Methodological), [Royal Statistical Society, Wiley], 46,\n440–464.\n\n\nDanisch, S., and Krumbiegel, J. (2021), “Makie.jl: Flexible\nhigh-performance data visualization for julia,” Journal of\nOpen Source Software, The Open Journal, 6, 3349. https://doi.org/10.21105/joss.03349.\n\n\nFaraway, J. J. (2004), Linear models with r, hapman &\nHall/CRC.\n\n\nGalton, F. (1886), “Regression towards mediocrity\nin hereditary stature.” The Journal of the\nAnthropological Institute of Great Britain and Ireland, [Royal\nAnthropological Institute of Great Britain; Ireland, Wiley], 15,\n246–263.\n\n\nKamiński, B. (2022), Julia for\ndata analysis, Manning.\n\n\nNazarathy, Y., and Klok, H. (2021), Statistics with julia:\nFundamentals for data science, machine learning and artificial\nintelligence, Springer.\n\n\nStoropoli, J., Huijzer, R., and Alonso, L. (2021), Julia data science.\n\n\nVertechi, P., and others (n.d.). AlgebraOfGraphics.jl.\n\n\nWackerly, D., Mendenhall, W., and Scheaffer, R. L. (2008),\nMathematical statistics with applications, Cengage.\n\n\nWickham, H. (2011), “The split-apply-combine strategy for data\nanalysis,” Journal of Statistical Software, 40, 1–29. https://doi.org/10.18637/jss.v040.i01.\n\n\nWickham, H. (2014), “Tidy data,” The\nJournal of Statistical Software, 59."
  }
]